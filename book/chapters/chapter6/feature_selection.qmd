# Feature Selection {#sec-feature-selection}

{{< include ../../common/_setup.qmd >}}

`r chapter = "Feature Selection"`
`r authors(chapter)`

`r index('Feature selection')`, also known as variable or descriptor selection\index{variable selection|see{feature selection}}\index{descriptor selection|see{feature selection}}, is the process of finding a subset of features to use with a given task and learner.
Using an *optimal set* of features can have several benefits:

* improved predictive performance, since we reduce overfitting on irrelevant features,
* robust models that do not rely on noisy features,
* simpler models that are easier to interpret,
* faster model fitting, e.g. for model updates,
* faster prediction, and
* no need to collect potentially expensive features.

However, these objectives will not necessarily be optimized by the same *optimal set* of features and thus feature selection can be seen as a `r index('multi-objective optimization')` problem.
In this chapter, we mostly focus on feature selection as a means of improving predictive performance, but also briefly cover optimization of multiple criteria (@sec-multicrit-featsel).

Reducing the number of features can improve models across many scenarios, but it can be especially helpful in datasets that have a high number of features in comparison to the number of data points.
Many learners perform implicit, also called embedded, feature selection,\index{feature selection!implicit}\index{feature selection!embedded} e.g. via the choice of variables used for splitting in a decision tree.
Most other feature selection methods are model agnostic, i.e. they can be used together with any learner.
Of the many different approaches to identifying relevant features, we will focus on two general concepts, which are described in detail below: Filter and Wrapper methods (@guyon2003;@chandrashekar2014).

## Filters {#sec-fs-filter}

Filter methods are `r index('preprocessing')` steps that can be applied before training a model.
A very simple filter approach could look like this:

1. calculate the correlation coefficient $\rho$ between each feature and a numeric target variable, and
2. select all features with $\rho > 0.2$ for further modeling steps.

This approach is a *univariate* filter because it only considers the univariate relationship between each feature and the target variable.
Further, it can only be applied to regression tasks with continuous features and the threshold of $\rho > 0.2$ is quite arbitrary.
Thus, more advanced filter methods, e.g. *multivariate* filters based on feature importance, usually perform better (@bommert2020).
On the other hand, a benefit of univariate filters is that they are usually computationally cheaper than more complex filter or wrapper methods.
In the following, it is described how to calculate univariate, multivariate and feature importance filters, how to access implicitly selected features, how to integrate filters in a machine learning pipeline and how to optimize filter thresholds.

Filter algorithms select features by assigning numeric scores to each feature, e.g. correlation between feature and target variables, use these to rank the features and select a feature subset based on the ranking.
Features that are assigned lower scores can then be omitted in subsequent modeling steps.
All filters are implemented via the package `r mlr3filters`.
Below, we cover how to

* instantiate a `Filter` object,
* calculate scores for a given task, and
* use calculated scores to select or drop features.

Special cases of filters are `r index('feature importance')` filters (@sec-fs-var-imp-filters) and embedded methods (@sec-fs-embedded-methods).
Feature importance filters select features that are important according to the model induced by a selected `Learner`.
They rely on the learner to extract information on feature importance from a trained model, for example, by inspecting a learned decision tree and returning the features that are used as split variables, or by computing model-agnostic feature importance (@sec-interpretation) values for each feature.
Embedded methods use the feature selection that is implicitly done by some learners and directly retrieve the internally selected features from the learner.

::: {.callout-tip}

## Independent learners and filters

The learner used in a feature importance or embedded filter is independent of learners used in subsequent modeling steps.
For example, one might use feature importance of a random forest for feature selection and train a neural network on the reduced feature set.
:::

Many filter methods are implemented in `mlr3filters`, including:

* Correlation, calculating Pearson or Spearman correlation between numeric features and numeric targets (`flt("correlation")`)
* Information gain, i.e. mutual information of the feature and the target or the reduction of uncertainty of the target due to a feature (`flt("information_gain")`)
* Minimal joint mutual information maximization, minimizing the joint information between selected features to avoid redundancy (`flt("jmim")`)
* Permutation score, which calculates permutation feature importance (see [@sec-interpretation]) with a given learner for each feature (`flt("permutation")`)
* Area under the ROC curve calculated for each feature separately (`flt("auc")`)

Most of the filter methods have some limitations, for example, the correlation filter can only be calculated for regression tasks with numeric features.
For a full list of all implemented filter methods we refer the reader to the `r link("https://mlr3filters.mlr-org.com")`, which also shows the supported task and features types.
A benchmark of filter methods was performed by @bommert2020, who recommend to not rely on a single filter method but try several ones if the available computational resources allow.
If only a single filter method is to be used, the authors recommend to use a feature importance filter using random forest permutation importance (see [@sec-fs-var-imp-filters]), similar to the permutation method described above, but also the JMIM and AUC filters performed well in their comparison.

### Calculating Filter Values {#sec-fs-calc}

The first step is to create a new R object using the class of the desired filter method.
These are accessible from the `r ref("mlr_filters", index = TRUE)` dictionary with the sugar function `r ref("flt()", index = TRUE)`[`mlr_filters`/`flt()`]{.aside}.
Each object of class `r ref("Filter", index = TRUE)` has a `$calculate()`\index{Filter!\$calculate()}[`$calculate()`]{.aside} method, which computes the filter values and ranks them in a descending order.
For example, we can use the information gain filter described above:

```{r feature-selection-001}
library(mlr3filters)
filter = flt("information_gain")
```

Such a `Filter` object can now be used to calculate the filter on the penguins data and get the results:

```{r feature-selection-002}
task_pen = tsk("penguins")
filter$calculate(task_pen)

as.data.table(filter)
```

Some filters have hyperparameters, which can be changed similar to setting hyperparameters of a `r ref("Learner")` using `$param_set$values`.
For example, to calculate `"spearman"` instead of `"pearson"` correlation with the correlation filter:

```{r feature-selection-003}
filter_cor = flt("correlation", method = "spearman")
filter_cor$param_set
```

As noted above, the correlation filter can only be calculated for regression tasks with numeric features and can thus not be used with the penguins data.

### Feature Importance Filters {#sec-fs-var-imp-filters}

To use feature importance filters, we can use a learner with integrated feature importance methods.
All learners with the property "importance" have this functionality.
A list of all learners with this property can be found with

```{r feature-selection-004, eval = FALSE}
as.data.table(mlr_learners)[sapply(properties, function(x) "importance" %in% x)]
```

or by searching the table on the page `r link("https://mlr-org.com/learners.html")`.

For some learners, the desired filter method needs to be set during learner creation.
For example, `lrn("classif.ranger")` comes with multiple integrated methods, which can be selected during construction:
To use the `r index('feature importance')` method `"impurity"`, select it during learner construction:

```{r feature-selection-005}
lrn("classif.ranger")$param_set$levels$importance
lrn_ranger = lrn("classif.ranger", importance = "impurity")
```

We first have to remove missing data because the learner cannot handle missing data, i.e. it does not have the property "missing":

```{r feature-selection-006}
task_pen = tsk("penguins")
task_pen$filter(which(complete.cases(task_pen$data())))
```

Now we can use `flt("importance")` to calculate importance values:

```{r feature-selection-006-2}
filter = flt("importance", learner = lrn_ranger)
filter$calculate(task_pen)
as.data.table(filter)
```

### Embedded Methods {#sec-fs-embedded-methods}

Many learners internally select a subset of the features which they find helpful for prediction, but ignore other features.
For example, a decision tree might never select some features for splitting.
These subsets can be used for feature selection, which we call `r index('embedded methods')` because the feature selection is embedded in the learner.
The selected features (and those not selected) can be queried if the learner has the `"selected_features"` property.
As above, we can find those learners with

```{r feature-selection-007, eval = FALSE}
as.data.table(mlr_learners)[sapply(properties, function(x) "selected_features" %in% x)]
```

For example, we can use `lrn("classif.rpart")`:

```{r feature-selection-007-2}
task_pen = tsk("penguins")
lrn_rpart = lrn("classif.rpart")
lrn_rpart$train(task_pen)
lrn_rpart$selected_features()
```

The features selected by the model can be extracted by a `Filter` object, where `$calculate()` corresponds to training the learner on the given task:

```{r feature-selection-008}
filter = flt("selected_features", learner = lrn_rpart)
filter$calculate(task_pen)
as.data.table(filter)
```

Contrary to other filter methods, embedded methods just return value of `1` (selected features) and `0` (dropped feature).

### Filter-based Feature Selection {#sec-fs-filter-based}

After calculating a score for each feature, one has to select the features to be kept or those to be dropped from further modeling steps.
For the `"selected_features"` filter described in embedded methods (@sec-fs-embedded-methods), this step is straight-forward since the methods assigns either a value of `1` for a feature to be kept or `0` for a feature to be dropped. Below, we find the names of features with a value of `1` and select those feature with `task$select()`:

```{r feature-selection-009}
task_pen = tsk("penguins")
filter = flt("selected_features", learner = lrn_rpart)
filter$calculate(task_pen)

# select all features used by rpart
keep = names(which(filter$scores == 1))
task_pen$select(keep)
task_pen$feature_names
```

For filter methods which assign continuous scores, there are essentially two ways to select features:

* select the top $k$ features, or
* select all features with a score above a threshold $\tau$,

where the first option is equivalent to dropping the bottom $p-k$ features.
For both options, one has to decide on a threshold, which is often quite arbitrary.
For example, to implement the first option with the information gain filter:

```{r feature-selection-010}
task_pen = tsk("penguins")
filter = flt("information_gain")
filter$calculate(task_pen)

# select top 3 features from information gain filter
keep = names(head(filter$scores, 3))
task_pen$select(keep)
task_pen$feature_names
```

Or, the second option with $\tau = 0.5$:

```{r feature-selection-011}
task_pen = tsk("penguins")
filter = flt("information_gain")
filter$calculate(task_pen)

# select all features with score >0.5 from information gain filter
keep = names(which(filter$scores > 0.5))
task_pen$select(keep)
task_pen$feature_names
```

In @sec-pipelines-featsel we will return to filter-based feature selection and how we can use `r index('pipelines')` and tuning to automate and optimize the feature selection process.

## Wrapper Methods {#sec-fs-wrapper}

Wrapper methods work by fitting models on selected feature subsets and evaluating their performance (@Kohavi1997).
This can be done in a sequential fashion, e.g. by iteratively adding features to the model in sequential forward selection, or in a parallel fashion, e.g. by evaluating random feature subsets in a random search.
Below, the use of these simple approaches is described in a common framework along with more advanced methods such as genetic search.
It is further shown how to select features by optimizing multiple performance measures and how to wrap a learner with feature selection to use it in pipelines or benchmarks.

::: {.callout-tip}

## Dependent learners and wrappers

In contrast to filters (@sec-fs-filter), the learner used in the wrapper feature selection is *not* independent of learners used in subsequent modeling steps.
The idea of wrapper methods is to directly include, i.e. wrap, the feature selection with the learner to optimize its performance.
:::

In more detail, wrapper methods iteratively select features that optimize a performance measure.
Instead of ranking features, a model is fit on a selected subset of features in each iteration and evaluated in resampling with respect to a selected performance measure.
The strategy that determines which feature subset is used in each iteration is given by the `r ref("FSelector", index = TRUE)` object.
A simple example is the sequential forward selection that starts with computing each single-feature model, selects the best one, and then iteratively adds the feature that leads to the largest performance improvement (@fig-sequential-forward-selection).

```{r optimization-003}
#| label: fig-sequential-forward-selection
#| fig-cap: Sequential Forward Selection.
#| fig-alt: Sequential Forward Selection.
#| echo: false
include_multi_graphics("Figures/mlr3book_figures-15.svg", "Figures/mlr3book_figures-15.png")
include_multi_graphics("Figures/mlr3book_figures-16.svg", "Figures/mlr3book_figures-16.png")
```

Wrapper methods can be used with any learner but need to train the learner potentially many times, leading to a computationally intensive method.
All wrapper methods are implemented via the package `r mlr3fselect`.
In this chapter, we cover how to

* instantiate an `FSelector` object,
* configure it, to e.g. respect a runtime limit or for different objectives,
* run it or fuse it with a `r ref("Learner")` via an `r ref("AutoFSelector")`.

::: {.callout-tip}

## Feature selection and HPO

Wrapper-based feature selection is very similar to HPO (@sec-optimization).
The major difference is that we search for well-performing feature subsets instead of hyperparameter configurations.
We will see below, that we can even use the same terminators, that some feature selection algorithms are similar to tuners and that we can also optimize multiple performance measures with feature selection.
:::

### Simple Forward Selection Example {#sec-fs-wrapper-example}

We start with the simple example from above and do sequential forward selection with the penguins data, in contrast to HPO (@sec-optimization), `r ref('fselect()', aside = TRUE)` directly starts the optimization and selects features.

```{r feature-selection-017, message=FALSE}
library(mlr3fselect)

# subset features to ease visualization
task_pen = tsk("penguins")
task_pen$select(c("bill_depth", "bill_length", "body_mass", "flipper_length"))

instance = fselect(
  fselector = fs("sequential"),
  task =  task_pen,
  learner = lrn_rpart,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.acc")
)
```

To show all analyzed feature subsets and the corresponding performance, we use `as.data.table(instance$archive)`.
In this example, the `batch_nr` column represents the iteration of the `r index('sequential forward selection')` and we start by looking at the first iteration.

```{r feature-selection-018}
dt = as.data.table(instance$archive)
dt[batch_nr == 1, 1:5]
```

We see that the feature `flipper_length` achieved the highest prediction performance in the first iteration and is thus selected.
We plot the performance over the iterations:

```{r feature-selection-018-5-evalF, eval = FALSE}
autoplot(instance, type = "performance")
```
```{r feature-selection-018-5-evalT, echo = FALSE, message = FALSE}
#| label: fig-fowardselection
#| fig-cap: Model performance in iterations of sequential forward selection.
#| fig-alt: 'Scatter and line plot with "Batch" on the x-axis and "classif.acc" on the y-axis. Line shows improving performance from 1 to batch 2 then increases very slightly in batch 3 and decreases in 4,the values are in the instance archive printed below.'
autoplot(instance, type = "performance") + ggplot2::scale_fill_grey() + ggplot2::scale_color_grey()
```

In the plot, we can see that adding features improves a lot between batches one and two but marginally for three and gets worse for four, which we can see in each iteration:

```{r feature-selection-018-2}
dt[batch_nr == 2, 1:5]
dt[batch_nr == 3, 1:5]
dt[batch_nr == 4, 1:5]
```

The improvement in batch three is small so we may even prefer to select a marginally worse model with two features to reduce data size.

To directly show the best feature set, we can use `$result_feature_set` which returns the features in alphabetical order (not order selected):

```{r feature-selection-019}
instance$result_feature_set
```

Internally, the `fselect()` function creates an `r ref('FSelectInstanceSingleCrit')` object and executes the feature selection with an `r ref('FSelector', index = TRUE)` object, based on the selected method, in this example an `r ref("FSelectorSequential")` object.
It uses the supplied resampling and measure to evaluate all feature subsets provided by the `FSelector` on the task.

At the heart of `mlr3fselect` are the R6 classes:

* `r ref("FSelectInstanceSingleCrit")`, `r ref("FSelectInstanceMultiCrit")`: These two classes describe the feature selection problem and store the results.
* `r ref("FSelector")`: This class is the base class for implementations of feature selection algorithms.

In the following two sections, these classes will be created manually, to learn more about the `mlr3fselect` package.

### The FSelectInstance Classes

To create an `r ref("FSelectInstanceSingleCrit")` object, we use the sugar function `r ref("fsi()", aside = TRUE)`:

```{r feature-selection-020}
instance = fsi(
  task = task_pen,
  learner = lrn_rpart,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.acc"),
  terminator = trm("evals", n_evals = 20)
)
```

Note that we have not selected a feature selection algorithm and thus did not select any features, yet.
We have also supplied a `Terminator`, which is used to stop the feature selection, these are the same objects as we saw in @sec-terminator.

To start the feature selection, we still need to select an algorithm which are defined via the `r ref("FSelector")` class, described in the next section.

### The FSelector Class

The `r index('FSelector', code = TRUE)` class is the base class for different feature selection algorithms.
The following algorithms are currently implemented in `mlr3fselect`:

* Random search, trying random feature subsets until termination (`fs("random_search")`)
* Exhaustive search, trying all possible feature subsets (`fs("exhaustive_search")`)
* Sequential search, i.e. sequential forward or backward selection (`fs("sequential")`)
* Recursive feature elimination, which uses learner's importance scores to iteratively remove features with low feature importance (`fs("rfe")`)
* Design points, trying all user-supplied feature sets (`fs("design_points")`)
* Genetic search, implementing a genetic algorithm which treats the features as a binary sequence and tries to find the best subset with mutations (`fs("genetic_search")`)
* Shadow variable search, which adds permuted copies of all features (shadow variables) and stops when a shadow variable is selected (`fs("shadow_variable_search")`)

Note that all these methods can be stopped (early) with a terminator, e.g. an exhaustive search can be stopped after a given number of evaluations.
In this example, we will use a simple random search and retrieve it from the `r ref("mlr_fselectors")` dictionary  with `r ref("fs()")`[`mlr_fselector`/`fs()`]{.aside}.

```{r feature-selection-021}
fselector = fs("random_search")
```

### Starting the Feature Selection

To start the feature selection, we pass the `r ref("FSelectInstanceSingleCrit")` object to the `$optimize()` method of the initialized `r ref("FSelector")` object:

```{r feature-selection-022, output=FALSE}
fselector$optimize(instance)
```

The algorithm proceeds as follows

1. The `FSelector` proposes at least one feature subset and may propose multiple subsets to improve parallelization, which can be controlled via the setting `batch_size`.
1. For each feature subset, the given learner is fitted on the task using the provided resampling and evaluated with the given measure.
1. All evaluations are stored in the archive of the `FSelectInstanceSingleCrit` object.
1. The terminator is queried if the budget is exhausted. If the budget is not exhausted, restart with 1) until it is.
1. Determine the feature subset with the best observed performance.
1. Store the best feature subset as the result in the instance object.

The best feature subset and the corresponding measured performance can be accessed from the instance:

```{r feature-selection-023}
  as.data.table(instance$result)[, .(features, classif.acc)]
```

As in the forward selection example above, one can investigate all resamplings which were undertaken, as they are stored in the archive of the `FSelectInstanceSingleCrit` object and can be accessed by using `as.data.table()`:

```{r feature-selection-024}
as.data.table(instance$archive)[1:5,
  .(bill_depth, bill_length, body_mass, flipper_length, classif.acc)]
```

Now the optimized feature subset can be used to subset the task and fit the model on all observations:

```{r feature-selection-025, eval=FALSE}
task_pen = tsk("penguins")

task_pen$select(instance$result_feature_set)
lrn_rpart$train(task_pen)
```

The trained model can now be used to make a prediction on external data.

::: {.callout-warning}

## Nested Resampling

Predicting on observations present in the data used for feature selection should be avoided.
The model has seen these observations already during feature selection and therefore performance evaluation results would be over-optimistic.
Instead, to get unbiased performance estimates for the current task_pen, nested resampling (see @sec-autofselect and @sec-nested-resampling) is required.
:::

### Optimizing Multiple Performance Measures {#sec-multicrit-featsel}

You might want to use multiple criteria to evaluate the performance of the feature subsets.
For example, you might want to select the subset with the highest classification accuracy and lowest time to train the model.
However, these two subsets will generally not coincide, i.e. the subset with highest classification accuracy will probably be another subset than that with lowest training time.
With `mlr3fselect`, the result is the pareto-optimal solution, i.e. the best feature subset for each of the criteria that is not dominated by another subset.
For the example with classification accuracy and training time, a feature subset that is best in accuracy *and* training time will dominate all other subsets and thus will be the only pareto-optimal solution.
If, however, different subsets are best in the two criteria, both subsets are pareto-optimal.
Again, we point out the similarity with HPO and refer to multi-objective hyperparameter optimization (see @sec-multi-metrics-tuning and @karl2022).

In the following example, we will perform feature selection on the sonar dataset. This time, we will use `r ref("FSelectInstanceMultiCrit")` to select a subset of features that has high sensitivity, i.e. TPR, and high specificity, i.e. TNR. The feature selection process with multiple criteria is similar to that with a single criterion, except that we select two measures to be optimized:

```{r feature-selection-026}
instance = fsi(
  task = tsk("sonar"),
  learner = lrn_rpart,
  resampling = rsmp("holdout"),
  measure = msrs(c("classif.tpr", "classif.tnr")),
  terminator = trm("evals", n_evals = 20)
)
```

The function `r ref("fsi")` creates an instance of `FSelectInstanceMultiCrit` if more than one measure is selected.
We now create an `r ref("FSelector")` and call the `$optimize()` function of the `FSelector` with the `FSelectInstanceMultiCrit` object, to search for the subset of features with the best TPR and FPR.
Note that these two measures cannot both be optimal at the same time (except for the perfect classifier) and we expect several pareto-optimal solutions.

```{r feature-selection-027, output=FALSE}
fselector = fs("random_search")
fselector$optimize(instance)
```

As above, the best feature subsets and the corresponding measured performance can be accessed from the instance.

```{r feature-selection-029}
as.data.table(instance$result)[, .(features, classif.tpr, classif.tnr)]
```

We see different tradeoffs of sensitivity and specificity but no feature subset is dominated by another, i.e. has worse sensitivity *and* specificity than any other subset.

### Automating the Feature Selection and Nested Resampling {#sec-autofselect}

The `r ref('AutoFSelector', aside = TRUE)` class wraps a learner and augments it with an automatic feature selection for a given task.
Because the `AutoFSelector` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
Below, a new learner is created.
This learner is then wrapped in a random search feature selector, which automatically starts a feature selection on the given task using an inner resampling, as soon as the wrapped learner is trained.
The sugar function `r ref("auto_fselector", aside = TRUE)` can be used to create an instance of `AutoFSelector`:

```{r feature-selection-030}
at = auto_fselector(
  fselector = fs("random_search"),
  learner = lrn("classif.log_reg"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("evals", n_evals = 10)
)
at
```

The `AutoFSelector` can then be passed to `benchmark()` or `resample()` for nested resampling (@sec-nested-resampling).
Below we compare our wrapped learner `at` with a normal logistic regression `lrn("classif.log_reg")`.

```{r feature-selection-031, warning=FALSE}
grid = benchmark_grid(tsk("sonar"), list(at, lrn("classif.log_reg")),
  rsmp("cv", folds = 3))

bmr = benchmark(grid)$aggregate(msr("classif.acc"))
as.data.table(bmr)[, .(learner_id, classif.acc)]
```

We can see that, in this example, the feature selection improves prediction performance.

## Conclusion

In this chapter, we learned how to perform feature selection with `mlr3`.
We introduced filter and wrapper methods and covered the optimization of multiple performance measures.
Once you have learnt about pipelines we will return to feature selection in @sec-pipelines-featsel.

If you are interested in learning more about feature selection then we recommend an overview of methods in @chandrashekar2014, a more formal and detailed introduction to filters and wrappers in @guyon2003, and a benchmark of filter methods by @bommert2020.

@tbl-api-feature-selection summarizes the most important functions and methods seen in this chapter.


| Underlying R6 Class | Constructor (if applicable) | Important methods |
| --- | --- | --- |
| `r ref("Filter")` | `r ref("flt()")` | `$calculate()` |
| `r ref("FSelectInstanceSingleCrit")` or `r ref("FSelectInstanceMultiCrit")` | `r ref("fselect()")` | - |
| `r ref("FSelector")` | `r ref("fs()")` | `$optimize()` |
| `r ref("AutoFSelector")` | `r ref("auto_fselector()")` | `$train()`/`$predict()` |

: Important classes and functions covered in this chapter with underlying `R6` class (if applicable), constructor to create an object of the class, and important class methods. {#tbl-api-feature-selection}

## Exercises

1. Calculate a correlation filter on the `mtcars` task.
2. Use the filter from the first exercise to select the five best features in `tsk("mtcars")`.
3. Apply a backward selection to the `penguins` task with `lrn("classif.rpart")` and holdout resampling by the measure classification accuracy. Compare the results with those in @sec-fs-wrapper-example. Answer the following questions:
    a. Do the selected features differ?
    b. Which feature selection method achieves a higher classification accuracy?
    c. Are the accuracy values in b) directly comparable? If not, what has to be changed to make them comparable?
4. Automate the feature selection as in @sec-autofselect with the `spam` task and `lrn("classif.log_reg")`.

::: {.content-visible when-format="html"}
`r citeas(chapter)`
:::
