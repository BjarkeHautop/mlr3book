<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.3.272" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />


<title>Applied Machine Learning Using mlr3 in R – 9  Preprocessing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<!-- htmldependencies:E3FAD763 -->
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body>


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn"
      data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" 
      aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation"
      onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <h1 class="quarto-secondary-nav-title"></h1>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" 
      aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation"
      onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="/">
      Applied Machine Learning Using mlr3 in R
      </a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="/Applied-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;1&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Introduction and Overview&lt;/span&gt;</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;2&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Data and Basic Modeling&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;3&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Evaluation and Benchmarking&lt;/span&gt;</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;4&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Hyperparameter Optimization&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;5&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Advanced Tuning Methods and Black Box Optimization&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;6&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Feature Selection&lt;/span&gt;</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;7&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Sequential Pipelines&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;8&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Non-sequential Pipelines and Tuning&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;9&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Preprocessing&lt;/span&gt;</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;10&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Advanced Technical Aspects of mlr3&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;11&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Large-Scale Benchmarking&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;12&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Model Interpretation&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;13&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Beyond Regression and Classification&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;14&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Algorithmic Fairness&lt;/span&gt;</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/appendices/citation_information.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;A&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Citation Information&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/appendices/session_info.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;B&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Session Info&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;C&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Solutions to exercises&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;D&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Tasks&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;E&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;Overview Tables&lt;/span&gt;</span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/chapters/appendices/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&lt;span class=&#39;chapter-number&#39;&gt;F&lt;/span&gt;  &lt;span class=&#39;chapter-title&#39;&gt;References&lt;/span&gt;</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" ></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <div id="quarto-toc-target"></div>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-preprocessing" class="quarto-section-identifier"><span class="chapter-number">9</span>  <span class="chapter-title">Preprocessing</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>
<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#data-cleaning" id="toc-data-cleaning"><span class="header-section-number">9.1</span> Data Cleaning</a></li>
  <li><a href="#factor-encoding" id="toc-factor-encoding"><span class="header-section-number">9.2</span> Factor Encoding</a></li>
  <li><a href="#sec-preprocessing-missing" id="toc-sec-preprocessing-missing"><span class="header-section-number">9.3</span> Missing Values</a></li>
  <li><a href="#sec-prepro-robustify" id="toc-sec-prepro-robustify"><span class="header-section-number">9.4</span> Pipeline Robustify</a></li>
  <li><a href="#sec-prepro-scale" id="toc-sec-prepro-scale"><span class="header-section-number">9.5</span> Scaling Features and Targets</a></li>
  <li><a href="#feature-extraction" id="toc-feature-extraction"><span class="header-section-number">9.6</span> Feature Extraction</a></li>
  <li><a href="#conclusion" id="toc-conclusion"><span class="header-section-number">9.7</span> Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises"><span class="header-section-number">9.8</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation"><span class="header-section-number">9.9</span> Citation</a></li>
  </ul>
</nav>
<p><strong>Janek Thomas</strong> <br> <em>Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML), and Essential Data Science Training GmbH</em> <br><br></p>
<p><a href="#sec-pipelines"><span class="quarto-unresolved-ref">sec-pipelines</span></a> and <a href="#sec-pipelines-nonseq"><span class="quarto-unresolved-ref">sec-pipelines-nonseq</span></a> provided a technical introduction to <a href="https://mlr3pipelines.mlr-org.com"><code>mlr3pipelines</code></a>, this chapter will now demonstrate how to use those pipelines to tackle common problems when preprocessing data for ML, including factor encoding, imputation of missing values, feature and target transformations, and feature extraction. Feature selection, whilst being an important preprocessing method, is covered in <a href="#sec-feature-selection"><span class="quarto-unresolved-ref">sec-feature-selection</span></a> for a more extensive overview.</p>
<p>In this book, preprocessing refers to everything that happens with the <em>data</em> before it is used to fit the model, while postprocessing<span class="column-margin">Postprocessing</span> encompasses everything that occurs with <em>predictions</em> after the model is fitted. Data cleaning<span class="column-margin">Data Cleaning</span> is an important part of preprocessing that involves the removal of errors, noise, and redundancy in the data; we only consider data cleaning very briefly as it is usually performed outside of <code>mlr3</code> on the raw dataset.</p>
<p>Another aspect of preprocessing is feature engineering<span class="column-margin">Feature Engineering</span>, which covers all other transformations of data before it is fed to the machine learning model, including the creation of features from possibly unstructured data, such as written text, sequences or images. The goal of feature engineering is to prepare the data so that a model can be trained on it, and/or to further improve predictive performance. It is important to note that feature engineering helps mostly for simpler algorithms, while highly complex models usually gain less from it and require little data preparation to be trained. Common difficulties in data that can be solved with feature engineering include features with (high) skew distributions, high cardinality categorical features, missing observations, high dimensional dimensionality and imbalanced classes in classification tasks. Deep learning has shown promising results in automating feature engineering, however its effectiveness depends on the complexity and nature of the data being processed, as well as the specific problem being addressed. Typically it is applicable to natural language processing and computer vision problems, while standard tabular data is lacking in structure for deep learning models to extract meaningful features automatically. Furthermore, different problems require different features to be extracted, and deep learning models may not always be able to identify the most relevant features for a given problem without human guidance. Hence, manual feature engineering is often required but with <code>mlr3pipelines</code>, we can simplify the process as much as possible.</p>
<p>As we work through this chapter we will use an adapted version of the Ames housing data <span class="citation" data-cites="de2011ames">(<a href="#ref-de2011ames" role="doc-biblioref">De Cock 2011</a>)</span>. We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset, the code to recreate this version of the data from the original raw data can be found at <a href="https://github.com/ja-thomas/extend_ames_housing">https://github.com/ja-thomas/extend_ames_housing</a>. This dataset was collected as an alternative to the Boston Housing data and is commonly used to demonstrate feature engineering and ML. Raw and processed versions of the data can be directly loaded from the <a href="https://cran.r-project.org/package=AmesHousing"><code>AmesHousing</code></a> package. The dataset includes 2,930 residential properties (rows) situated in Ames, Iowa, sold between 2006 and 2010. It contains 81 features on various aspects of the house, size and shape of the lot, and information about its condition and quality. The prediction target is the sale price in USD, hence it is a regression task.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-001_4a9d5df508daeaa13948a71b19ad3814">
<div class="sourceCode" id="cb1"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>repo <span class="ot">=</span> <span class="st">&quot;ja-thomas/extend_ames_housing/main/data/ames_dirty.csv&quot;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>ames <span class="ot">=</span> <span class="fu">fread</span>(<span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com/&quot;</span>, repo),</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">stringsAsFactors =</span> <span class="cn">TRUE</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<section id="data-cleaning" class="level2" data-number="9.1">
<h2 data-number="9.1"><span class="header-section-number">9.1</span> Data Cleaning</h2>
<p>As a first step we explore the data and look for simple problems such as constant or duplicated features. This can be done quite efficiently with a package like <a href="https://cran.r-project.org/package=DataExplorer"><code>DataExplorer</code></a> or <a href="https://cran.r-project.org/package=skimr"><code>skimr</code></a> which can be used to create a large number of plots.</p>
<p>Instead of pretending to discover issues with the data, below we will just summarize the most important findings for data cleaning:</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-003_ba2ed9685a28891252c7bbe53f97126a">
<div class="sourceCode" id="cb2"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. `Misc_Feature_2` is a factor with only a single level `othr`.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames<span class="sc">$</span>Misc_Feature_2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Othr 
2930 </code></pre>
</div>
<div class="sourceCode" id="cb4"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. `Condition_2` and `Condition_3` are identical.</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">identical</span>(ames<span class="sc">$</span>Condition_2, ames<span class="sc">$</span>Condition_3)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
<div class="sourceCode" id="cb6"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. `Lot_Area` and `Lot_Area_m2` represent the same data but on different scales</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(ames<span class="sc">$</span>Lot_Area, ames<span class="sc">$</span>Lot_Area_m2)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
</div>
<p>For all three problems, simply removing the problematic features (or feature in a pair) is the best course of action.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-006_88f1a3c92b245d4daa1563d42040a1eb">
<div class="sourceCode" id="cb8"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>to_remove <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;Lot_Area_m2&quot;</span>, <span class="st">&quot;Condition_3&quot;</span>, <span class="st">&quot;Misc_Feature_2&quot;</span>)</span></code></pre></div>
</div>
<p>Other typical problems that should be checked are:</p>
<ol type="1">
<li>ID columns, i.e., columns that are unique for every observations should be removed or tagged.</li>
<li><code>NA</code>s not correctly encoded, e.g. as <code>"NA"</code> or <code>""</code></li>
<li>Semantic errors in the data, e.g., negative <code>Lot_Area</code></li>
<li>Numeric features encoded as categorical for learners that can not handle such features.</li>
</ol>
<p>Before we continue with feature engineering we will create a task, measure, and resampling strategy to use throughout the chapter.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-007_6b920ba4899c18c10cd3c4c4a23624f1">
<div class="sourceCode" id="cb9"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3verse)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>tsk_ames <span class="ot">=</span> <span class="fu">as_task_regr</span>(ames, <span class="at">target =</span> <span class="st">&quot;Sale_Price&quot;</span>, <span class="at">id =</span> <span class="st">&quot;ames&quot;</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># remove problematic features</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>tsk_ames<span class="sc">$</span><span class="fu">select</span>(<span class="fu">setdiff</span>(tsk_ames<span class="sc">$</span>feature_names, to_remove))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>measure <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">&quot;regr.mae&quot;</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>rsmp_cv3 <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>rsmp_cv3<span class="sc">$</span><span class="fu">instantiate</span>(tsk_ames)</span></code></pre></div>
</div>
<p>Lastly we run a very simple experiment to verify our setup works as expected with a simple featureless baseline, note below we set <code>robust = TRUE</code> to always predict the <em>median</em> sale price as opposed to the <em>mean</em>.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-008_2a517160efb1f4d6e7994fcf31c11a99">
<div class="sourceCode" id="cb10"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lrn_baseline <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.featureless&quot;</span>, <span class="at">robust =</span> <span class="cn">TRUE</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>lrn_baseline<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;Baseline&quot;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>rr_baseline <span class="ot">=</span> <span class="fu">resample</span>(tsk_ames, lrn_baseline, rsmp_cv3)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>rr_baseline<span class="sc">$</span><span class="fu">aggregate</span>(measure)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>regr.mae 
   56056 </code></pre>
</div>
</div>
</section>
<section id="factor-encoding" class="level2" data-number="9.2">
<h2 data-number="9.2"><span class="header-section-number">9.2</span> Factor Encoding</h2>
<p>We refer to variables as categorical features if they can only take a limited set of values, for example the <code>Paved_Drive</code> feature can only take values <code>Dirt_Gravel</code>, <code>Partial_Pavement</code>, and <code>Paved</code>.</p>
<p>Many machine learning algorithms implementations, such as XGBoost <span class="citation" data-cites="chen2016xgboost">(<a href="#ref-chen2016xgboost" role="doc-biblioref">Chen and Guestrin 2016</a>)</span>, cannot handle categorical data and so categorical features must be encoded into numerical variables.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-010_611e7b2db548805e1fb0009ed81977f0">
<div class="sourceCode" id="cb12"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>lrn_xgb <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.xgboost&quot;</span>, <span class="at">nrounds =</span> <span class="dv">100</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span><span class="fu">train</span>(tsk_ames)</span></code></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error: &lt;TaskRegr:ames&gt; has the following unsupported feature types: factor</code></pre>
</div>
</div>
<p>Categorical features can be distinguished from one another by their cardinality, which refers to the number of levels they contain. There are three types of categorical features: binary (two levels), low-cardinality, and high-cardinality; there is no universal threshold for when a feature should be considered high-cardinality however one can consider this threshold to be a tunable hyperparameter that can be tuned. For now we will consider high-cardinality to be features with more than 10 levels:</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-3_dad33e8ffc1ecd7fd4da81c4438b4539">
<div class="sourceCode" id="cb14"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which</span>(<span class="fu">lengths</span>(tsk_ames<span class="sc">$</span><span class="fu">levels</span>()) <span class="sc">&gt;</span> <span class="dv">10</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] &quot;Exterior_1st&quot; &quot;Exterior_2nd&quot; &quot;MS_SubClass&quot;  &quot;Neighborhood&quot;</code></pre>
</div>
</div>
<p>Binary features can be trivially encoded by setting one of the feature levels to <code>1</code> and the other to <code>0</code>.</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-4_a69472b86576cd4bd73864be47e87296">
<div class="sourceCode" id="cb16"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which</span>(<span class="fu">lengths</span>(tsk_ames<span class="sc">$</span><span class="fu">levels</span>()) <span class="sc">==</span> <span class="dv">2</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] &quot;Alley&quot;       &quot;Central_Air&quot; &quot;Street&quot;     </code></pre>
</div>
</div>
<p>Low-cardinality features can be handled by one-hot encoding<span class="column-margin">One-hot Encoding</span>. One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature. Theoretically it is sufficient to create one less binary feature than levels, as setting all binary features to zero is also a valid representation. This is typically called dummy or treatment encoding and is required if the learner is a generalized linear (GLM) or additive model (GAM) model.</p>
<p>Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding. Impact encoding is a good approach to handle high-cardinality features. Impact encoding<span class="column-margin">Impact Encoding</span> converts categorical features into numeric values based on the impact of the feature on the target. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps:</p>
<ol type="1">
<li>Group the target variable by the categorical feature.</li>
<li>Compute the mean of the target variable for each group.</li>
<li>Compute the global mean of the target variable.</li>
<li>Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.</li>
<li>Replace the categorical feature with the impact scores.</li>
</ol>
<p>Impact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. The main advantage, compared to one-hot encoding is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, it is crucial that the encoding process is embedded in the cross-validation process to avoid leakage between training and testing data (<a href="#sec-performance"><span class="quarto-unresolved-ref">sec-performance</span></a>).</p>
<p>As well as encoding features, another basic preprocessing step is to remove any features that are constant (only have one level and should have been removed as part of EDA). In addition, it may be essential to collapse levels that occur very rarely as these may be missed during resampling, though stratification can be used to mitigate this (<a href="#sec-strat-group"><span class="quarto-unresolved-ref">sec-strat-group</span></a>).</p>
<p>In the code below we use <code>po("removeconstants")</code> to remove features with only one level, <code>po("collapsefactors")</code> to collapse levels that occur less than 1% of the time in the data, <code>po("encodeimpact")</code> to impact encode high-cardinality features, <code>po("encode", method = "one-hot")</code> to one-hot encode low-cardinality features, and finally <code>po("encode", method = "treatment")</code> to treatment encode binary features.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-011_2eda4e5c1e369f80ac631107960a5a2d">
<div class="sourceCode" id="cb18"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>factor_pipeline <span class="ot">=</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;removeconstants&quot;</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;collapsefactors&quot;</span>, <span class="at">no_collapse_above_prevalence =</span> <span class="fl">0.01</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;encodeimpact&quot;</span>, <span class="at">affect_columns =</span> <span class="fu">selector_cardinality_greater_than</span>(<span class="dv">10</span>),</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">id =</span> <span class="st">&quot;high_card_enc&quot;</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;encode&quot;</span>, <span class="at">method =</span> <span class="st">&quot;one-hot&quot;</span>, <span class="at">affect_columns =</span> <span class="fu">selector_cardinality_greater_than</span>(<span class="dv">2</span>),</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">id =</span> <span class="st">&quot;low_card_enc&quot;</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;encode&quot;</span>, <span class="at">method =</span> <span class="st">&quot;treatment&quot;</span>, <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">&quot;factor&quot;</span>),</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">id =</span> <span class="st">&quot;binary_enc&quot;</span>)</span></code></pre></div>
</div>
<p>Now we can apply this pipeline to our xgboost model to use it in a benchmark experiment; we also compare a simpler pipeline that only uses one-hot encoding to demonstrate performance difference resulting from different strategies.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-013_ad33c2478233e50fe27c80ee5668949b">
<div class="sourceCode" id="cb19"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>glrn_xgb_impact <span class="ot">=</span> <span class="fu">as_learner</span>(factor_pipeline <span class="sc">%&gt;&gt;%</span> lrn_xgb)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>glrn_xgb_impact<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;XGB_enc_impact&quot;</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>glrn_xgb_one_hot <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">&quot;encode&quot;</span>) <span class="sc">%&gt;&gt;%</span> lrn_xgb)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>glrn_xgb_one_hot<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;XGB_enc_onehot&quot;</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>learners <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">baseline =</span> lrn_baseline,</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">xgb_impact =</span> glrn_xgb_impact,</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">xgb_one_hot =</span> glrn_xgb_one_hot</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(tsk_ames, learners, rsmp_cv3))</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)[, .(learner_id, regr.mae)]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       learner_id regr.mae
1:       Baseline    56056
2: XGB_enc_impact    16068
3: XGB_enc_onehot    16098</code></pre>
</div>
</div>
<p>In this small experiment we see that the difference between the extended factor encoding pipeline and the simpler one-hot encoding strategy pipeline is only moderate. If you are interested in learning more about different encoding strategies, including a benchmark study comparing them, we recommend <span class="citation" data-cites="pargent2022regularized">Pargent et al. (<a href="#ref-pargent2022regularized" role="doc-biblioref">2022</a>)</span>.</p>
</section>
<section id="sec-preprocessing-missing" class="level2" data-number="9.3">
<h2 data-number="9.3"><span class="header-section-number">9.3</span> Missing Values</h2>
<p>A common problem in real-world data is missing data. In the Ames dataset, several variables have at least one missing data point:</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-5_2820979dac185991b020f8b346217b3d">
<div class="sourceCode" id="cb21"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which</span>(tsk_ames<span class="sc">$</span><span class="fu">missings</span>() <span class="sc">&gt;</span> <span class="dv">0</span>)  )</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] &quot;Alley&quot;          &quot;BsmtFin_SF_1&quot;   &quot;BsmtFin_SF_2&quot;   &quot;BsmtFin_Type_1&quot;
 [5] &quot;BsmtFin_Type_2&quot; &quot;Bsmt_Cond&quot;      &quot;Bsmt_Exposure&quot;  &quot;Bsmt_Full_Bath&quot;
 [9] &quot;Bsmt_Half_Bath&quot; &quot;Bsmt_Qual&quot;      &quot;Bsmt_Unf_SF&quot;    &quot;Electrical&quot;    
[13] &quot;Fence&quot;          &quot;Fireplace_Qu&quot;   &quot;Garage_Area&quot;    &quot;Garage_Cars&quot;   
[17] &quot;Garage_Cond&quot;    &quot;Garage_Finish&quot;  &quot;Garage_Qual&quot;    &quot;Garage_Type&quot;   
[21] &quot;Garage_Yr_Blt&quot;  &quot;Lot_Frontage&quot;   &quot;Mas_Vnr_Area&quot;   &quot;Mas_Vnr_Type&quot;  
[25] &quot;Misc_Feature&quot;   &quot;Pool_QC&quot;        &quot;Total_Bsmt_SF&quot; </code></pre>
</div>
</div>
<p>Many learners cannot handle missing values automatically (e.g., <code>lrn("regr.ranger")</code> and <code>lrn("regr.lm")</code>), other learners can handle missing values but may use simple methods that may not be ideal (e.g., just omitting rows with missing data).</p>
<p>The simplest data imputation<span class="column-margin">Data Imputation</span> method is to replace missing values by the feature’s mean (<code>po("imputemean")</code>) (<a href="#fig-imputation">Figure <span class="quarto-unresolved-ref">fig-imputation</span></a>), median (<code>po("imputemedian")</code>), or mode (<code>po("imputemode")</code>). Alternatively, one can impute by sampling from the empirical distribution of the feature, for example a histogram (<code>po("imputehist")</code>). Instead of guessing at what a missing feature might be, missing values could instead be replaced by a new level, for example called <code>.MISSING</code> (<code>po("imputeoor")</code>). For numeric features, <span class="citation" data-cites="ding2010investigation">Ding and Simonoff (<a href="#ref-ding2010investigation" role="doc-biblioref">2010</a>)</span> show that for binary classification and tree-based models, encoding missing values out-of-range (OOR), e.g. as two times the largest observed value, is a reasonable approach.</p>
<div class="cell" data-hash="preprocessing_cache/html/fig-imputation_238a8df2f19abae3406b60e0623e9362">
<div class="cell-output-display">
<div id="fig-imputation" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="Figures/mlr3book_figures-13.svg" class="quarto-discovered-preview-image img-fluid" alt="Impute missing values." /></p>
<p><figcaption>Figure 9.1: Impute missing values.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>It is very important for predictive tasks that you keep track of missing data as it is common for missing data to be informative in itself. As a real-world example, medical data is usually better collected for White communities than racially minoritized ones. Imputing data from minoritized communities would at best mask this data bias, and at worst would make the data bias even worse by making vastly inaccurate assumptions (see <a href="#sec-fairness"><span class="quarto-unresolved-ref">sec-fairness</span></a> for data bias and algorithmic fairness). Hence, imputation should be tracked by adding binary indicator features (one for each imputed feature) that are <code>1</code> if the feature was missing for an observation and <code>0</code> if it was present (<code>po("missind")</code>).</p>
<p>In the code below we create a pipeline form the <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOp.html" class="refcode"><code>PipeOp</code></a>s listed above as well as making use of <code>po("featureunion")</code> to combine multiple <code>PipeOp</code>s acting on the <code>"integer"</code> columns.</p>
<div class="cell" data-crop="true" data-hash="preprocessing_cache/html/fig-impute_1c723639ba075b05ce36359df1847b3c">
<div class="sourceCode" id="cb23"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>impute_hist <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;missind&quot;</span>,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">type =</span> <span class="st">&quot;integer&quot;</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">&quot;integer&quot;</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;imputehist&quot;</span>,</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">&quot;integer&quot;</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    )) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;featureunion&quot;</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;imputeoor&quot;</span>,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">&quot;factor&quot;</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>impute_hist<span class="sc">$</span><span class="fu">plot</span>(<span class="at">horizontal =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<div class="cell-output-display">
<div id="fig-impute" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="preprocessing_files/figure-html/fig-impute-1.png" class="img-fluid" alt="Flow diagram shows &#39;&lt;INPUT&gt;&#39; with arrows to &#39;missind&#39; and &#39;imputehist&#39;, which both have arrows to &#39;featureunion&#39;, which has an arrow to &#39;imputeoor&#39; that has an arrow to &#39;&lt;OUTPUT&#39;&gt;." width="672" /></p>
<p><figcaption>Figure 9.2: Pipeline to impute missing values of numeric features by histogram with binary indicators and missings in categoricals out-of-range with a new level.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>Using this pipeline we can now run experiments with <code>lrn("regr.ranger")</code>, which cannot handle missing data; we also compare a simpler pipeline that only uses OOR imputation to demonstrate performance difference resulting from different strategies.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-016_96893f1dcca2548dae35cc117e20a8ee">
<div class="sourceCode" id="cb24"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_hist <span class="ot">=</span> <span class="fu">as_learner</span>(impute_hist <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.ranger&quot;</span>))</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_hist<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;RF_imp_Hist&quot;</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_oor <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">&quot;imputeoor&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.ranger&quot;</span>))</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_oor<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;RF_imp_OOR&quot;</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(tsk_ames, <span class="fu">c</span>(glrn_rf_impute_hist, glrn_rf_impute_oor), rsmp_cv3)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>bmr_new <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">combine</span>(bmr_new)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)[, .(learner_id, regr.mae)]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       learner_id regr.mae
1:       Baseline    56056
2: XGB_enc_impact    16068
3: XGB_enc_onehot    16098
4:    RF_imp_Hist    16400
5:     RF_imp_OOR    16395</code></pre>
</div>
</div>
<p>Similarly to encoding, we see limited difference in performance between the different imputation strategies.</p>
<p>Many more advanced imputation strategies exist, including model based imputation where machine learning models are used to predict missing values before passing, and multiple imputation where data is repeatedly resampling and imputed in each sample (e.g., by mean imputation) to attain more robust estimates. However, these more advanced techniques rarely improve the model substantially and the simple imputation techniques introduced above are usually sufficient <span class="citation" data-cites="Poulos2018">(<a href="#ref-Poulos2018" role="doc-biblioref">Poulos and Valle 2018</a>)</span>.</p>
</section>
<section id="sec-prepro-robustify" class="level2" data-number="9.4">
<h2 data-number="9.4"><span class="header-section-number">9.4</span> Pipeline Robustify</h2>
<p><code>mlr3pipelines</code> offers a simple and reusable pipeline for (among other things) imputation and factor encoding called <code>ppl("robustify")</code>, which includes sensible defaults that can be used most of the time when encoding or imputing data. The pipeline includes the following <code>PipeOp</code>s:</p>
<ol type="1">
<li><code>po("removeconstants")</code> – Constant features are removed.</li>
<li><code>po("colapply")</code> – Character and ordinal features are encoded as categorical, and date/time features are encoded as numeric.</li>
<li><code>po("imputehist")</code> – Numeric features are imputed by histogram.</li>
<li><code>po("imputesample")</code> – Logical features are imputed by sampling from the empirical distribution.</li>
<li><code>po("missind")</code> – Missing data indicators are added for imputed numeric and logical variables</li>
<li><code>po("imputeoor")</code> – Missing values of categorical features are encoded with a new level</li>
<li><code>po("fixfactors")</code> – Fixes levels of categorical features such that the same levels are present during prediction and training (which may involve dropping empty factor levels)</li>
<li><code>po("imputesample")</code> – Missing values in categorical features introduced from dropping levels in the previous step are imputed by sampling from the empirical distributions.</li>
<li><code>po("collapsefactors")</code> – Categorical features levels are collapsed (starting from the rarest factors in the training data) until there are less than 1000 levels</li>
<li><code>po("encode")</code> – Categorical features are one-hot encoded</li>
<li><code>po("removeconstants")</code> – Constant features that might have been created in the previous steps are removed</li>
</ol>
<p>Linear regression is a simple model that cannot handle most problems that we may face when processing data, but with the <code>ppl("robustify")</code> pipeline we can now include it in our experiment:</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-019_88760051ff13d1350dc7964344ce63ea">
<div class="sourceCode" id="cb26"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>glrn_lm_robust <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">ppl</span>(<span class="st">&quot;robustify&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.lm&quot;</span>))</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>glrn_lm_robust<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;lm_robust&quot;</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>bmr_new <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(tsk_ames, glrn_lm_robust,  rsmp_cv3))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">combine</span>(bmr_new)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)[, .(learner_id, regr.mae)]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       learner_id regr.mae
1:       Baseline    56056
2: XGB_enc_impact    16068
3: XGB_enc_onehot    16098
4:    RF_imp_Hist    16400
5:     RF_imp_OOR    16395
6:      lm_robust    16298</code></pre>
</div>
</div>
<p>Robustifying the linear regression results in a model that vastly outperforms the featureless baseline and is competitive when compared to more complex machine learning models.</p>
</section>
<section id="sec-prepro-scale" class="level2" data-number="9.5">
<h2 data-number="9.5"><span class="header-section-number">9.5</span> Scaling Features and Targets</h2>
<p>Simple transformations of features and the target can be beneficial (and sometimes essential) for certain learners. In particular, log transformation of the target can help in making the distribution more symmetrical and can help reduce the impact of outliers; this is particularly important for algorithms that assume the target is normally distributed. Similarly, log transformation of skewed features can help to reduce the influence of outliers. In <a href="#fig-sale">Figure <span class="quarto-unresolved-ref">fig-sale</span></a> we plot the distribution of the target in the <code>ames</code> dataset and then the log-transformed target, we can see how simply taking the log of the variable results in a distribution that is much more symmetrical and with fewer outliers.</p>
<div class="cell" data-hash="preprocessing_cache/html/fig-sale_2d566aeea27ae8e44788475a601e12bf">
<div class="sourceCode" id="cb28"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># copy ames data</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>log_ames <span class="ot">=</span> <span class="fu">copy</span>(ames)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co"># log transform target</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>log_ames[, logSalePrice <span class="sc">:</span><span class="er">=</span> <span class="fu">log</span>(Sale_Price)]</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(<span class="fu">as_task_regr</span>(log_ames, <span class="at">target =</span> <span class="st">&quot;Sale_Price&quot;</span>)) <span class="sc">+</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>(<span class="fu">as_task_regr</span>(log_ames, <span class="at">target =</span> <span class="st">&quot;logSalePrice&quot;</span>))</span></code></pre></div>
<div class="cell-output-display">
<div id="fig-sale" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="preprocessing_files/figure-html/fig-sale-1.png" class="img-fluid" alt="Two boxplots. Left plot shows house prices up to $600,000, the majority of prices are between roughly $100,000-$200,000. Right plot shows log house prices primarily around 12 with an even range between 11 and 13 and a few outliers on both sides." width="672" /></p>
<p><figcaption>Figure 9.3: Distribution of house sales prices (in USD) in the ames dataset before (left) and after (right) log transformation. Before transformation there is a skewed distribution of prices towards cheaper properties with a few outliers of very expensive properties. After transformation the distribution is much more symmetrical with the majority of points evenly spread around the same range.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>Normalization of features may also be necessary to ensure features with a larger scale do not have a higher impact, which is especially important for distance based methods such as K-nearest neighbor models or regularized parametric models such as Lasso or Elastic net. Many models internally scale the data if required by the algorithm so most of the time we do not need to manually do this in preprocessing, though if this is required then <code>po("scale")</code> can be used to center and scale numeric features.</p>
<p>Any transformations applied to the target during training must be inverted during model prediction to ensure predictions are made on the correct scale. By example, say we are interested in log transforming the target, then we would take the following steps:</p>
<div class="cell" data-hash="preprocessing_cache/html/unnamed-chunk-7_3f2d6e09249f2e8f8f90a660d374eef9">
<div class="sourceCode" id="cb29"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">5</span>), <span class="at">y =</span> <span class="fu">runif</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         x     y
1: 0.48004 10.25
2: 0.14466 10.75
3: 0.05795 18.30
4: 0.65004 17.34
5: 0.37355 10.48</code></pre>
</div>
<div class="sourceCode" id="cb31"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. log transform the target</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>df[, y <span class="sc">:</span><span class="er">=</span> <span class="fu">log</span>(y)]</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>y</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.327 2.375 2.907 2.853 2.350</code></pre>
</div>
<div class="sourceCode" id="cb33"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. make linear regression predictions</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co">#    predictions on the log-transformed scale</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">=</span> <span class="fu">predict</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x, df), df)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>yhat</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    1     2     3     4     5 
2.556 2.571 2.575 2.548 2.561 </code></pre>
</div>
<div class="sourceCode" id="cb35"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. transform to correct scale with inverse of log function</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co">#    predictions on the original scale</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(yhat)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    1     2     3     4     5 
12.88 13.08 13.13 12.79 12.95 </code></pre>
</div>
</div>
<p>In this simple experiment we could manually transform and invert the target, however this is much more complex when dealing with resampling and benchmarking experiments and so the pipeline <code>ppl("targettrafo")</code> will do this heavy lifting for you. The pipeline includes a parameter <code>targetmutate.trafo</code> for the transformation to be applied during training to the target, as well as <code>targetmutate.inverter</code> for the transformation to be applied to invert the original transformation during prediction. So now let us consider the log transformation by adding this pipeline to our robust linear regression model:</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-020_13c61d5ba150e66afcf9970d39308ced">
<div class="sourceCode" id="cb37"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>glrn_log_lm_robust <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">ppl</span>(<span class="st">&quot;targettrafo&quot;</span>, <span class="at">graph =</span> glrn_lm_robust,</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">targetmutate.trafo =</span> <span class="cf">function</span>(x)<span class="fu">log</span>(x),</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">targetmutate.inverter =</span> <span class="cf">function</span>(x) <span class="fu">list</span>(<span class="at">response =</span> <span class="fu">exp</span>(x<span class="sc">$</span>response))))</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>glrn_log_lm_robust<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;lm_robust_logtrafo&quot;</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>bmr_new <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(tsk_ames, glrn_log_lm_robust, rsmp_cv3))</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">combine</span>(bmr_new)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)[, .(learner_id, regr.mae)]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           learner_id regr.mae
1:           Baseline    56056
2:     XGB_enc_impact    16068
3:     XGB_enc_onehot    16098
4:        RF_imp_Hist    16400
5:         RF_imp_OOR    16395
6:          lm_robust    16298
7: lm_robust_logtrafo    15557</code></pre>
</div>
</div>
<p>With the target transformation and the <code>ppl("robustify")</code> pipeline, the simple linear regression now appears to be the best performing model.</p>
</section>
<section id="feature-extraction" class="level2" data-number="9.6">
<h2 data-number="9.6"><span class="header-section-number">9.6</span> Feature Extraction</h2>
<p>As a final step of data preprocessing we will look at feature extraction. In <a href="#sec-feature-selection"><span class="quarto-unresolved-ref">sec-feature-selection</span></a> we look at automated feature selection and how automated approaches with filters and wrappers can be used to reduce a dataset to the optimal set of features. Feature extraction differs from this process as we are now interested in features that are highlight dependent on one another and all together may provide useful information but not individually. As a concrete example, consider the power consumption of kitchen appliances in houses in the Ames dataset.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-023_8653e68921625ee1395de834277912ba">
<div class="sourceCode" id="cb39"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>repo <span class="ot">=</span> <span class="st">&quot;ja-thomas/extend_ames_housing/main/data/energy_usage.csv&quot;</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>energy_data <span class="ot">=</span> <span class="fu">fread</span>(<span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com/&quot;</span>, repo),</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">stringsAsFactors =</span> <span class="cn">TRUE</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<p>In this dataset, each row of represents one house and each feature is the total power consumption from kitchen appliances at a given time <span class="citation" data-cites="bagnall2017great">(<a href="#ref-bagnall2017great" role="doc-biblioref">Bagnall et al. 2017</a>)</span>. The consumption is measured in 2-minute intervals, resulting in 720 features.</p>
<div class="cell" data-hash="preprocessing_cache/html/fig-energy_ed12d9bbca80ffb46c4ab52f09ec005f">
<div class="sourceCode" id="cb40"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">y =</span> <span class="fu">as.numeric</span>(energy_data[<span class="dv">1</span>, ])), <span class="fu">aes</span>(<span class="at">y =</span> y, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">720</span>)) <span class="sc">+</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;2-Minute Interval&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Power Consumption&quot;</span>)</span></code></pre></div>
<div class="cell-output-display">
<div id="fig-energy" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="preprocessing_files/figure-html/fig-energy-1.png" class="img-fluid" alt="Line plot with &#39;2-Minute Interval&#39; on axis ranging from 1 to 720 and &#39;Power Consumption&#39; on y-axis ranging from 0 to 20. There are spikes at around (200, 20), (300, 20), and then some consistently raised usage between (500-700, 3)." width="672" /></p>
<p><figcaption>Figure 9.4: Energy consumption of one example house in a day, recorded in 2-minute intervals.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>Adding these 720 features to our full dataset is a bad idea as each individual feature does not provide meaningful information, similarly we cannot automate selection of the best feature subset for the same reason. Instead we can <em>extract</em> information about the curves to gain insights into the kitchen’s overall energy usage. For example, we could extract the maximum used wattage, overall used wattage, number of peaks, and other similar features.</p>
<p>To extract features we will write our own <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOp.html" class="refcode"><code>PipeOp</code></a> that inherits from <code>[</code>PipeOpTaskPreprocSimple<code>](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreprocSimple.html){.refcode}. To do this we simply add a private method called</code>.transform_dt` that hardcodes the operations on our task. In this example we select the functional features (which all start with “att”), extract the mean, minimum, maximum, and variance of the power consumption, and then remove the functional features.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-025_d17cc7fb256633221bf5d07627e343eb">
<div class="sourceCode" id="cb41"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>PipeOpFuncExtract <span class="ot">=</span> R6<span class="sc">::</span><span class="fu">R6Class</span>(<span class="st">&quot;PipeOpFuncExtract&quot;</span>,</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">inherit =</span> mlr3pipelines<span class="sc">::</span>PipeOpTaskPreprocSimple,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">private =</span> <span class="fu">list</span>(</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">.transform_dt =</span> <span class="cf">function</span>(dt, levels) {</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        ffeat_names <span class="ot">=</span> <span class="fu">paste0</span>(<span class="st">&quot;att&quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">720</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>        ffeats <span class="ot">=</span> dt[, ..ffeat_names]</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>        dt[, energy_means <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, mean)]</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        dt[, energy_mins <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, min)]</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        dt[, energy_maxs <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, max)]</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>        dt[, energy_vars <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, var)]</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>        dt[, (ffeat_names) <span class="sc">:</span><span class="er">=</span> <span class="cn">NULL</span>]</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>        dt</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell" data-hash="preprocessing_cache/html/fig-functional-features_b9e704a4323490d13f230272b683d4f5">
<div class="cell-output-display">
<div id="fig-functional-features" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="Figures/mlr3book_figures-14.svg" class="img-fluid" alt="Functional features." /></p>
<p><figcaption>Figure 9.5: Functional features.</figcaption></p>
</figure>
</div>
</div>
</div>
<p>Before using this in an experiment we first test that the <code>PipeOp</code> works as expected.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-026_449ca8f70ca846885e13a20aa4bcfa44">
<div class="sourceCode" id="cb42"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext <span class="ot">=</span> <span class="fu">cbind</span>(ames, energy_data)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext <span class="ot">=</span> <span class="fu">as_task_regr</span>(tsk_ames_ext, <span class="st">&quot;Sale_Price&quot;</span>, <span class="st">&quot;ames_ext&quot;</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># remove the redundant variables identified at the start of this chapter</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext<span class="sc">$</span><span class="fu">select</span>(<span class="fu">setdiff</span>(tsk_ames_ext<span class="sc">$</span>feature_names, to_remove))</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>func_extractor <span class="ot">=</span> PipeOpFuncExtract<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;energy_extract&quot;</span>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext <span class="ot">=</span> func_extractor<span class="sc">$</span><span class="fu">train</span>(<span class="fu">list</span>(tsk_ames_ext))[[<span class="dv">1</span>]]</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext<span class="sc">$</span><span class="fu">data</span>(<span class="dv">1</span>, <span class="fu">c</span>(<span class="st">&quot;energy_means&quot;</span>, <span class="st">&quot;energy_mins&quot;</span>, <span class="st">&quot;energy_maxs&quot;</span>, <span class="st">&quot;energy_vars&quot;</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   energy_means energy_mins energy_maxs energy_vars
1:        1.062     0.01427       21.98       3.708</code></pre>
</div>
</div>
<p>These outputs look sensible compared to <a href="#fig-energy">Figure <span class="quarto-unresolved-ref">fig-energy</span></a> so we can now run our final benchmark experiment using feature extraction. We do not need to add the <code>PipeOp</code> to each learner as we can apply it once (as above) before any model training by applying it to all available data.</p>
<div class="cell" data-hash="preprocessing_cache/html/preprocessing-027_43e480f47213c9aa47b2407d31f6c5cb">
<div class="sourceCode" id="cb44"><pre class="sourceCode r cell-code"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>learners <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">baseline =</span> lrn_baseline,</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree =</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.rpart&quot;</span>),</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">xgb_impact =</span> glrn_xgb_impact,</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">rf_impute_oor =</span> glrn_rf_impute_oor,</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">lm_robust =</span> glrn_lm_robust,</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">log_lm_robust =</span> glrn_log_lm_robust</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>bmr_final <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(<span class="fu">c</span>(tsk_ames_ext, tsk_ames), learners, rsmp_cv3))</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>bmr_final<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    nr  task_id         learner_id resampling_id iters regr.mae
 1:  1 ames_ext           Baseline            cv     3    56056
 2:  2 ames_ext         regr.rpart            cv     3    27111
 3:  3 ames_ext     XGB_enc_impact            cv     3    14400
 4:  4 ames_ext         RF_imp_OOR            cv     3    14320
 5:  5 ames_ext          lm_robust            cv     3    15093
---                                                            
 8:  8     ames         regr.rpart            cv     3    27371
 9:  9     ames     XGB_enc_impact            cv     3    16068
10: 10     ames         RF_imp_OOR            cv     3    16354
11: 11     ames          lm_robust            cv     3    16291
12: 12     ames lm_robust_logtrafo            cv     3    15555
Hidden columns: resample_result</code></pre>
</div>
</div>
<p>The final results indicate that adding these extracted features improved the performance of all models (except the featureless baseline).</p>
<p>In this example, we could have just applied the transformations to the dataset directly. The advantage of using the <code>PipeOp</code> is that we could have chained it to a subset of learners to prevent a blow-up of experiments in the benchmark experiment.</p>
</section>
<section id="conclusion" class="level2" data-number="9.7">
<h2 data-number="9.7"><span class="header-section-number">9.7</span> Conclusion</h2>
<p>In this chapter we built on everything learnt in <a href="#sec-pipelines"><span class="quarto-unresolved-ref">sec-pipelines</span></a> and <a href="#sec-pipelines-nonseq"><span class="quarto-unresolved-ref">sec-pipelines-nonseq</span></a> to look at concrete usage of pipelines for data preprocessing. We focused primarily on feature engineering, which can make use of <a href="https://mlr3pipelines.mlr-org.com"><code>mlr3pipelines</code></a> to automate preprocessing as much as possible whilst still ensuring user control. We looked at factor encoding for categorical variables, imputing missing data, scaling variables, and feature extraction. Preprocessing is almost always required in machine learning experiments, and applying the <code>ppl("robustify")</code> pipeline will help in many cases to simplify this process by applying the most common preprocessing steps, we will see this in use in <a href="#sec-large-benchmarking"><span class="quarto-unresolved-ref">sec-large-benchmarking</span></a>.</p>
<p>We have not introduced any new classes here so in <a href="#tbl-prepro-api">Table <span class="quarto-unresolved-ref">tbl-prepro-api</span></a> we list the <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOp.html" class="refcode"><code>PipeOp</code></a>s and <a href="https://mlr3pipelines.mlr-org.com/reference/Graph.html" class="refcode"><code>Graph</code></a>s discussed in this chapter.</p>
<div id="tbl-prepro-api">
<table>
<caption>Table 9.1: <code>PipeOp</code>s and <code>Graph</code>s discussed in this chapter.</caption>
<thead>
<tr class="header">
<th>PipeOp/Graph</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_removeconstants.html" class="refcode"><code>PipeOpRemoveConstants</code></a></td>
<td>Remove variables consisting of one value</td>
</tr>
<tr class="even">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_collapsefactors.html" class="refcode"><code>PipeOpCollapseFactors</code></a></td>
<td>Combine rare factor levels</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_encodeimpact.html" class="refcode"><code>PipeOpEncodeImpact</code></a></td>
<td>Impact encoding</td>
</tr>
<tr class="even">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_encode.html" class="refcode"><code>PipeOpEncode</code></a></td>
<td>Other factor encoding methods</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_missind.html" class="refcode"><code>PipeOpMissInd</code></a></td>
<td>Add an indicator column to track missing data</td>
</tr>
<tr class="even">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html" class="refcode"><code>PipeOpImputeHist</code></a></td>
<td>Impute missing data by sampling from a histogram</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputeoor.html" class="refcode"><code>PipeOpImputeOOR</code></a></td>
<td>Impute missing data with out-of-range values</td>
</tr>
<tr class="even">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_graphs_robustify.html" class="refcode"><code>pipeline_robustify</code></a></td>
<td>Graph with common imputation and encoding methods</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3pipelines.mlr-org.com/reference/mlr_graphs_targettrafo.html" class="refcode"><code>pipeline_targettrafo</code></a></td>
<td>Graph to transform target during training and invert transformation during prediction</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="exercises" class="level2" data-number="9.8">
<h2 data-number="9.8"><span class="header-section-number">9.8</span> Exercises</h2>
<!-- FIXME: ADD EXERCISES -->
</section>
<section id="citation" class="level2" data-number="9.9">
<h2 data-number="9.9"><span class="header-section-number">9.9</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Thomas J. (2024). Preprocessing. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/preprocessing.html.</p>
<div id="quarto-navigation-envelope" class="hidden">
<p><span class="hidden" data-render-id="quarto-int-sidebar-title">Applied Machine Learning Using mlr3 in R</span> <span class="hidden" data-render-id="quarto-int-navbar-title">Applied Machine Learning Using mlr3 in R</span> <span class="hidden" data-render-id="quarto-int-next"><span class="chapter-number">10</span>  <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span> <span class="hidden" data-render-id="quarto-int-prev"><span class="chapter-number">8</span>  <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/index.html">Getting Started</span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter1/introduction_and_overview.html"><span class="chapter-number">1</span>  <span class="chapter-title">Introduction and Overview</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:quarto-sidebar-section-1">Fundamentals</span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter2/data_and_basic_modeling.html"><span class="chapter-number">2</span>  <span class="chapter-title">Data and Basic Modeling</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter3/evaluation_and_benchmarking.html"><span class="chapter-number">3</span>  <span class="chapter-title">Evaluation and Benchmarking</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:quarto-sidebar-section-2">Tuning and Feature Selection</span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter4/hyperparameter_optimization.html"><span class="chapter-number">4</span>  <span class="chapter-title">Hyperparameter Optimization</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html"><span class="chapter-number">5</span>  <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter6/feature_selection.html"><span class="chapter-number">6</span>  <span class="chapter-title">Feature Selection</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:quarto-sidebar-section-3">Pipelines and Preprocessing</span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter7/sequential_pipelines.html"><span class="chapter-number">7</span>  <span class="chapter-title">Sequential Pipelines</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter8/non-sequential_pipelines_and_tuning.html"><span class="chapter-number">8</span>  <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter9/preprocessing.html"><span class="chapter-number">9</span>  <span class="chapter-title">Preprocessing</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:quarto-sidebar-section-4">Advanced Topics</span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter10/advanced_technical_aspects_of_mlr3.html"><span class="chapter-number">10</span>  <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter11/large-scale_benchmarking.html"><span class="chapter-number">11</span>  <span class="chapter-title">Large-Scale Benchmarking</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter12/model_interpretation.html"><span class="chapter-number">12</span>  <span class="chapter-title">Model Interpretation</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter13/beyond_regression_and_classification.html"><span class="chapter-number">13</span>  <span class="chapter-title">Beyond Regression and Classification</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/chapter14/algorithmic_fairness.html"><span class="chapter-number">14</span>  <span class="chapter-title">Algorithmic Fairness</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:quarto-sidebar-section-5">Appendices</span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/appendices/citation_information.html"><span class="chapter-number">A</span>  <span class="chapter-title">Citation Information</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/appendices/session_info.html"><span class="chapter-number">B</span>  <span class="chapter-title">Session Info</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/appendices/solutions.html"><span class="chapter-number">C</span>  <span class="chapter-title">Solutions to exercises</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/appendices/tasks.html"><span class="chapter-number">D</span>  <span class="chapter-title">Tasks</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/appendices/overview-tables.html"><span class="chapter-number">E</span>  <span class="chapter-title">Overview Tables</span></span> <span class="hidden" data-render-id="quarto-int-sidebar:undefined">—</span> <span class="hidden" data-render-id="quarto-int-sidebar:/chapters/appendices/references.html"><span class="chapter-number">F</span>  <span class="chapter-title">References</span></span> <span class="hidden" data-render-id="footer-left">All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</span> <span class="hidden" data-render-id="footer-center"><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></span> <span class="hidden" data-render-id="footer-right">Built with <a href="https://quarto.org/">Quarto</a>.</span></p>
</div>
<div id="quarto-meta-markdown" class="hidden">
<p><span class="hidden" data-render-id="quarto-metatitle">Applied Machine Learning Using mlr3 in R - <span id="sec-preprocessing" class="quarto-section-identifier"><span class="chapter-number">9</span>  <span class="chapter-title">Preprocessing</span></span></span> <span class="hidden" data-render-id="quarto-twittercardtitle">Applied Machine Learning Using mlr3 in R - <span id="sec-preprocessing" class="quarto-section-identifier"><span class="chapter-number">9</span>  <span class="chapter-title">Preprocessing</span></span></span> <span class="hidden" data-render-id="quarto-ogcardtitle">Applied Machine Learning Using mlr3 in R - <span id="sec-preprocessing" class="quarto-section-identifier"><span class="chapter-number">9</span>  <span class="chapter-title">Preprocessing</span></span></span> <span class="hidden" data-render-id="quarto-metasitename">Applied Machine Learning Using mlr3 in R</span> <span class="hidden" data-render-id="quarto-twittercarddesc"></span> <span class="hidden" data-render-id="quarto-ogcardddesc"></span></p>
</div>
<!-- -->
<div class="quarto-embedded-source-code">
<div class="sourceCode" id="cb46" data-shortcodes="false"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Preprocessing {#sec-preprocessing}</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = &quot;Preprocessing&quot;`</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="in">`r authors(chapter)`</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>@sec-pipelines and @sec-pipelines-nonseq provided a technical introduction to <span class="in">`r mlr3pipelines`</span>, this chapter will now demonstrate how to use those pipelines to tackle common problems when <span class="in">`r index(&#39;preprocessing&#39;)`</span> data for ML, including factor encoding, imputation of missing values, feature and target transformations, and feature extraction.</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>Feature selection, whilst being an important preprocessing method, is covered in @sec-feature-selection for a more extensive overview.</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>In this book, preprocessing refers to everything that happens with the *data* before it is used to fit the model, while `r index(&#39;postprocessing&#39;, aside = TRUE)` encompasses everything that occurs with *predictions* after the model is fitted.</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="in">`r index(&#39;Data cleaning&#39;, aside = TRUE)`</span>\index{exploratory data analysis|see{Data cleaning}} is an important part of preprocessing that involves the removal of errors, noise, and redundancy in the data; we only consider data cleaning very briefly as it is usually performed outside of <span class="in">`mlr3`</span> on the raw dataset.</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>Another aspect of preprocessing is <span class="in">`r index(&#39;feature engineering&#39;, aside = TRUE)`</span>, which covers all other transformations of data before it is fed to the machine learning model, including the creation of features from possibly unstructured data, such as written text, sequences or images.</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>The goal of feature engineering is to prepare the data so that a model can be trained on it, and/or to further improve predictive performance.</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>It is important to note that feature engineering helps mostly for simpler algorithms, while highly complex models usually gain less from it and require little data preparation to be trained.</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>Common difficulties in data that can be solved with feature engineering include features with (high) skew distributions, high cardinality categorical features, missing observations, high dimensional dimensionality and imbalanced classes in classification tasks.</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>Deep learning has shown promising results in automating feature engineering, however its effectiveness depends on the complexity and nature of the data being processed, as well as the specific problem being addressed.</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>Typically it is applicable to natural language processing and computer vision problems, while standard tabular data is lacking in structure for deep learning models to extract meaningful features automatically.</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>Furthermore, different problems require different features to be extracted, and deep learning models may not always be able to identify the most relevant features for a given problem without human guidance.</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>Hence, manual feature engineering is often required but with <span class="in">`mlr3pipelines`</span>, we can simplify the process as much as possible.</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>As we work through this chapter we will use an adapted version of the Ames housing data <span class="co">[</span><span class="ot">@de2011ames</span><span class="co">]</span>.</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset, the code to recreate this version of the data from the original raw data can be found at <span class="in">`r link(&quot;https://github.com/ja-thomas/extend_ames_housing&quot;)`</span>.</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>This dataset was collected as an alternative to the Boston Housing data and is commonly used to demonstrate feature engineering and ML.</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>Raw and processed versions of the data can be directly loaded from the <span class="in">`r ref_pkg(&quot;AmesHousing&quot;)`</span> package.</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>The dataset includes 2,930 residential properties (rows) situated in Ames, Iowa, sold between 2006 and 2010.</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>It contains 81 features on various aspects of the house, size and shape of the lot, and information about its condition and quality.</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>The prediction target is the sale price in USD, hence it is a regression task.</span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-001, echo = TRUE, eval = TRUE, message=FALSE}</span></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>repo <span class="ot">=</span> <span class="st">&quot;ja-thomas/extend_ames_housing/main/data/ames_dirty.csv&quot;</span></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>ames <span class="ot">=</span> <span class="fu">fread</span>(<span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com/&quot;</span>, repo),</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">stringsAsFactors =</span> <span class="cn">TRUE</span></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Cleaning</span></span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>As a first step we explore the data and look for simple problems such as constant or duplicated features.</span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>This can be done quite efficiently with a package like <span class="in">`r ref_pkg(&quot;DataExplorer&quot;)`</span> or <span class="in">`r ref_pkg(&quot;skimr&quot;)`</span> which can be used to create a large number of plots.</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a>Instead of pretending to discover issues with the data, below we will just summarize the most important findings for data cleaning:</span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-003, message=FALSE}</span></span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. `Misc_Feature_2` is a factor with only a single level `othr`.</span></span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames<span class="sc">$</span>Misc_Feature_2)</span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. `Condition_2` and `Condition_3` are identical.</span></span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a><span class="fu">identical</span>(ames<span class="sc">$</span>Condition_2, ames<span class="sc">$</span>Condition_3)</span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. `Lot_Area` and `Lot_Area_m2` represent the same data but on different scales</span></span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(ames<span class="sc">$</span>Lot_Area, ames<span class="sc">$</span>Lot_Area_m2)</span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>For all three problems, simply removing the problematic features (or feature in a pair) is the best course of action.</span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-006, message=FALSE}</span></span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a>to_remove <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;Lot_Area_m2&quot;</span>, <span class="st">&quot;Condition_3&quot;</span>, <span class="st">&quot;Misc_Feature_2&quot;</span>)</span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-61"><a href="#cb46-61" aria-hidden="true" tabindex="-1"></a>Other typical problems that should be checked are:</span>
<span id="cb46-62"><a href="#cb46-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-63"><a href="#cb46-63" aria-hidden="true" tabindex="-1"></a>1) ID columns, i.e., columns that are unique for every observations should be removed or tagged.</span>
<span id="cb46-64"><a href="#cb46-64" aria-hidden="true" tabindex="-1"></a>2) <span class="in">`NA`</span>s not correctly encoded, e.g. as <span class="in">`&quot;NA&quot;`</span> or <span class="in">`&quot;&quot;`</span></span>
<span id="cb46-65"><a href="#cb46-65" aria-hidden="true" tabindex="-1"></a>3) Semantic errors in the data, e.g., negative <span class="in">`Lot_Area`</span></span>
<span id="cb46-66"><a href="#cb46-66" aria-hidden="true" tabindex="-1"></a>4) Numeric features encoded as categorical for learners that can not handle such features.</span>
<span id="cb46-67"><a href="#cb46-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-68"><a href="#cb46-68" aria-hidden="true" tabindex="-1"></a>Before we continue with feature engineering we will create a task, measure, and resampling strategy to use throughout the chapter.</span>
<span id="cb46-69"><a href="#cb46-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-70"><a href="#cb46-70" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-007, message=FALSE}</span></span>
<span id="cb46-71"><a href="#cb46-71" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3verse)</span>
<span id="cb46-72"><a href="#cb46-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-73"><a href="#cb46-73" aria-hidden="true" tabindex="-1"></a>tsk_ames <span class="ot">=</span> <span class="fu">as_task_regr</span>(ames, <span class="at">target =</span> <span class="st">&quot;Sale_Price&quot;</span>, <span class="at">id =</span> <span class="st">&quot;ames&quot;</span>)</span>
<span id="cb46-74"><a href="#cb46-74" aria-hidden="true" tabindex="-1"></a><span class="co"># remove problematic features</span></span>
<span id="cb46-75"><a href="#cb46-75" aria-hidden="true" tabindex="-1"></a>tsk_ames<span class="sc">$</span><span class="fu">select</span>(<span class="fu">setdiff</span>(tsk_ames<span class="sc">$</span>feature_names, to_remove))</span>
<span id="cb46-76"><a href="#cb46-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-77"><a href="#cb46-77" aria-hidden="true" tabindex="-1"></a>measure <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">&quot;regr.mae&quot;</span>)</span>
<span id="cb46-78"><a href="#cb46-78" aria-hidden="true" tabindex="-1"></a>rsmp_cv3 <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb46-79"><a href="#cb46-79" aria-hidden="true" tabindex="-1"></a>rsmp_cv3<span class="sc">$</span><span class="fu">instantiate</span>(tsk_ames)</span>
<span id="cb46-80"><a href="#cb46-80" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-81"><a href="#cb46-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-82"><a href="#cb46-82" aria-hidden="true" tabindex="-1"></a>Lastly we run a very simple experiment to verify our setup works as expected with a simple featureless baseline, note below we set <span class="in">`robust = TRUE`</span> to always predict the *median* sale price as opposed to the *mean*.</span>
<span id="cb46-83"><a href="#cb46-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-84"><a href="#cb46-84" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-008, echo = TRUE, eval = TRUE, message=FALSE}</span></span>
<span id="cb46-85"><a href="#cb46-85" aria-hidden="true" tabindex="-1"></a>lrn_baseline <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.featureless&quot;</span>, <span class="at">robust =</span> <span class="cn">TRUE</span>)</span>
<span id="cb46-86"><a href="#cb46-86" aria-hidden="true" tabindex="-1"></a>lrn_baseline<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;Baseline&quot;</span></span>
<span id="cb46-87"><a href="#cb46-87" aria-hidden="true" tabindex="-1"></a>rr_baseline <span class="ot">=</span> <span class="fu">resample</span>(tsk_ames, lrn_baseline, rsmp_cv3)</span>
<span id="cb46-88"><a href="#cb46-88" aria-hidden="true" tabindex="-1"></a>rr_baseline<span class="sc">$</span><span class="fu">aggregate</span>(measure)</span>
<span id="cb46-89"><a href="#cb46-89" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-90"><a href="#cb46-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-91"><a href="#cb46-91" aria-hidden="true" tabindex="-1"></a><span class="fu">## Factor Encoding</span></span>
<span id="cb46-92"><a href="#cb46-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-93"><a href="#cb46-93" aria-hidden="true" tabindex="-1"></a>We refer to variables as categorical features if they can only take a limited set of values, for example the <span class="in">`Paved_Drive`</span> feature can only take values <span class="in">`Dirt_Gravel`</span>, <span class="in">`Partial_Pavement`</span>, and <span class="in">`Paved`</span>.</span>
<span id="cb46-94"><a href="#cb46-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-95"><a href="#cb46-95" aria-hidden="true" tabindex="-1"></a>Many machine learning algorithms implementations, such as XGBoost <span class="co">[</span><span class="ot">@chen2016xgboost</span><span class="co">]</span>, cannot handle categorical data and so categorical features must be encoded into numerical variables.</span>
<span id="cb46-96"><a href="#cb46-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-97"><a href="#cb46-97" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-010, echo = TRUE, eval = TRUE, message=FALSE, error=TRUE}</span></span>
<span id="cb46-98"><a href="#cb46-98" aria-hidden="true" tabindex="-1"></a>lrn_xgb <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.xgboost&quot;</span>, <span class="at">nrounds =</span> <span class="dv">100</span>)</span>
<span id="cb46-99"><a href="#cb46-99" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span><span class="fu">train</span>(tsk_ames)</span>
<span id="cb46-100"><a href="#cb46-100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-101"><a href="#cb46-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-102"><a href="#cb46-102" aria-hidden="true" tabindex="-1"></a>Categorical features can be distinguished from one another by their cardinality, which refers to the number of levels they contain.</span>
<span id="cb46-103"><a href="#cb46-103" aria-hidden="true" tabindex="-1"></a>There are three types of categorical features: binary (two levels), low-cardinality, and high-cardinality; there is no universal threshold for when a feature should be considered high-cardinality however one can consider this threshold to be a tunable hyperparameter that can be tuned.</span>
<span id="cb46-104"><a href="#cb46-104" aria-hidden="true" tabindex="-1"></a>For now we will consider high-cardinality to be features with more than 10 levels:</span>
<span id="cb46-105"><a href="#cb46-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-106"><a href="#cb46-106" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb46-107"><a href="#cb46-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-108"><a href="#cb46-108" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb46-109"><a href="#cb46-109" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which</span>(<span class="fu">lengths</span>(tsk_ames<span class="sc">$</span><span class="fu">levels</span>()) <span class="sc">&gt;</span> <span class="dv">10</span>))</span>
<span id="cb46-110"><a href="#cb46-110" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-111"><a href="#cb46-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-112"><a href="#cb46-112" aria-hidden="true" tabindex="-1"></a>Binary features can be trivially encoded by setting one of the feature levels to <span class="in">`1`</span> and the other to <span class="in">`0`</span>.</span>
<span id="cb46-113"><a href="#cb46-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-114"><a href="#cb46-114" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb46-115"><a href="#cb46-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-116"><a href="#cb46-116" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb46-117"><a href="#cb46-117" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which</span>(<span class="fu">lengths</span>(tsk_ames<span class="sc">$</span><span class="fu">levels</span>()) <span class="sc">==</span> <span class="dv">2</span>))</span>
<span id="cb46-118"><a href="#cb46-118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-119"><a href="#cb46-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-120"><a href="#cb46-120" aria-hidden="true" tabindex="-1"></a>Low-cardinality features can be handled by <span class="in">`r index(&#39;one-hot encoding&#39;, aside = TRUE)`</span>.</span>
<span id="cb46-121"><a href="#cb46-121" aria-hidden="true" tabindex="-1"></a>One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature.</span>
<span id="cb46-122"><a href="#cb46-122" aria-hidden="true" tabindex="-1"></a>Theoretically it is sufficient to create one less binary feature than levels, as setting all binary features to zero is also a valid representation.</span>
<span id="cb46-123"><a href="#cb46-123" aria-hidden="true" tabindex="-1"></a>This is typically called dummy or treatment encoding and is required if the learner is a generalized linear (GLM) or additive model (GAM) model.</span>
<span id="cb46-124"><a href="#cb46-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-125"><a href="#cb46-125" aria-hidden="true" tabindex="-1"></a>Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding.</span>
<span id="cb46-126"><a href="#cb46-126" aria-hidden="true" tabindex="-1"></a>Impact encoding is a good approach to handle high-cardinality features.</span>
<span id="cb46-127"><a href="#cb46-127" aria-hidden="true" tabindex="-1"></a><span class="in">`r index(&#39;Impact encoding&#39;, aside = TRUE)`</span> converts categorical features into numeric values based on the impact of the feature on the target.</span>
<span id="cb46-128"><a href="#cb46-128" aria-hidden="true" tabindex="-1"></a>The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature.</span>
<span id="cb46-129"><a href="#cb46-129" aria-hidden="true" tabindex="-1"></a>Impact encoding involves the following steps:</span>
<span id="cb46-130"><a href="#cb46-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-131"><a href="#cb46-131" aria-hidden="true" tabindex="-1"></a>1) Group the target variable by the categorical feature.</span>
<span id="cb46-132"><a href="#cb46-132" aria-hidden="true" tabindex="-1"></a>2) Compute the mean of the target variable for each group.</span>
<span id="cb46-133"><a href="#cb46-133" aria-hidden="true" tabindex="-1"></a>3) Compute the global mean of the target variable.</span>
<span id="cb46-134"><a href="#cb46-134" aria-hidden="true" tabindex="-1"></a>4) Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.</span>
<span id="cb46-135"><a href="#cb46-135" aria-hidden="true" tabindex="-1"></a>5) Replace the categorical feature with the impact scores.</span>
<span id="cb46-136"><a href="#cb46-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-137"><a href="#cb46-137" aria-hidden="true" tabindex="-1"></a>Impact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target.</span>
<span id="cb46-138"><a href="#cb46-138" aria-hidden="true" tabindex="-1"></a>The main advantage, compared to one-hot encoding is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features.</span>
<span id="cb46-139"><a href="#cb46-139" aria-hidden="true" tabindex="-1"></a>As information from the target is used to compute the impact scores, it is crucial that the encoding process is embedded in the cross-validation process to avoid leakage between training and testing data (@sec-performance).</span>
<span id="cb46-140"><a href="#cb46-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-141"><a href="#cb46-141" aria-hidden="true" tabindex="-1"></a>As well as encoding features, another basic preprocessing step is to remove any features that are constant (only have one level and should have been removed as part of EDA).</span>
<span id="cb46-142"><a href="#cb46-142" aria-hidden="true" tabindex="-1"></a>In addition, it may be essential to collapse levels that occur very rarely as these may be missed during resampling, though stratification can be used to mitigate this (@sec-strat-group).</span>
<span id="cb46-143"><a href="#cb46-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-144"><a href="#cb46-144" aria-hidden="true" tabindex="-1"></a>In the code below we use <span class="in">`po(&quot;removeconstants&quot;)`</span> to remove features with only one level, <span class="in">`po(&quot;collapsefactors&quot;)`</span> to collapse levels that occur less than 1% of the time in the data, <span class="in">`po(&quot;encodeimpact&quot;)`</span> to impact encode high-cardinality features, <span class="in">`po(&quot;encode&quot;, method = &quot;one-hot&quot;)`</span> to one-hot encode low-cardinality features, and finally <span class="in">`po(&quot;encode&quot;, method = &quot;treatment&quot;)`</span> to treatment encode binary features.</span>
<span id="cb46-145"><a href="#cb46-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-146"><a href="#cb46-146" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-011, message=FALSE}</span></span>
<span id="cb46-147"><a href="#cb46-147" aria-hidden="true" tabindex="-1"></a>factor_pipeline <span class="ot">=</span></span>
<span id="cb46-148"><a href="#cb46-148" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;removeconstants&quot;</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb46-149"><a href="#cb46-149" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;collapsefactors&quot;</span>, <span class="at">no_collapse_above_prevalence =</span> <span class="fl">0.01</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb46-150"><a href="#cb46-150" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;encodeimpact&quot;</span>, <span class="at">affect_columns =</span> <span class="fu">selector_cardinality_greater_than</span>(<span class="dv">10</span>),</span>
<span id="cb46-151"><a href="#cb46-151" aria-hidden="true" tabindex="-1"></a>        <span class="at">id =</span> <span class="st">&quot;high_card_enc&quot;</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb46-152"><a href="#cb46-152" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;encode&quot;</span>, <span class="at">method =</span> <span class="st">&quot;one-hot&quot;</span>, <span class="at">affect_columns =</span> <span class="fu">selector_cardinality_greater_than</span>(<span class="dv">2</span>),</span>
<span id="cb46-153"><a href="#cb46-153" aria-hidden="true" tabindex="-1"></a>        <span class="at">id =</span> <span class="st">&quot;low_card_enc&quot;</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb46-154"><a href="#cb46-154" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;encode&quot;</span>, <span class="at">method =</span> <span class="st">&quot;treatment&quot;</span>, <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">&quot;factor&quot;</span>),</span>
<span id="cb46-155"><a href="#cb46-155" aria-hidden="true" tabindex="-1"></a>        <span class="at">id =</span> <span class="st">&quot;binary_enc&quot;</span>)</span>
<span id="cb46-156"><a href="#cb46-156" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-157"><a href="#cb46-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-158"><a href="#cb46-158" aria-hidden="true" tabindex="-1"></a>Now we can apply this pipeline to our xgboost model to use it in a benchmark experiment; we also compare a simpler pipeline that only uses one-hot encoding to demonstrate performance difference resulting from different strategies.</span>
<span id="cb46-159"><a href="#cb46-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-160"><a href="#cb46-160" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-013, message=FALSE}</span></span>
<span id="cb46-161"><a href="#cb46-161" aria-hidden="true" tabindex="-1"></a>glrn_xgb_impact <span class="ot">=</span> <span class="fu">as_learner</span>(factor_pipeline <span class="sc">%&gt;&gt;%</span> lrn_xgb)</span>
<span id="cb46-162"><a href="#cb46-162" aria-hidden="true" tabindex="-1"></a>glrn_xgb_impact<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;XGB_enc_impact&quot;</span></span>
<span id="cb46-163"><a href="#cb46-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-164"><a href="#cb46-164" aria-hidden="true" tabindex="-1"></a>glrn_xgb_one_hot <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">&quot;encode&quot;</span>) <span class="sc">%&gt;&gt;%</span> lrn_xgb)</span>
<span id="cb46-165"><a href="#cb46-165" aria-hidden="true" tabindex="-1"></a>glrn_xgb_one_hot<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;XGB_enc_onehot&quot;</span></span>
<span id="cb46-166"><a href="#cb46-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-167"><a href="#cb46-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-168"><a href="#cb46-168" aria-hidden="true" tabindex="-1"></a>learners <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb46-169"><a href="#cb46-169" aria-hidden="true" tabindex="-1"></a>    <span class="at">baseline =</span> lrn_baseline,</span>
<span id="cb46-170"><a href="#cb46-170" aria-hidden="true" tabindex="-1"></a>    <span class="at">xgb_impact =</span> glrn_xgb_impact,</span>
<span id="cb46-171"><a href="#cb46-171" aria-hidden="true" tabindex="-1"></a>    <span class="at">xgb_one_hot =</span> glrn_xgb_one_hot</span>
<span id="cb46-172"><a href="#cb46-172" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-173"><a href="#cb46-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-174"><a href="#cb46-174" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(tsk_ames, learners, rsmp_cv3))</span>
<span id="cb46-175"><a href="#cb46-175" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)[, .(learner_id, regr.mae)]</span>
<span id="cb46-176"><a href="#cb46-176" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-177"><a href="#cb46-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-178"><a href="#cb46-178" aria-hidden="true" tabindex="-1"></a>In this small experiment we see that the difference between the extended factor encoding pipeline and the simpler one-hot encoding strategy pipeline is only moderate.</span>
<span id="cb46-179"><a href="#cb46-179" aria-hidden="true" tabindex="-1"></a>If you are interested in learning more about different encoding strategies, including a benchmark study comparing them, we recommend @pargent2022regularized.</span>
<span id="cb46-180"><a href="#cb46-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-181"><a href="#cb46-181" aria-hidden="true" tabindex="-1"></a><span class="fu">## Missing Values {#sec-preprocessing-missing}</span></span>
<span id="cb46-182"><a href="#cb46-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-183"><a href="#cb46-183" aria-hidden="true" tabindex="-1"></a>A common problem in real-world data is <span class="in">`r index(&#39;missing data&#39;)`</span>.</span>
<span id="cb46-184"><a href="#cb46-184" aria-hidden="true" tabindex="-1"></a>In the Ames dataset, several variables have at least one missing data point:</span>
<span id="cb46-185"><a href="#cb46-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-186"><a href="#cb46-186" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb46-187"><a href="#cb46-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-188"><a href="#cb46-188" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb46-189"><a href="#cb46-189" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">which</span>(tsk_ames<span class="sc">$</span><span class="fu">missings</span>() <span class="sc">&gt;</span> <span class="dv">0</span>)  )</span>
<span id="cb46-190"><a href="#cb46-190" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-191"><a href="#cb46-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-192"><a href="#cb46-192" aria-hidden="true" tabindex="-1"></a>Many learners cannot handle missing values automatically (e.g., <span class="in">`lrn(&quot;regr.ranger&quot;)`</span> and <span class="in">`lrn(&quot;regr.lm&quot;)`</span>), other learners can handle missing values but may use simple methods that may not be ideal (e.g., just omitting rows with missing data).</span>
<span id="cb46-193"><a href="#cb46-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-194"><a href="#cb46-194" aria-hidden="true" tabindex="-1"></a>The simplest <span class="in">`r index(&#39;data imputation&#39;, aside = TRUE)`</span> method is to replace missing values by the feature&#39;s mean (<span class="in">`po(&quot;imputemean&quot;)`</span>) (@fig-imputation), median (<span class="in">`po(&quot;imputemedian&quot;)`</span>), or mode (<span class="in">`po(&quot;imputemode&quot;)`</span>).</span>
<span id="cb46-195"><a href="#cb46-195" aria-hidden="true" tabindex="-1"></a>Alternatively, one can impute by sampling from the empirical distribution of the feature, for example a histogram (<span class="in">`po(&quot;imputehist&quot;)`</span>).</span>
<span id="cb46-196"><a href="#cb46-196" aria-hidden="true" tabindex="-1"></a>Instead of guessing at what a missing feature might be, missing values could instead be replaced by a new level, for example called <span class="in">`.MISSING`</span> (<span class="in">`po(&quot;imputeoor&quot;)`</span>).</span>
<span id="cb46-197"><a href="#cb46-197" aria-hidden="true" tabindex="-1"></a>For numeric features, @ding2010investigation show that for binary classification and tree-based models, encoding missing values out-of-range (OOR), e.g. as two times the largest observed value, is a reasonable approach.</span>
<span id="cb46-198"><a href="#cb46-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-199"><a href="#cb46-199" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb46-200"><a href="#cb46-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-201"><a href="#cb46-201" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb46-202"><a href="#cb46-202" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-imputation</span></span>
<span id="cb46-203"><a href="#cb46-203" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Impute missing values.</span></span>
<span id="cb46-204"><a href="#cb46-204" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Impute missing values.</span></span>
<span id="cb46-205"><a href="#cb46-205" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb46-206"><a href="#cb46-206" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">&quot;Figures/mlr3book_figures-13.svg&quot;</span>, <span class="st">&quot;Figures/mlr3book_figures-13.png&quot;</span>)</span>
<span id="cb46-207"><a href="#cb46-207" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-208"><a href="#cb46-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-209"><a href="#cb46-209" aria-hidden="true" tabindex="-1"></a>It is very important for predictive tasks that you keep track of missing data as it is common for missing data to be informative in itself.</span>
<span id="cb46-210"><a href="#cb46-210" aria-hidden="true" tabindex="-1"></a>As a real-world example, medical data is usually better collected for White communities than racially minoritized ones.</span>
<span id="cb46-211"><a href="#cb46-211" aria-hidden="true" tabindex="-1"></a>Imputing data from minoritized communities would at best mask this data bias, and at worst would make the data bias even worse by making vastly inaccurate assumptions (see @sec-fairness for data bias and algorithmic fairness).</span>
<span id="cb46-212"><a href="#cb46-212" aria-hidden="true" tabindex="-1"></a>Hence, imputation should be tracked by adding binary indicator features (one for each imputed feature) that are <span class="in">`1`</span> if the feature was missing for an observation and <span class="in">`0`</span> if it was present (<span class="in">`po(&quot;missind&quot;)`</span>).</span>
<span id="cb46-213"><a href="#cb46-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-214"><a href="#cb46-214" aria-hidden="true" tabindex="-1"></a>In the code below we create a pipeline form the <span class="in">`r ref(&quot;PipeOp&quot;)`</span>s listed above as well as making use of <span class="in">`po(&quot;featureunion&quot;)`</span> to combine multiple <span class="in">`PipeOp`</span>s acting on the <span class="in">`&quot;integer&quot;`</span> columns.</span>
<span id="cb46-215"><a href="#cb46-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-216"><a href="#cb46-216" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-014, message=FALSE, crop=TRUE}</span></span>
<span id="cb46-217"><a href="#cb46-217" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-impute</span></span>
<span id="cb46-218"><a href="#cb46-218" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Pipeline to impute missing values of numeric features by histogram with binary indicators and missings in categoricals out-of-range with a new level.</span></span>
<span id="cb46-219"><a href="#cb46-219" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: &quot;Flow diagram shows &#39;&lt;INPUT&gt;&#39; with arrows to &#39;missind&#39; and &#39;imputehist&#39;, which both have arrows to &#39;featureunion&#39;, which has an arrow to &#39;imputeoor&#39; that has an arrow to &#39;&lt;OUTPUT&#39;&gt;.&quot;</span></span>
<span id="cb46-220"><a href="#cb46-220" aria-hidden="true" tabindex="-1"></a>impute_hist <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb46-221"><a href="#cb46-221" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;missind&quot;</span>,</span>
<span id="cb46-222"><a href="#cb46-222" aria-hidden="true" tabindex="-1"></a>        <span class="at">type =</span> <span class="st">&quot;integer&quot;</span>,</span>
<span id="cb46-223"><a href="#cb46-223" aria-hidden="true" tabindex="-1"></a>        <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">&quot;integer&quot;</span>)</span>
<span id="cb46-224"><a href="#cb46-224" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb46-225"><a href="#cb46-225" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;imputehist&quot;</span>,</span>
<span id="cb46-226"><a href="#cb46-226" aria-hidden="true" tabindex="-1"></a>        <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">&quot;integer&quot;</span>)</span>
<span id="cb46-227"><a href="#cb46-227" aria-hidden="true" tabindex="-1"></a>    )) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb46-228"><a href="#cb46-228" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;featureunion&quot;</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb46-229"><a href="#cb46-229" aria-hidden="true" tabindex="-1"></a>    <span class="fu">po</span>(<span class="st">&quot;imputeoor&quot;</span>,</span>
<span id="cb46-230"><a href="#cb46-230" aria-hidden="true" tabindex="-1"></a>        <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">&quot;factor&quot;</span>)</span>
<span id="cb46-231"><a href="#cb46-231" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb46-232"><a href="#cb46-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-233"><a href="#cb46-233" aria-hidden="true" tabindex="-1"></a>impute_hist<span class="sc">$</span><span class="fu">plot</span>(<span class="at">horizontal =</span> <span class="cn">TRUE</span>)</span>
<span id="cb46-234"><a href="#cb46-234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-235"><a href="#cb46-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-236"><a href="#cb46-236" aria-hidden="true" tabindex="-1"></a>Using this pipeline we can now run experiments with <span class="in">`lrn(&quot;regr.ranger&quot;)`</span>, which cannot handle missing data; we also compare a simpler pipeline that only uses OOR imputation to demonstrate performance difference resulting from different strategies.</span>
<span id="cb46-237"><a href="#cb46-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-238"><a href="#cb46-238" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-016}</span></span>
<span id="cb46-239"><a href="#cb46-239" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_hist <span class="ot">=</span> <span class="fu">as_learner</span>(impute_hist <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.ranger&quot;</span>))</span>
<span id="cb46-240"><a href="#cb46-240" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_hist<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;RF_imp_Hist&quot;</span></span>
<span id="cb46-241"><a href="#cb46-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-242"><a href="#cb46-242" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_oor <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">&quot;imputeoor&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.ranger&quot;</span>))</span>
<span id="cb46-243"><a href="#cb46-243" aria-hidden="true" tabindex="-1"></a>glrn_rf_impute_oor<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;RF_imp_OOR&quot;</span></span>
<span id="cb46-244"><a href="#cb46-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-245"><a href="#cb46-245" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(tsk_ames, <span class="fu">c</span>(glrn_rf_impute_hist, glrn_rf_impute_oor), rsmp_cv3)</span>
<span id="cb46-246"><a href="#cb46-246" aria-hidden="true" tabindex="-1"></a>bmr_new <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb46-247"><a href="#cb46-247" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">combine</span>(bmr_new)</span>
<span id="cb46-248"><a href="#cb46-248" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)[, .(learner_id, regr.mae)]</span>
<span id="cb46-249"><a href="#cb46-249" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-250"><a href="#cb46-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-251"><a href="#cb46-251" aria-hidden="true" tabindex="-1"></a>Similarly to encoding, we see limited difference in performance between the different imputation strategies.</span>
<span id="cb46-252"><a href="#cb46-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-253"><a href="#cb46-253" aria-hidden="true" tabindex="-1"></a>Many more advanced imputation strategies exist, including model based imputation where machine learning models are used to predict missing values before passing, and multiple imputation where data is repeatedly resampling and imputed in each sample (e.g., by mean imputation) to attain more robust estimates.</span>
<span id="cb46-254"><a href="#cb46-254" aria-hidden="true" tabindex="-1"></a>However, these more advanced techniques rarely improve the model substantially and the simple imputation techniques introduced above are usually sufficient <span class="co">[</span><span class="ot">@Poulos2018</span><span class="co">]</span>.</span>
<span id="cb46-255"><a href="#cb46-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-256"><a href="#cb46-256" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pipeline Robustify {#sec-prepro-robustify}</span></span>
<span id="cb46-257"><a href="#cb46-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-258"><a href="#cb46-258" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3pipelines`</span> offers a simple and reusable pipeline for (among other things) imputation and factor encoding called <span class="in">`ppl(&quot;robustify&quot;)`</span>, which includes sensible defaults that can be used most of the time when encoding or imputing data.</span>
<span id="cb46-259"><a href="#cb46-259" aria-hidden="true" tabindex="-1"></a>The pipeline includes the following <span class="in">`PipeOp`</span>s:</span>
<span id="cb46-260"><a href="#cb46-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-261"><a href="#cb46-261" aria-hidden="true" tabindex="-1"></a>1) <span class="in">`po(&quot;removeconstants&quot;)`</span> -- Constant features are removed.</span>
<span id="cb46-262"><a href="#cb46-262" aria-hidden="true" tabindex="-1"></a>2) <span class="in">`po(&quot;colapply&quot;)`</span> -- Character and ordinal features are encoded as categorical, and date/time features are encoded as numeric.</span>
<span id="cb46-263"><a href="#cb46-263" aria-hidden="true" tabindex="-1"></a>3) <span class="in">`po(&quot;imputehist&quot;)`</span> -- Numeric features are imputed by histogram.</span>
<span id="cb46-264"><a href="#cb46-264" aria-hidden="true" tabindex="-1"></a>4) <span class="in">`po(&quot;imputesample&quot;)`</span> -- Logical features are imputed by sampling from the empirical distribution.</span>
<span id="cb46-265"><a href="#cb46-265" aria-hidden="true" tabindex="-1"></a>5) <span class="in">`po(&quot;missind&quot;)`</span> -- Missing data indicators are added for imputed numeric and logical variables</span>
<span id="cb46-266"><a href="#cb46-266" aria-hidden="true" tabindex="-1"></a>6) <span class="in">`po(&quot;imputeoor&quot;)`</span> -- Missing values of categorical features are encoded with a new level</span>
<span id="cb46-267"><a href="#cb46-267" aria-hidden="true" tabindex="-1"></a>7) <span class="in">`po(&quot;fixfactors&quot;)`</span> -- Fixes levels of categorical features such that the same levels are present during prediction and training (which may involve dropping empty factor levels)</span>
<span id="cb46-268"><a href="#cb46-268" aria-hidden="true" tabindex="-1"></a>8) <span class="in">`po(&quot;imputesample&quot;)`</span> -- Missing values in categorical features introduced from dropping levels in the previous step are imputed by sampling from the empirical distributions.</span>
<span id="cb46-269"><a href="#cb46-269" aria-hidden="true" tabindex="-1"></a>9) <span class="in">`po(&quot;collapsefactors&quot;)`</span> -- Categorical features levels are collapsed (starting from the rarest factors in the training data) until there are less than 1000 levels</span>
<span id="cb46-270"><a href="#cb46-270" aria-hidden="true" tabindex="-1"></a>10) <span class="in">`po(&quot;encode&quot;)`</span> -- Categorical features are one-hot encoded</span>
<span id="cb46-271"><a href="#cb46-271" aria-hidden="true" tabindex="-1"></a>11) <span class="in">`po(&quot;removeconstants&quot;)`</span> -- Constant features that might have been created in the previous steps are removed</span>
<span id="cb46-272"><a href="#cb46-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-273"><a href="#cb46-273" aria-hidden="true" tabindex="-1"></a>Linear regression is a simple model that cannot handle most problems that we may face when processing data, but with the <span class="in">`ppl(&quot;robustify&quot;)`</span> pipeline we can now include it in our experiment:</span>
<span id="cb46-274"><a href="#cb46-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-275"><a href="#cb46-275" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-019, warning = FALSE}</span></span>
<span id="cb46-276"><a href="#cb46-276" aria-hidden="true" tabindex="-1"></a>glrn_lm_robust <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">ppl</span>(<span class="st">&quot;robustify&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.lm&quot;</span>))</span>
<span id="cb46-277"><a href="#cb46-277" aria-hidden="true" tabindex="-1"></a>glrn_lm_robust<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;lm_robust&quot;</span></span>
<span id="cb46-278"><a href="#cb46-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-279"><a href="#cb46-279" aria-hidden="true" tabindex="-1"></a>bmr_new <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(tsk_ames, glrn_lm_robust,  rsmp_cv3))</span>
<span id="cb46-280"><a href="#cb46-280" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">combine</span>(bmr_new)</span>
<span id="cb46-281"><a href="#cb46-281" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)[, .(learner_id, regr.mae)]</span>
<span id="cb46-282"><a href="#cb46-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-283"><a href="#cb46-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-284"><a href="#cb46-284" aria-hidden="true" tabindex="-1"></a>Robustifying the linear regression results in a model that vastly outperforms the featureless baseline and is competitive when compared to more complex machine learning models.</span>
<span id="cb46-285"><a href="#cb46-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-286"><a href="#cb46-286" aria-hidden="true" tabindex="-1"></a><span class="fu">## Scaling Features and Targets {#sec-prepro-scale}</span></span>
<span id="cb46-287"><a href="#cb46-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-288"><a href="#cb46-288" aria-hidden="true" tabindex="-1"></a>Simple transformations of features and the target can be beneficial (and sometimes essential) for certain learners.</span>
<span id="cb46-289"><a href="#cb46-289" aria-hidden="true" tabindex="-1"></a>In particular, log transformation of the target can help in making the distribution more symmetrical and can help reduce the impact of outliers; this is particularly important for algorithms that assume the target is normally distributed.</span>
<span id="cb46-290"><a href="#cb46-290" aria-hidden="true" tabindex="-1"></a>Similarly, log transformation of skewed features can help to reduce the influence of outliers.</span>
<span id="cb46-291"><a href="#cb46-291" aria-hidden="true" tabindex="-1"></a>In @fig-sale we plot the distribution of the target in the <span class="in">`ames`</span> dataset and then the log-transformed target, we can see how simply taking the log of the variable results in a distribution that is much more symmetrical and with fewer outliers.</span>
<span id="cb46-292"><a href="#cb46-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-293"><a href="#cb46-293" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-001, echo = TRUE, eval = TRUE, message=FALSE}</span></span>
<span id="cb46-294"><a href="#cb46-294" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-sale</span></span>
<span id="cb46-295"><a href="#cb46-295" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Distribution of house sales prices (in USD) in the ames dataset before (left) and after (right) log transformation. Before transformation there is a skewed distribution of prices towards cheaper properties with a few outliers of very expensive properties. After transformation the distribution is much more symmetrical with the majority of points evenly spread around the same range.</span></span>
<span id="cb46-296"><a href="#cb46-296" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Two boxplots. Left plot shows house prices up to $600,000, the majority of prices are between roughly $100,000-$200,000. Right plot shows log house prices primarily around 12 with an even range between 11 and 13 and a few outliers on both sides.</span></span>
<span id="cb46-297"><a href="#cb46-297" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb46-298"><a href="#cb46-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-299"><a href="#cb46-299" aria-hidden="true" tabindex="-1"></a><span class="co"># copy ames data</span></span>
<span id="cb46-300"><a href="#cb46-300" aria-hidden="true" tabindex="-1"></a>log_ames <span class="ot">=</span> <span class="fu">copy</span>(ames)</span>
<span id="cb46-301"><a href="#cb46-301" aria-hidden="true" tabindex="-1"></a><span class="co"># log transform target</span></span>
<span id="cb46-302"><a href="#cb46-302" aria-hidden="true" tabindex="-1"></a>log_ames[, logSalePrice <span class="sc">:</span><span class="er">=</span> <span class="fu">log</span>(Sale_Price)]</span>
<span id="cb46-303"><a href="#cb46-303" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb46-304"><a href="#cb46-304" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(<span class="fu">as_task_regr</span>(log_ames, <span class="at">target =</span> <span class="st">&quot;Sale_Price&quot;</span>)) <span class="sc">+</span></span>
<span id="cb46-305"><a href="#cb46-305" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>(<span class="fu">as_task_regr</span>(log_ames, <span class="at">target =</span> <span class="st">&quot;logSalePrice&quot;</span>))</span>
<span id="cb46-306"><a href="#cb46-306" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-307"><a href="#cb46-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-308"><a href="#cb46-308" aria-hidden="true" tabindex="-1"></a>Normalization of features may also be necessary to ensure features with a larger scale do not have a higher impact, which is especially important for distance based methods such as K-nearest neighbor models or regularized parametric models such as Lasso or Elastic net.</span>
<span id="cb46-309"><a href="#cb46-309" aria-hidden="true" tabindex="-1"></a>Many models internally scale the data if required by the algorithm so most of the time we do not need to manually do this in preprocessing, though if this is required then <span class="in">`po(&quot;scale&quot;)`</span> can be used to center and scale numeric features.</span>
<span id="cb46-310"><a href="#cb46-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-311"><a href="#cb46-311" aria-hidden="true" tabindex="-1"></a>Any transformations applied to the target during training must be inverted during model prediction to ensure predictions are made on the correct scale.</span>
<span id="cb46-312"><a href="#cb46-312" aria-hidden="true" tabindex="-1"></a>By example, say we are interested in log transforming the target, then we would take the following steps:</span>
<span id="cb46-313"><a href="#cb46-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-314"><a href="#cb46-314" aria-hidden="true" tabindex="-1"></a>quarto-executable-code-5450563D</span>
<span id="cb46-315"><a href="#cb46-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-316"><a href="#cb46-316" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb46-317"><a href="#cb46-317" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">5</span>), <span class="at">y =</span> <span class="fu">runif</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>))</span>
<span id="cb46-318"><a href="#cb46-318" aria-hidden="true" tabindex="-1"></a>df</span>
<span id="cb46-319"><a href="#cb46-319" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. log transform the target</span></span>
<span id="cb46-320"><a href="#cb46-320" aria-hidden="true" tabindex="-1"></a>df[, y <span class="sc">:</span><span class="er">=</span> <span class="fu">log</span>(y)]</span>
<span id="cb46-321"><a href="#cb46-321" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>y</span>
<span id="cb46-322"><a href="#cb46-322" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. make linear regression predictions</span></span>
<span id="cb46-323"><a href="#cb46-323" aria-hidden="true" tabindex="-1"></a><span class="co">#    predictions on the log-transformed scale</span></span>
<span id="cb46-324"><a href="#cb46-324" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">=</span> <span class="fu">predict</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x, df), df)</span>
<span id="cb46-325"><a href="#cb46-325" aria-hidden="true" tabindex="-1"></a>yhat</span>
<span id="cb46-326"><a href="#cb46-326" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. transform to correct scale with inverse of log function</span></span>
<span id="cb46-327"><a href="#cb46-327" aria-hidden="true" tabindex="-1"></a><span class="co">#    predictions on the original scale</span></span>
<span id="cb46-328"><a href="#cb46-328" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(yhat)</span>
<span id="cb46-329"><a href="#cb46-329" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-330"><a href="#cb46-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-331"><a href="#cb46-331" aria-hidden="true" tabindex="-1"></a>In this simple experiment we could manually transform and invert the target, however this is much more complex when dealing with resampling and benchmarking experiments and so the pipeline <span class="in">`ppl(&quot;targettrafo&quot;)`</span> will do this heavy lifting for you.</span>
<span id="cb46-332"><a href="#cb46-332" aria-hidden="true" tabindex="-1"></a>The pipeline includes a parameter <span class="in">`targetmutate.trafo`</span> for the transformation to be applied during training to the target, as well as <span class="in">`targetmutate.inverter`</span> for the transformation to be applied to invert the original transformation during prediction.</span>
<span id="cb46-333"><a href="#cb46-333" aria-hidden="true" tabindex="-1"></a>So now let us consider the log transformation by adding this pipeline to our robust linear regression model:</span>
<span id="cb46-334"><a href="#cb46-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-335"><a href="#cb46-335" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-020, warning=FALSE}</span></span>
<span id="cb46-336"><a href="#cb46-336" aria-hidden="true" tabindex="-1"></a>glrn_log_lm_robust <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">ppl</span>(<span class="st">&quot;targettrafo&quot;</span>, <span class="at">graph =</span> glrn_lm_robust,</span>
<span id="cb46-337"><a href="#cb46-337" aria-hidden="true" tabindex="-1"></a>  <span class="at">targetmutate.trafo =</span> <span class="cf">function</span>(x)<span class="fu">log</span>(x),</span>
<span id="cb46-338"><a href="#cb46-338" aria-hidden="true" tabindex="-1"></a>  <span class="at">targetmutate.inverter =</span> <span class="cf">function</span>(x) <span class="fu">list</span>(<span class="at">response =</span> <span class="fu">exp</span>(x<span class="sc">$</span>response))))</span>
<span id="cb46-339"><a href="#cb46-339" aria-hidden="true" tabindex="-1"></a>glrn_log_lm_robust<span class="sc">$</span>id <span class="ot">=</span> <span class="st">&quot;lm_robust_logtrafo&quot;</span></span>
<span id="cb46-340"><a href="#cb46-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-341"><a href="#cb46-341" aria-hidden="true" tabindex="-1"></a>bmr_new <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(tsk_ames, glrn_log_lm_robust, rsmp_cv3))</span>
<span id="cb46-342"><a href="#cb46-342" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">combine</span>(bmr_new)</span>
<span id="cb46-343"><a href="#cb46-343" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)[, .(learner_id, regr.mae)]</span>
<span id="cb46-344"><a href="#cb46-344" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-345"><a href="#cb46-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-346"><a href="#cb46-346" aria-hidden="true" tabindex="-1"></a>With the target transformation and the <span class="in">`ppl(&quot;robustify&quot;)`</span> pipeline, the simple linear regression now appears to be the best performing model.</span>
<span id="cb46-347"><a href="#cb46-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-348"><a href="#cb46-348" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Extraction</span></span>
<span id="cb46-349"><a href="#cb46-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-350"><a href="#cb46-350" aria-hidden="true" tabindex="-1"></a>As a final step of data preprocessing we will look at <span class="in">`r index(&#39;feature extraction&#39;)`</span>.</span>
<span id="cb46-351"><a href="#cb46-351" aria-hidden="true" tabindex="-1"></a>In @sec-feature-selection we look at automated feature selection and how automated approaches with filters and wrappers can be used to reduce a dataset to the optimal set of features.</span>
<span id="cb46-352"><a href="#cb46-352" aria-hidden="true" tabindex="-1"></a>Feature extraction differs from this process as we are now interested in features that are highlight dependent on one another and all together may provide useful information but not individually.</span>
<span id="cb46-353"><a href="#cb46-353" aria-hidden="true" tabindex="-1"></a>As a concrete example, consider the power consumption of kitchen appliances in houses in the Ames dataset.</span>
<span id="cb46-354"><a href="#cb46-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-355"><a href="#cb46-355" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-023, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}</span></span>
<span id="cb46-356"><a href="#cb46-356" aria-hidden="true" tabindex="-1"></a>repo <span class="ot">=</span> <span class="st">&quot;ja-thomas/extend_ames_housing/main/data/energy_usage.csv&quot;</span></span>
<span id="cb46-357"><a href="#cb46-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-358"><a href="#cb46-358" aria-hidden="true" tabindex="-1"></a>energy_data <span class="ot">=</span> <span class="fu">fread</span>(<span class="fu">paste0</span>(<span class="st">&quot;https://raw.githubusercontent.com/&quot;</span>, repo),</span>
<span id="cb46-359"><a href="#cb46-359" aria-hidden="true" tabindex="-1"></a>    <span class="at">stringsAsFactors =</span> <span class="cn">TRUE</span></span>
<span id="cb46-360"><a href="#cb46-360" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-361"><a href="#cb46-361" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-362"><a href="#cb46-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-363"><a href="#cb46-363" aria-hidden="true" tabindex="-1"></a>In this dataset, each row of represents one house and each feature is the total power consumption from kitchen appliances at a given time <span class="co">[</span><span class="ot">@bagnall2017great</span><span class="co">]</span>.</span>
<span id="cb46-364"><a href="#cb46-364" aria-hidden="true" tabindex="-1"></a>The consumption is measured in 2-minute intervals, resulting in 720 features.</span>
<span id="cb46-365"><a href="#cb46-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-366"><a href="#cb46-366" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-024, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}</span></span>
<span id="cb46-367"><a href="#cb46-367" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-energy</span></span>
<span id="cb46-368"><a href="#cb46-368" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Energy consumption of one example house in a day, recorded in 2-minute intervals.</span></span>
<span id="cb46-369"><a href="#cb46-369" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Line plot with &#39;2-Minute Interval&#39; on axis ranging from 1 to 720 and &#39;Power Consumption&#39; on y-axis ranging from 0 to 20. There are spikes at around (200, 20), (300, 20), and then some consistently raised usage between (500-700, 3).</span></span>
<span id="cb46-370"><a href="#cb46-370" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb46-371"><a href="#cb46-371" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">y =</span> <span class="fu">as.numeric</span>(energy_data[<span class="dv">1</span>, ])), <span class="fu">aes</span>(<span class="at">y =</span> y, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">720</span>)) <span class="sc">+</span></span>
<span id="cb46-372"><a href="#cb46-372" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb46-373"><a href="#cb46-373" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;2-Minute Interval&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Power Consumption&quot;</span>)</span>
<span id="cb46-374"><a href="#cb46-374" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-375"><a href="#cb46-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-376"><a href="#cb46-376" aria-hidden="true" tabindex="-1"></a>Adding these 720 features to our full dataset is a bad idea as each individual feature does not provide meaningful information, similarly we cannot automate selection of the best feature subset for the same reason.</span>
<span id="cb46-377"><a href="#cb46-377" aria-hidden="true" tabindex="-1"></a>Instead we can *extract* information about the curves to gain insights into the kitchen&#39;s overall energy usage.</span>
<span id="cb46-378"><a href="#cb46-378" aria-hidden="true" tabindex="-1"></a>For example, we could extract the maximum used wattage, overall used wattage, number of peaks, and other similar features.</span>
<span id="cb46-379"><a href="#cb46-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-380"><a href="#cb46-380" aria-hidden="true" tabindex="-1"></a>To extract features we will write our own <span class="in">`r ref(&quot;PipeOp&quot;)`</span> that inherits from `<span class="in">`r ref(&quot;PipeOpTaskPreprocSimple&quot;)`</span>.</span>
<span id="cb46-381"><a href="#cb46-381" aria-hidden="true" tabindex="-1"></a>To do this we simply add a private method called <span class="in">`.transform_dt`</span> that hardcodes the operations on our task.</span>
<span id="cb46-382"><a href="#cb46-382" aria-hidden="true" tabindex="-1"></a>In this example we select the functional features (which all start with &quot;att&quot;), extract the mean, minimum, maximum, and variance of the power consumption, and then remove the functional features.</span>
<span id="cb46-383"><a href="#cb46-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-384"><a href="#cb46-384" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-025}</span></span>
<span id="cb46-385"><a href="#cb46-385" aria-hidden="true" tabindex="-1"></a>PipeOpFuncExtract <span class="ot">=</span> R6<span class="sc">::</span><span class="fu">R6Class</span>(<span class="st">&quot;PipeOpFuncExtract&quot;</span>,</span>
<span id="cb46-386"><a href="#cb46-386" aria-hidden="true" tabindex="-1"></a>  <span class="at">inherit =</span> mlr3pipelines<span class="sc">::</span>PipeOpTaskPreprocSimple,</span>
<span id="cb46-387"><a href="#cb46-387" aria-hidden="true" tabindex="-1"></a>  <span class="at">private =</span> <span class="fu">list</span>(</span>
<span id="cb46-388"><a href="#cb46-388" aria-hidden="true" tabindex="-1"></a>    <span class="at">.transform_dt =</span> <span class="cf">function</span>(dt, levels) {</span>
<span id="cb46-389"><a href="#cb46-389" aria-hidden="true" tabindex="-1"></a>        ffeat_names <span class="ot">=</span> <span class="fu">paste0</span>(<span class="st">&quot;att&quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">720</span>)</span>
<span id="cb46-390"><a href="#cb46-390" aria-hidden="true" tabindex="-1"></a>        ffeats <span class="ot">=</span> dt[, ..ffeat_names]</span>
<span id="cb46-391"><a href="#cb46-391" aria-hidden="true" tabindex="-1"></a>        dt[, energy_means <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, mean)]</span>
<span id="cb46-392"><a href="#cb46-392" aria-hidden="true" tabindex="-1"></a>        dt[, energy_mins <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, min)]</span>
<span id="cb46-393"><a href="#cb46-393" aria-hidden="true" tabindex="-1"></a>        dt[, energy_maxs <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, max)]</span>
<span id="cb46-394"><a href="#cb46-394" aria-hidden="true" tabindex="-1"></a>        dt[, energy_vars <span class="sc">:</span><span class="er">=</span> <span class="fu">apply</span>(ffeats, <span class="dv">1</span>, var)]</span>
<span id="cb46-395"><a href="#cb46-395" aria-hidden="true" tabindex="-1"></a>        dt[, (ffeat_names) <span class="sc">:</span><span class="er">=</span> <span class="cn">NULL</span>]</span>
<span id="cb46-396"><a href="#cb46-396" aria-hidden="true" tabindex="-1"></a>        dt</span>
<span id="cb46-397"><a href="#cb46-397" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb46-398"><a href="#cb46-398" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb46-399"><a href="#cb46-399" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-400"><a href="#cb46-400" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-401"><a href="#cb46-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-402"><a href="#cb46-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-403"><a href="#cb46-403" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-003}</span></span>
<span id="cb46-404"><a href="#cb46-404" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-functional-features</span></span>
<span id="cb46-405"><a href="#cb46-405" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Functional features.</span></span>
<span id="cb46-406"><a href="#cb46-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Functional features.</span></span>
<span id="cb46-407"><a href="#cb46-407" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb46-408"><a href="#cb46-408" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">&quot;Figures/mlr3book_figures-14.svg&quot;</span>, <span class="st">&quot;Figures/mlr3book_figures-14.png&quot;</span>)</span>
<span id="cb46-409"><a href="#cb46-409" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-410"><a href="#cb46-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-411"><a href="#cb46-411" aria-hidden="true" tabindex="-1"></a>Before using this in an experiment we first test that the <span class="in">`PipeOp`</span> works as expected.</span>
<span id="cb46-412"><a href="#cb46-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-413"><a href="#cb46-413" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-026}</span></span>
<span id="cb46-414"><a href="#cb46-414" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext <span class="ot">=</span> <span class="fu">cbind</span>(ames, energy_data)</span>
<span id="cb46-415"><a href="#cb46-415" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext <span class="ot">=</span> <span class="fu">as_task_regr</span>(tsk_ames_ext, <span class="st">&quot;Sale_Price&quot;</span>, <span class="st">&quot;ames_ext&quot;</span>)</span>
<span id="cb46-416"><a href="#cb46-416" aria-hidden="true" tabindex="-1"></a><span class="co"># remove the redundant variables identified at the start of this chapter</span></span>
<span id="cb46-417"><a href="#cb46-417" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext<span class="sc">$</span><span class="fu">select</span>(<span class="fu">setdiff</span>(tsk_ames_ext<span class="sc">$</span>feature_names, to_remove))</span>
<span id="cb46-418"><a href="#cb46-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-419"><a href="#cb46-419" aria-hidden="true" tabindex="-1"></a>func_extractor <span class="ot">=</span> PipeOpFuncExtract<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;energy_extract&quot;</span>)</span>
<span id="cb46-420"><a href="#cb46-420" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext <span class="ot">=</span> func_extractor<span class="sc">$</span><span class="fu">train</span>(<span class="fu">list</span>(tsk_ames_ext))[[<span class="dv">1</span>]]</span>
<span id="cb46-421"><a href="#cb46-421" aria-hidden="true" tabindex="-1"></a>tsk_ames_ext<span class="sc">$</span><span class="fu">data</span>(<span class="dv">1</span>, <span class="fu">c</span>(<span class="st">&quot;energy_means&quot;</span>, <span class="st">&quot;energy_mins&quot;</span>, <span class="st">&quot;energy_maxs&quot;</span>, <span class="st">&quot;energy_vars&quot;</span>))</span>
<span id="cb46-422"><a href="#cb46-422" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-423"><a href="#cb46-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-424"><a href="#cb46-424" aria-hidden="true" tabindex="-1"></a>These outputs look sensible compared to @fig-energy so we can now run our final benchmark experiment using feature extraction.</span>
<span id="cb46-425"><a href="#cb46-425" aria-hidden="true" tabindex="-1"></a>We do not need to add the <span class="in">`PipeOp`</span> to each learner as we can apply it once (as above) before any model training by applying it to all available data.</span>
<span id="cb46-426"><a href="#cb46-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-427"><a href="#cb46-427" aria-hidden="true" tabindex="-1"></a><span class="in">```{r preprocessing-027, warning=FALSE}</span></span>
<span id="cb46-428"><a href="#cb46-428" aria-hidden="true" tabindex="-1"></a>learners <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb46-429"><a href="#cb46-429" aria-hidden="true" tabindex="-1"></a>    <span class="at">baseline =</span> lrn_baseline,</span>
<span id="cb46-430"><a href="#cb46-430" aria-hidden="true" tabindex="-1"></a>    <span class="at">tree =</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.rpart&quot;</span>),</span>
<span id="cb46-431"><a href="#cb46-431" aria-hidden="true" tabindex="-1"></a>    <span class="at">xgb_impact =</span> glrn_xgb_impact,</span>
<span id="cb46-432"><a href="#cb46-432" aria-hidden="true" tabindex="-1"></a>    <span class="at">rf_impute_oor =</span> glrn_rf_impute_oor,</span>
<span id="cb46-433"><a href="#cb46-433" aria-hidden="true" tabindex="-1"></a>    <span class="at">lm_robust =</span> glrn_lm_robust,</span>
<span id="cb46-434"><a href="#cb46-434" aria-hidden="true" tabindex="-1"></a>    <span class="at">log_lm_robust =</span> glrn_log_lm_robust</span>
<span id="cb46-435"><a href="#cb46-435" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-436"><a href="#cb46-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-437"><a href="#cb46-437" aria-hidden="true" tabindex="-1"></a>bmr_final <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(<span class="fu">c</span>(tsk_ames_ext, tsk_ames), learners, rsmp_cv3))</span>
<span id="cb46-438"><a href="#cb46-438" aria-hidden="true" tabindex="-1"></a>bmr_final<span class="sc">$</span><span class="fu">aggregate</span>(<span class="at">measure =</span> measure)</span>
<span id="cb46-439"><a href="#cb46-439" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb46-440"><a href="#cb46-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-441"><a href="#cb46-441" aria-hidden="true" tabindex="-1"></a>The final results indicate that adding these extracted features improved the performance of all models (except the featureless baseline).</span>
<span id="cb46-442"><a href="#cb46-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-443"><a href="#cb46-443" aria-hidden="true" tabindex="-1"></a>In this example, we could have just applied the transformations to the dataset directly.</span>
<span id="cb46-444"><a href="#cb46-444" aria-hidden="true" tabindex="-1"></a>The advantage of using the <span class="in">`PipeOp`</span> is that we could have chained it to a subset of learners to prevent a blow-up of experiments in the benchmark experiment.</span>
<span id="cb46-445"><a href="#cb46-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-446"><a href="#cb46-446" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb46-447"><a href="#cb46-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-448"><a href="#cb46-448" aria-hidden="true" tabindex="-1"></a>In this chapter we built on everything learnt in @sec-pipelines and @sec-pipelines-nonseq to look at concrete usage of pipelines for data preprocessing.</span>
<span id="cb46-449"><a href="#cb46-449" aria-hidden="true" tabindex="-1"></a>We focused primarily on feature engineering, which can make use of <span class="in">`r mlr3pipelines`</span> to automate preprocessing as much as possible whilst still ensuring user control.</span>
<span id="cb46-450"><a href="#cb46-450" aria-hidden="true" tabindex="-1"></a>We looked at factor encoding for categorical variables, imputing missing data, scaling variables, and feature extraction.</span>
<span id="cb46-451"><a href="#cb46-451" aria-hidden="true" tabindex="-1"></a>Preprocessing is almost always required in machine learning experiments, and applying the <span class="in">`ppl(&quot;robustify&quot;)`</span> pipeline will help in many cases to simplify this process by applying the most common preprocessing steps, we will see this in use in @sec-large-benchmarking.</span>
<span id="cb46-452"><a href="#cb46-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-453"><a href="#cb46-453" aria-hidden="true" tabindex="-1"></a>We have not introduced any new classes here so in @tbl-prepro-api we list the <span class="in">`r ref(&quot;PipeOp&quot;)`</span>s and <span class="in">`r ref(&quot;Graph&quot;)`</span>s discussed in this chapter.</span>
<span id="cb46-454"><a href="#cb46-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-455"><a href="#cb46-455" aria-hidden="true" tabindex="-1"></a>| PipeOp/Graph | Description |</span>
<span id="cb46-456"><a href="#cb46-456" aria-hidden="true" tabindex="-1"></a>| --- | -- |</span>
<span id="cb46-457"><a href="#cb46-457" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref(&quot;PipeOpRemoveConstants&quot;)`</span> | Remove variables consisting of one value |</span>
<span id="cb46-458"><a href="#cb46-458" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref(&quot;PipeOpCollapseFactors&quot;)`</span> | Combine rare factor levels |</span>
<span id="cb46-459"><a href="#cb46-459" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref(&quot;PipeOpEncodeImpact&quot;)`</span> | Impact encoding |</span>
<span id="cb46-460"><a href="#cb46-460" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref(&quot;PipeOpEncode&quot;)`</span> | Other factor encoding methods |</span>
<span id="cb46-461"><a href="#cb46-461" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref(&quot;PipeOpMissInd&quot;)`</span> | Add an indicator column to track missing data |</span>
<span id="cb46-462"><a href="#cb46-462" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref(&quot;PipeOpImputeHist&quot;)`</span> | Impute missing data by sampling from a histogram |</span>
<span id="cb46-463"><a href="#cb46-463" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref(&quot;PipeOpImputeOOR&quot;)`</span> | Impute missing data with out-of-range values |</span>
<span id="cb46-464"><a href="#cb46-464" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref(&quot;pipeline_robustify&quot;)`</span> | Graph with common imputation and encoding methods |</span>
<span id="cb46-465"><a href="#cb46-465" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref(&quot;pipeline_targettrafo&quot;)`</span> | Graph to transform target during training and invert transformation during prediction |</span>
<span id="cb46-466"><a href="#cb46-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-467"><a href="#cb46-467" aria-hidden="true" tabindex="-1"></a>: <span class="in">`PipeOp`</span>s and <span class="in">`Graph`</span>s discussed in this chapter. {#tbl-prepro-api}</span>
<span id="cb46-468"><a href="#cb46-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-469"><a href="#cb46-469" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb46-470"><a href="#cb46-470" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">FIXME</span><span class="co">: ADD EXERCISES --&gt;</span></span>
<span id="cb46-471"><a href="#cb46-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-472"><a href="#cb46-472" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format=&quot;html&quot;}</span>
<span id="cb46-473"><a href="#cb46-473" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb46-474"><a href="#cb46-474" aria-hidden="true" tabindex="-1"></a>:::</span></code></pre></div>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-bagnall2017great" class="csl-entry" role="listitem">
Bagnall, Anthony, Jason Lines, Aaron Bostrom, James Large, and Eamonn Keogh. 2017. <span>“The Great Time Series Classification Bake Off: A Review and Experimental Evaluation of Recent Algorithmic Advances.”</span> <em>Data Mining and Knowledge Discovery</em> 31: 606–60.
</div>
<div id="ref-chen2016xgboost" class="csl-entry" role="listitem">
Chen, Tianqi, and Carlos Guestrin. 2016. <span>“Xgboost: A Scalable Tree Boosting System.”</span> In <em>Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 785–94.
</div>
<div id="ref-de2011ames" class="csl-entry" role="listitem">
De Cock, Dean. 2011. <span>“Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.”</span> <em>Journal of Statistics Education</em> 19 (3).
</div>
<div id="ref-ding2010investigation" class="csl-entry" role="listitem">
Ding, Yufeng, and Jeffrey S Simonoff. 2010. <span>“An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data.”</span> <em>Journal of Machine Learning Research</em> 11 (1).
</div>
<div id="ref-pargent2022regularized" class="csl-entry" role="listitem">
Pargent, Florian, Florian Pfisterer, Janek Thomas, and Bernd Bischl. 2022. <span>“Regularized Target Encoding Outperforms Traditional Methods in Supervised Machine Learning with High Cardinality Features.”</span> <em>Computational Statistics</em> 37 (5): 2671–92.
</div>
<div id="ref-Poulos2018" class="csl-entry" role="listitem">
Poulos, Jason, and Rafael Valle. 2018. <span>“Missing Data Imputation for Supervised Learning.”</span> <em>Applied Artificial Intelligence</em> 32 (2): 186–96. <a href="https://doi.org/10.1080/08839514.2018.1448143">https://doi.org/10.1080/08839514.2018.1448143</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a  href="/chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class='chapter-number'>8</span>  <span class='chapter-title'>Non-sequential Pipelines and Tuning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a  href="/chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="pagination-link">
        <span class="nav-page-text"><span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <div class='footer-contents'>All content licensed under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) <br> &copy; Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.
</div>  
    </div>   
    <div class="nav-footer-center">
      <div class='footer-contents'>[Website](https://mlr-org.com) | [GitHub](https://github.com/mlr-org/mlr3book) | [Gallery](https://mlr-org.com/gallery) | [Mattermost](https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/)</div>  
    </div>
    <div class="nav-footer-right">
      <div class='footer-contents'>Built with [Quarto](https://quarto.org/).</div>  
    </div>
  </div>
</footer>

</body>

</html>