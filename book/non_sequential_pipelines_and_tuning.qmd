# Non-sequential Pipelines and Tuning {#sec-pipelines-nonseq}

{{< include _setup.qmd >}}

`r chapter = "Non-sequential Pipelines and Tuning"`
`r authors(chapter)`

```{r pipelines-setup, include = FALSE, cache = FALSE}
library("mlr3oml")
dir.create(here::here("book", "openml"), showWarnings = FALSE, recursive = TRUE)
options(mlr3oml.cache = here::here("book", "openml", "cache"))
```

In @sec-pipelines we looked at simple sequential pipelines that can be built using the `Graph` class and a few `PipeOp` objects.
In this chapter we will take this further and look at non-sequential pipelines that can perform more complex operations.
We will then look at tuning pipelines by combining methods in `r mlr3tuning` and `r mlr3pipelines` and will consider some concrete examples using multi-fidelity tuning (@sec-hyperband) and feature selection (@sec-feature-selection).

We saw the power of the `%>>%`-operator in @sec-pipelines to assemble graphs from combinations of multiple `PipeOp`s and `Learner`s.
Given a single `PipeOp` or `Learner`, the `%>>%`-operator will arrange these objects into a linear `Graph` with each `PipeOp` acting in sequence.
However, by using the `r ref("gunion()")` function, we can instead combine multiple `PipeOp`s, `Graph`s, or a mixture of both, into a parallel `Graph`.
In the following example, we create a `Graph` that centers its inputs and then copies the scaled data to two parallel streams: one replaces the data with columns that indicate whether data is missing, the other imputes missing data using the median (we will return to this in @sec-preprocessing-missing).
The outputs of both streams are then combined into a single dataset using `r ref("PipeOpFeatureUnion")`.

```{r 05-pipelines-modeling-003-evalF, eval = FALSE}
library(mlr3pipelines)

gr = po("scale", center = TRUE, scale = FALSE) %>>%
  gunion(list(
    po("missind"),
    po("imputemedian")
  )) %>>%
  po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-modeling-003-evalT, fig.width = 8, eval = TRUE, echo = FALSE}
#| label: fig-pipelines-parallel-plot
#| fig-cap: 'Simple parallel pipeline plot showing a common data source being scaled then the same data being passed to two `PipeOp`s in parallel whose outputs are then combined and returned to the user.'
#| fig-alt: 'Six boxes where first two are "<INPUT> -> scale", then "scale" has two arrows to "missind" and "imputemedian" which both have an arrow to "featureunion -> <OUTPUT>".'
library(mlr3pipelines)

gr = po("scale", center = TRUE, scale = FALSE) %>>%
  gunion(list(
    po("missind"),
    po("imputemedian")
  )) %>>%
  po("featureunion")
fig <- magick::image_graph(width = 1500, height = 1000, res = 100, pointsize = 24)
gr$plot(horizontal = TRUE)
invisible(dev.off())
magick::image_trim(fig)
```

When applied to the first three rows of the `pima` task we can see how this imputes missing data and adds a column indicating where values were missing.

```{r 05-pipelines-modeling-004, eval = TRUE}
pima_head = tsk("pima")$filter(1:3)
pima_head$data(cols = c("diabetes", "insulin", "triceps"))
result = gr$train(pima_head)[[1]]
result$data(cols = c("diabetes", "insulin", "missing_insulin", "triceps",
  "missing_triceps"))
```

## `po("select")`, `po("featureunion")`, and `affect_columns`

It is common in `Graph`s for an operation to be applied to a subset of features.
In `r mlr3pipelines` this can either (@fig-pipelines-select-affect) be achieved by passing the column subset to the `affect_columns` hyperparameter of a `PipeOp` (assuming it has that hyperparameter), which controls which columns should be affected by the `PipeOp`.
Alternatively, one can use the `r ref("PipeOpSelect", aside = TRUE)` operator to create operations in parallel on specified feature subsets, and uniting the result using `r ref("PipeOpFeatureUnion", aside = TRUE)`.

```{r eval = TRUE}
#| label: fig-pipelines-select-affect
#| layout-nrow: 2
#| fig-cap: "Two methods of setting up `PipeOp`s (`po(op1)` and `po(op2)`) that operate on complementary features (X and ¬X) of an input task."
#| fig-alt: 'Top plot shows the sequential pipeline "po(op1, affected_columns: ¬X") -> po(op2, affected_columns: X"). Bottom plot shows the parallel pipeline that starts with an arrow splitting and then pointing to both  po("select", ¬X) and po("select", X). These respectively point to po(op1) and po(op2), which then both point to po("featureunion").'
#| fig-subcap:
#|   - 'The `affect_columns` hyperparameter can be used to restrict operations to a subset of features. When used pipelines may still be run in sequence.'
#|   - 'Operating on subsets of tasks using concurrent paths by first splitting the inputs with `po("select")` and then combining outputs with `po("featureunion").'
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/affect_pipe.svg")
knitr::include_graphics("Figures/select_pipe.svg")
```

Both methods make use of `r ref("Selector", aside = TRUE)`-functions.
These are helper-functions that indicate to a `PipeOp` which features it should apply to.
`Selectors` may match column names by regular expressions `r ref("selector_grep()")`, or by column type `r ref("selector_type()")`.
`Selectors` can also be used to join variables (`r ref("selector_union()")`), return their set difference (`r ref("selector_setdiff()")`), or select the complement of features from another `Selector` (`r ref("selector_invert()")`).

For example, in @sec-pipelines-pipeops we applied PCA to the bill length and depth of penguins from `penguins_simple` by first selecting these columns using the `Task` method `$select()` and then applying the `PipeOp`.
We can now do this more simply with `selector_grep`, and could go on to use `selector_invert` to apply some other `PipeOp` to other features, below we use `po("scale")` and make use of the `affect_columns` method:

```{r 05-pipelines-multicol-1, eval = TRUE}
sel_bill = selector_grep("^bill")
sel_not_bill = selector_invert(sel_bill)

gr = po("scale", affect_columns = sel_not_bill) %>>%
  po("pca", affect_columns = sel_bill)

result = gr$train(tsk("penguins_simple"))
result[[1]]$data()[1:3, 1:5]
```

The biggest advantage of this method is that it creates a very simple, sequential Graph.
However, one disadvantage of the `affect_columns` method is that it is relatively easy to have unexpected results if the ordering of `PipeOp`s is mixed up.
For example, if we had reversed the order of `po("pca")` and `po("scale")` above then we would have first created columns `"PC1"` and `"PC2"` and then scaled these as they would match the pattern in `sel_not_bill`.
Creating parallel paths with `po("select")` can help mitigate such errors by selecting features given by the `Selector` and creating an independent data processing streams with the given feature subset.
Below we pass the parallel pipelines to `gunion` as a `list` to ensure they receive the same input, and then combine the outputs with `po("featureunion")`.

```{r 05-pipelines-multicol-3-evalF, eval = FALSE}
path_pca = po("select", id = "s_bill", selector = sel_bill) %>>% po("pca")
path_scale = po("select", id = "s_not_bill", selector = sel_not_bill) %>>% po("scale")

gr = gunion(list(path_pca, path_scale)) %>>% po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-multicol-3-evalT, fig.width = 8, eval = TRUE, echo = FALSE}
#| label: fig-pipelines-pcascale
#| fig-cap: Visualization of our `Graph` where features are split into two paths, one with PCA and one with scaling, then combined and returned.
#| fig-alt: 'Seven boxes where first is "<INPUT>" which points to "s_bill -> pca" and "s_not_bill" -> scale", then both "pca" and "scale" point to "featureunion -> <OUTPUT>".'
po_select_bill = po("select", id = "s_bill", selector = sel_bill)
po_select_not_bill = po("select", id = "s_not_bill", selector = sel_not_bill)

path_pca =  po_select_bill %>>% po("pca")
path_scale = po_select_not_bill %>>% po("scale")

gr = gunion(list(path_pca, path_scale)) %>>% po("featureunion")
fig <- magick::image_graph(width = 1500, height = 1000, res = 100, pointsize = 24)
gr$plot(horizontal = TRUE)
invisible(dev.off())
magick::image_trim(fig)
```

The `po("select")` method also has the significant advantage that it allows the same set of features to be used on multiple operations and also allows some features to remain untransformed (`affect_columns` effectively filters out any columns not included in a `Selector`).
`r ref("PipeOpNOP")` performs no operation on its inputs and is thus useful when you only want to perform a transformation on a subset of features and leave the others untouched:

```{r 05-pipelines-multicol-5-evalF, eval = FALSE}
gr = gunion(list(
  po_select_bill %>>% po("scale"),
  po_select_not_bill %>>% po("nop")
)) %>>% po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-multicol-5-evalT, fig.width = 8, eval = TRUE, echo = FALSE}
#| label: fig-pipelines-selectnop
#| fig-cap: Visualization of our `Graph` where features are split into two paths, features that start with 'bill' are scaled and the rest are untransformed.
#| fig-alt: 'Six boxes where first is "<INPUT>" which points to "s_bill -> scale" and "s_not_bill -> nop", then both "scale" and "nop" point to "featureunion -> <OUTPUT>".'
gr = gunion(list(
  po_select_bill %>>% po("scale"),
  po_select_not_bill %>>% po("nop")
)) %>>% po("featureunion")
fig <- magick::image_graph(width = 1500, height = 1000, res = 100, pointsize = 24)
gr$plot(horizontal = TRUE)
invisible(dev.off())
magick::image_trim(fig)
```

```{r 05-pipelines-multicol-6, eval = TRUE}
gr$train(tsk("penguins_simple"))[[1]]$data()[1:3, 1:5]
```

##  Common Patterns and `ppl()` {#sec-pipelines-ppl}

Now you have the tools to create sequential and non-sequential pipelines, you can now create an infinite number of transformations on `Task`, `Learner`, and `Prediction` objects.
In @sec-pipelines-bagging and @sec-pipelines-stack we will work through two complex examples to demonstrate how you can make complex and powerful graphs using the methods and classes we have already looked at.
However, there are many common problems in ML that can be well solved by the same pipelines, and so to make your life easier we have implemented and saved these pipelines in the `r ref("mlr_graphs", aside = TRUE)` dictionary; pipelines in the dictionary can be accessed with the `r ref("ppl()", aside = TRUE)` sugar function.

At the time of writing, this dictionary includes seven `Graph`s (required arguments included):

* `ppl("bagging", graph)`: In `r mlr3pipelines`, bagging, described in detail in @sec-pipelines-bagging, is the process of running a `graph` multiple times on different data samples and then averaging the results. The `iterations` argument controls the number of bagging iterations, `frac` controls the sampling fraction, the averaging method can also be adjusted through the `averager` parameter.
* `ppl("branch", graphs)`: Uses `r ref("PipeOpBranch")` to create different path branches from the given `graphs` where only one branch is evaluated. This is returned to in more detail in @sec-pipelines-branch.
* `ppl("greplicate", graph, n)`: Create a `Graph` that replicates `graph` (which can also be a single `PipeOp`) `n` times. The pipeline avoids ID clashes by adding a suffix to each `PipeOp`, we will see this pipeline in use in @sec-pipelines-bagging.
* `ppl("ovr", graph)`: One-versus-rest classification for converting multiclass classificaiton tasks into several binary classification tasks with one task for each class in the original task. These tasks are then evaluated by the given `graph`, which should be a learner (or a pipeline containing a learner that emits a prediction). The predictions made on the binary tasks are combined into the multiclass prediction needed for the original task.
* `ppl("robustify")`: Perform common preprocessing steps to make any `Task` compatible with a given `Learner`. Optional arguments are the `r ref("Task")` and `r ref("Learner")` in question, as well as individual switches that decide which kind of preprocessing should be done. The "robustify" Graph element queries the metadata provided by the respective objects and performs only the necessary preprocessing. This pipeline is returned to in more detail in @sec-prepro-robustify.
* `ppl("stacking", base_learners, super_learner)`: Stacking, returned to in detail in @sec-pipelines-stack, is the process of using predictions from one or more models (`base_learners`) as features in a subsequent model (`super_learner`)
* `ppl("targettrafo", graph)`: Create a `Graph` that transforms the prediction target of a task and ensures that any transformations applied during training (using the function passed to the `targetmutate.trafo` hyperparameter) are inverted in the resulting predictions (using the function passed to the `targetmutate.inverter` hyperparameter); an example is given in @sec-prepro-scale.

## Bagging with `"greplicate"` and `"subsample"` {#sec-pipelines-bagging}

The basic idea of `r index('bagging')` (from **b**ootstrapp **agg**regat**ing**), introduced by @Breiman1996, is to aggregate create multiple predictors into a single, more powerful predictor (@fig-pipelines-bagging).
Predictions are usually aggregated by the arithmetic mean for regression tasks or majority vote for classification.
The underlying intuition behind bagging is that averaging a set of weak, but diverse (i.e., only weakly correlated) predictors can reduce the variance of the overall prediction.
Each learner is trained on a different random sample of the original data.

Although we have already seen that a pre-constructed bagging pipeline is available with `ppl("bagging")`, in this section we will build our own pipeline from scratch to showcase how to construct a complex `Graph`, which will look something like @fig-pipelines-bagging.

```{r, echo = FALSE}
#| label: fig-pipelines-bagging
#| fig-cap: "Graph that performs Bagging by independently subsampling data and fitting individual decision tree learners. The resulting predictions are aggregated by a majority vote `PipeOp`."
#| fig-alt: 'Graph shows "Dtrain" with arrows to four separate po("subsample") boxes that each have a separate arrow to four more po("classif.rpart") boxes that each have an arrow to the same one po("classif.avg") box.'
#| out.width: "70%"
knitr::include_graphics("Figures/nonlinear_pipeops.svg")
```

To begin, we use `r ref("PipeOpSubsample")` to sample a fraction of the data (here 70%), which is then passed to a classification tree.

```{r 05-pipelines-non-sequential-009, eval = TRUE}
single_pred = po("subsample", frac = 0.7) %>>% lrn("classif.rpart")
```

Next we use `r ref("pipeline_greplicate")` to copy the graph `single_pred` 10 times (`n = 10`) and finally `r ref("PipeOpClassifAvg")` to take the majority vote of all predictions.

```{r 05-pipelines-non-sequential-010-evalT, eval = FALSE}
pred_set = ppl("greplicate", graph = single_pred, n = 10)
bagging = pred_set %>>% po("classifavg", innum = 10)
bagging$plot()
```

```{r 05-pipelines-non-sequential-010-evalF, echo = FALSE}
#| label: fig-pipelines-bagginggraph
#| fig-cap: Constructed bagging `Graph` with one input being sampled many times for 10 different learners.
#| fig-alt: 'Parallel pipeline showing "<INPUT>" pointing to ten PipeOps "subsample_1",...,"subsample_10" that each separately point to "classif.rpart_1",...,"classif.rpart_10" respectively, which all point to the same "classifavg -> <OUTPUT>".'
pred_set = ppl("greplicate", graph = single_pred, n = 10)
bagging = pred_set %>>% po("classifavg", innum = 10)
fig <- magick::image_graph(width = 2000, height = 1000, res = 100, pointsize = 17)
bagging$plot()
invisible(dev.off())
magick::image_trim(fig)
```

Now let us see how well our bagging pipeline compares to the single decision tree and to a random forest, we will test this on the `sonar` task.

```{r 05-pipelines-non-sequential-013}
l_bag = as_learner(bagging)
l_bag$id = "bagging"
learner_rpart = lrn("classif.rpart")
grid = benchmark_grid(tsk("sonar"),
  c(l_bag, learner_rpart, lrn("classif.ranger")), rsmp("cv", folds = 3))
bmr = benchmark(grid)
bmr$aggregate()[, .(learner_id, classif.ce)]
```

The bagged learner performs noticeably better than the decision tree but worse than the random forest.
The steps above are saved as an accessible graph in `r ref("pipeline_bagging")`, to construct the above we would simply construct the `"bagging"` pipeline specifying the learner to 'bag', number of iterations, fraction of data to sample, and the `PipeOp` to average the predictions, note we set `collect_multiplicity = TRUE` which collects the predictions across paths, which technically use the `r ref("Multiplicity")` method that we will not discuss here but refer the reader to the documentation.

```{r, eval = FALSE}
ppl("bagging", lrn("classif.rpart"),
  iterations = 10, frac = 0.7,
  averager = po("classifavg", collect_multiplicity = TRUE))
```

The main difference between our pipeline and a random forest is that the latter also performs "feature bagging", where only a random subset of available features is considered at each split point.
While we cannot implement this directly with `r mlr3pipelines`, we can use a custom `Selector` method to approximate this method.
For efficiency we will now use `ppl("bagging")` to recreate the steps above, we also create a custom `Selector` by passing a function that takes as input the task and returns a sample of the features, we sample the square root of the number of features to mimic the implementation in `r ref("ranger::ranger")`.

```{r 05-bagging-ex}
# custom selector
selector_subsample = function(task) {
  sample(task$feature_names, sqrt(length(task$feature_names)))
}

# bagging pipeline with our selector
bagging_quasi_rf = ppl("bagging",
  graph = po("select", selector = selector_subsample) %>>%
    lrn("classif.rpart", minsplit = 1),
  iterations = 100,
  averager = po("classifavg", collect_multiplicity = TRUE)
)

# bootstrap resampling
bagging_quasi_rf$param_set$values$subsample.replace = TRUE

# convert to learner
l_quasi_rf = as_learner(bagging_quasi_rf)
l_quasi_rf$id = "quasi.rf"

# benchmark
grid = benchmark_grid(tsks("sonar"),
  c(l_quasi_rf, lrn("classif.ranger", num.trees = 100)),
  rsmp("cv", folds = 5)
)
bmr = benchmark(grid)
bmr$aggregate(msrs(c("classif.ce", "time_both")))[, .(learner_id, classif.ce, time_both)]
```

In only a few lines of code, we took a weak learner and turned it into a powerful model that we can see is comparable to the implementation in `r ref("ranger::ranger")`, however our pipeline is much slower as `r ref("ranger::ranger")` uses C++.
In the next section we will look at a second example, which makes use of cross-validation within pipelines.

## Stacking with "learner_cv" {#sec-pipelines-stack}

Stacking [@Wolpert1992] is another technique that can significantly improve model performance.
The basic idea behind stacking is to use predictions from one model as features for a subsequent model to try to improve performance (@fig-pipelines-stacking).
As with bagging, we will demonstrate how to create a stacking pipeline manually, although a pre-constructed pipeline is available with `ppl("stacking")`.

```{r eval=TRUE, fig.align='center', echo = FALSE}
#| label: fig-pipelines-stacking
#| fig-cap: "Graph that performs Stacking by fitting three models and using their outputs as features for another model after combining with `PipeOpFeatureUnion`."
#| fig-alt: 'Graph shows "Dtrain" with arrows to three boxes: "Decision Tree", "KNN", and "Lasso Regression". Each of these points to the same "Feature Union -> Logistic Regression".'
#| out.width: 70%
knitr::include_graphics("Figures/stacking.svg")
```

Stacking depends on the predictions of models that are trained on the same data.
Therefore, analogously to nested resampling when tuning (@sec-nested-resampling), we must limit overfitting by stacking features from cross-validated data, this is possible in pipelines with `r ref("PipeOpLearnerCV", aside = TRUE)`.
Each of the models is then trained on a subset of folds and makes predictions on the out-of-fold data, obtaining predictions for all data points in our input data without 'leaking' information into the prediction stage.

We first create various learners that produce the predictions that will be used as features, known as the "level 0" learners.
Like bagging, stacking also often uses 'weak' learners that might not perform well on their own, in this example we use a classification tree, k-nearest-neighbours, and a regularized GLM.
Each learner is wrapped in `po("leaner_cv")` which trains and predicts with the given learner, returning the model predictions in a new `Task` as its output.

```{r 05-pipelines-non-sequential-015, eval = TRUE}
learner_rpart = lrn("classif.rpart", predict_type = "prob")
po_rpart_cv = po("learner_cv", learner = learner_rpart,
  resampling.folds = 2, id = "rpart_cv"
)

learner_knn = lrn("classif.kknn", predict_type = "prob")
po_knn_cv = po("learner_cv",
  learner = learner_knn,
  resampling.folds = 2, id = "knn_cv"
)

learner_glmnet = lrn("classif.glmnet", predict_type = "prob")
po_glmnet_cv = po("learner_cv",
  learner = learner_glmnet,
  resampling.folds = 2, id = "glmnet_cv"
)
```

These learners are combined using `r ref("gunion()")`, and `"po("featureunion"))` is used to merge their predictions.
This is demonstrated in the output of `$train()`:

```{r 05-pipelines-non-sequential-016, eval = TRUE}
level_0 = gunion(list(po_rpart_cv, po_knn_cv, po_glmnet_cv))
combined = level_0 %>>% po("featureunion")

combined$train(tsk("sonar"))[[1]]$head()
```

:::{.callout-tip}
In this example the original features were removed as each `PipeOp` only returns the predictions made by the respective learners.
To retain the original features, simply include `po("nop")` in the list passed to `r ref("gunion()")`.
:::

The resulting task contains the predicted probabilities for both classes made from each of the level 0 learners.
However as the probabilities always add up to 1 we only need the predictions for one of the classes (as this is a binary classification task), so we can use `po("select")` to only keep predictions for one class (we choose `"M"` in this example).


```{r 05-pipelines-non-sequential-017, eval = TRUE}
stack = combined %>>%
  po("select", selector = selector_grep("\\.M"))
```

Finally we can combine our pipeline with the final model that will take these predictions as its input.

```{r 05-pipelines-non-sequential-018-evalF, eval = FALSE}
stack = stack %>>% po("learner", lrn("classif.log_reg"))
stack$plot(horizontal = TRUE)
```

```{r 05-pipelines-non-sequential-018-evalT, fig.width = 10, echo = FALSE}
#| label: fig-pipelines-stackinggraph
#| fig-cap: 'Constructed stacking Graph with one input being passed to three weak learners whose predictions pass the logistic regression.'
#| fig-alt: 'Graph with "<INPUT>" in the first box with arrows to three boxes: "rpart_cv", "knn_cv", "glmnet_cv", which all have arrows pointing to the same boxes: "featureunion -> select -> classif.log_reg -> <OUTPUT>".'
stack = stack %>>% po("learner", lrn("classif.log_reg"))
fig <- magick::image_graph(width = 2000, height = 1000, res = 100, pointsize = 24)
stack$plot(horizontal = TRUE)
invisible(dev.off())
magick::image_trim(fig)
```

As we used logistic regression as the final model, we can inspect the weights of the level 0 learners by looking at the final trained model:

```{r 05-pipelines-non-sequential-019-x, eval = TRUE}
learner_stack = as_learner(stack)
learner_stack$train(tsk("sonar"))
learner_stack$base_learner()$model
```

The model weights suggest that `r c("rpart", "knn", "glmnet")[which.max(learner_stack$base_learner()$model$coefficients[-1])]` influences the predictions the most with the largest coefficient.
To confirm this we can benchmark the individual models alongside the stacking pipeline.

```{r 05-pipelines-non-sequential-019-1-background}
learner_stack$id = "stacking"
grid = benchmark_grid(
  tsks("sonar"),
  list(learner_rpart, learner_knn, learner_glmnet, learner_stack),
  rsmp("holdout")
)
bmr = benchmark(grid)
bmr$aggregate()[, .(learner_id, classif.ce)]
```

This experiment confirms that of the individual models, the KNN learner performs the best, however our stacking pipeline outperforms them all.
Now that we have seen the inner workings of this pipeline, next time you might want to more efficiently create it using `r ref("pipeline_stacking")`, to copy the example above you would run:

```{r, eval = FALSE}
ppl("stacking",
  base_learners = lrns(c("classif.rpart", "classif.kknn", "classif.glmnet")),
    super_learner = lrn("classif.log_reg")
)
```

Having covered the building blocks of `r mlr3pipelines` and seen these in practice, we will now turn to more advanced functionality, combining pipelines with tuning.

## Tuning Graphs {#sec-pipelines-tuning}

{{< include _optional.qmd >}}
<!-- fixme: reviewed to here -->
Having as many options for preprocessing as provided by `r mlr3pipelines` has many benefits, but it also comes with a drawback:
It enlarges the space of possible hyperparameter configurations considerably.
Not only do preprocessing operations bring their own hyperparameter settings, but the decisions on whether to do preprocessing, and which preprocessing operation to perform, also need to be made.
Tuning ML models with `r mlr3pipelines` can be considered at various levels of complexity:

1. Tuning the hyperparameters of a learner or a PipeOp individually when it is part of a Graph.
2. Jointly tuning the hyperparameters of both learner and its preprocessing operations.
3. Tuning not only the hyperparameters, but also the choice of which operation to perform.

The first level is not much different from tuning individual learners, as described in @sec-optimization.
The only thing to watch out for here is that a learner's hyperparameter names are prefixed with its ID, as shown in @sec-pipelines-inspect.
The second level is demonstrated in the following @sec-pipelines-combined.
The third level is also referred to as the "Combined Algorithm Selection and Hyperparameter optimization" (CASH) [@Thornton2013].
This is demonstrated in @sec-pipelines-branch, which shows how to use alternative path branching.
An alternative way of implementing it is to use `r ref("PipeOpProxy", "po(\"proxy\")")`, demonstrated in the (advanced) @sec-pipelines-proxy.

In this section, we will use the well-known "MNIST" dataset [@lecun1998gradient], which comprises 28 x 28 pixel images of handwritten digits.
It is a classification task with the goal of identifying the digits accurately.
It is often used to demonstrate deep learning with convolutional layers.
However, for this demonstration, we will not use the information about the relative location of pixels.
Instead, we use each pixel's intensity as a separate feature.
We obtain the data from OpenML, which is described in greater detail in @sec-large-benchmarking.
To speed up performance estimation, we subset the given data to 5% of its original size.
```{r 06-pipelines-get-mnist-openml}
library("mlr3oml")
mnist_data = odt(id = 554)
subset = sample(mnist_data$nrow, mnist_data$nrow * 0.05, replace = FALSE)
mnist_task = as_task_classif(mnist_data$data[subset],
    target = "class", id = "mnist")
head(mnist_task$feature_names)
```


### Tuning Combined Spaces {#sec-pipelines-combined}

Instead of using deep learning, we will use the much simpler k-nearest-neighbor (KNN) learner `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")`, again with the simple `"rectangular"` distance kernel.
We investigate if doing a principal component analysis using `r ref("PipeOpPCA", "po(\"pca\")")`, and selecting the highest variance components using the `rank.` hyperparameter, can improve performance.
Because the `k` hyperparameter of the KNN learner also needs to be found, we need to tune it simultaneously.

First, we need to define the Graph that we are tuning.
We have the option of setting each component's hyperparameter being tuned to `to_tune()`, as is done in @sec-optimization, but here we will demonstrate how to define the search space `r ref("ParamSet")` directly.

```{r 05-pipelines-modeling-008}
library("mlr3learners")
graph_learner = po("pca") %>>%
  lrn("classif.kknn", kernel = "rectangular")
graph_learner = as_learner(graph_learner)

library("paradox")
search_space = ps(
  pca.rank. = p_int(
    lower = 2,
    upper = length(mnist_task$feature_names),
    logscale = TRUE
  ),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE)
)
```

We tune this using the `r ref("tune")` function on a 6x6 grid, stepping through both the number of selected principal components (`pca.rank.`) and the KNN's `k` hyperparameter on a log-scale.


```{r debug-dummy, echo=FALSE, eval = TRUE}
instance = list(result_x_domain = list(pca.rank. = 30))
```
```{r 05-pipelines-modeling-009}
library("mlr3tuning")

instance = tune(
  tuner = tnr("random_search"),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  search_space = search_space,
  term_evals = 10
)

instance$result
```

:::{.callout-tip}
Tuning complex pipelines can take a long time:
Not only are the individual training steps often more complex and therefore take longer; the increased search space dimension usually also means that more evaluations need to be performed to get an acceptable level of performance.
It is therefore recommended to make use of parallelization using `r ref_pkg("future")`, which is demonstrated in @sec-parallelization.
:::


Note that the output values are the logarithm of the actual result, which is:
```{r 05-pipelines-modeling-009-2}
instance$result_x_domain
```

The observed performance values are shown in @fig-pipelines-opttrace-1.
It becomes clear that, for different values of `rank.`, different `k` values can be optimal, so jointly tuning both hyperparameters is prudent.

```{r calcbaseline, include = FALSE}
baselineperf = resample(mnist_task, po("pca") %>>% lrn("classif.kknn", kernel = "rectangular"), rsmp("cv", folds = 3))$aggregate(msr("classif.ce"))
baselineperf2 = resample(mnist_task, lrn("classif.kknn", kernel = "rectangular"), rsmp("cv", folds = 3))$aggregate(msr("classif.ce"))
```

```{r fig.align='center', fig.width = 7, fig.height = 3}
#| label: fig-pipelines-opttrace-1
#| fig-cap: "Observed performance values when optimizing both `rank.` of `po(\"pca\")` and `k` of `lrn(\"classif.kknn\")` at the same time.
#|   The x-axis shows `k` values (log scale). Each facet (i.e. individual plot) shows the behavior for a different `rank.` value, displayed at the top.
#|   The red star shows the performance of the untuned model: using `po(\"pca\") %>>% lrn(\"classif.kknn\")` with their defaults: `rank.` set to the number of features, and `k` set to 7."
#| fig-alt: "Plot showing performance values of pca, followed by KNN."
#| echo: false
library("ggplot2")

ggplot(instance$archive$data[, c(rbindlist(x_domain), list(classif.ce = classif.ce))], aes(x = classif.kknn.k, y = classif.ce)) +
  geom_line() +
  ggstar::geom_star(data = data.frame(classif.kknn.k = 7, classif.ce = baselineperf, pca.rank. = length(mnist_task$feature_names)), fill = "red", size = 4, starshape = 1) +
  facet_grid(cols = vars(pca.rank.)) +
  scale_x_log10() +
  theme_minimal() +
  labs(title = "Performance of po(\"pca\") %>>% lrn(\"classif.kknn\")")
```

### Tuning Alternative Paths with `po("branch")` {#sec-pipelines-branch}

While we see that tuning the `r ref("PipeOpPCA", "po(\"pca\")")` has benefits, we have not yet seen whether using PCA at all is beneficial.
It is possible that using the tuned PCA simply does the least damage, or that a much simpler operation would perform equally well or better.

Here we can use the `r ref("PipeOpBranch", "po(\"branch\")")` and `r ref("PipeOpUnbranch", "po(\"unbranch\")")` PipeOps, which make it possible to specify multiple alternative paths.
Data only flows along one of these paths, which can be controlled by a hyperparameter, as is shown in @fig-pipelines-alternatives (a).
This concept makes it possible to tune alternative preprocessing methods or alternative learner models.

`po("(un)branch")` is initialized either with the number of branches, or with a `character`-vector indicating the names of the branches.
If names are given, the "branch-choosing" hyperparameter becomes more readable.
In the following, we set three options:

1. Doing nothing (`r ref("PipeOpNOP", "po(\"nop\")")`)
2. Applying a PCA
3. Removing constant features and applying the Yeo-Johnson transform, using `r ref("PipeOpYeoJohnson", "po(\"yeojohnson\")")`

It is important to "unbranch" again after "branching" to ensure that the outputs are merged into one result objects.

For this demo, we will use the optimal `rank.` value from the previous optimization.

```{r 05-pipelines-non-sequential-003, eval = TRUE}
rank_opt = instance$result_x_domain$pca.rank.

graph = po("branch", c("nop", "pca", "yeojohnson")) %>>%
  gunion(list(
    po("nop"),
    po("pca", rank. = rank_opt),
    po("removeconstants") %>>% po("yeojohnson")
  )) %>>% po("unbranch", c("nop", "pca", "yeojohnson"))
```

The resulting Graph looks as follows:

```{r 05-pipelines-non-sequential-004, fig.width = 11, eval = TRUE}
graph$plot(horizontal = TRUE)
```

The output of this Graph depends on the setting of the `branch.selection` hyperparameter:

```{r 05-pipelines-branch-01}
graph$param_set$values$branch.selection = "pca"  # use the "PCA" path
head(graph$train(mnist_task)[[1]]$feature_names)
graph$param_set$values$branch.selection = "nop"  # use the "No-Op" path
head(graph$train(mnist_task)[[1]]$feature_names)
```

<!-- FIXME:

ADD

  The choice between PCA, Yeo-Johnson transform, and no-op that is shown in  can, for example, be produced by calling:

  ppl("branch", graphs = pos(c("pca", "yeojohnson", "nop")))

 -->

Tuning this hyperparameter can help determine which of the possible options works best in combination with a given learner.
Branching can even be used to tune which of several learners is most appropriate for a given dataset.
We now extend this example so that both the preprocessing (PCA, Yeo-Johnson transform, or no preprocessing), as well as the model to use (KNN or decision tree) can be tuned.
For this, we add another branching pathway to our Graph:

```{r 05-pipelines-branch-02, eval = TRUE}
par(cex = 0.7)
graph_learner = graph %>>%
  po("branch", c("classif.rpart", "classif.kknn"), id = "branch2") %>>%
    gunion(list(
      lrn("classif.rpart"),
      lrn("classif.kknn", kernel = "rectangular")
    )) %>>%
  po("unbranch", c("classif.rpart", "classif.kknn"), id = "unbranch2")
graph_learner = as_learner(graph_learner)
graph_learner$graph$plot()
```

Note that it is necessary to give the two branching operations different IDs to avoid name clashes.

Finally, we would still like to tune the `k` hyperparameter of the KNN learner, as it may depend on the type of preprocessing performed.
However, this hyperparameter is only active when the "`classif.kknn`" path is chosen.
We therefore have to declare a dependency in the search space.
Our search space, which tunes over all options of both branching operators as well as the KNN learner's `k` hyperparameter, is as follows:


```{r 05-pipelines-branch-03}
search_space = ps(
  branch.selection = p_fct(c("nop", "pca", "yeojohnson")),
  branch2.selection = p_fct(c("classif.rpart", "classif.kknn")),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE,
    depends = branch2.selection == "classif.kknn")
)

# set seed for comparison with the alternative optimization method below
set.seed(1)
instance = tune(
  tuner = tnr("random_search"),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  search_space = search_space,
  term_evals = 10
)

cbind(
  as.data.table(instance$result_x_domain),
  classif.ce = instance$result_y
)
```

The results reveal that the tuned KNN performs better than the untuned decision tree, "classif.rpart".
A more thorough investigation could try to tune the decision tree to check if its performance can be brought to the level of the KNN algorithm.
However, increasing the number of options significantly enlarges the search space.
The `grid_search` tuner, which we have used here because of its straightforward interpretability, is not recommended for search spaces that have more than a very small number of dimensions.
Therefore, if this investigation were to be carried further, one would need to use a different optimizer, such as random search or Bayesian optimization, the latter of which is demonstrated in @sec-bayesian-optimization.

```{r fig.align='center', fig.width = 7, fig.height = 3}
#| label: fig-pipelines-opttrace-2
#| fig-cap: "Observed performance values when optimizing both preprocessing (between \"pca\", \"yeojohnson\", and \"nop\", which is no preprocessing) and learner (between KNN and the decision tree \"classif.rpart\") at the same time.
#|   The x-axis shows preprocessing. Each facet (i.e. individual plot) shows the learner being used, together with setting for \"k\", if applicable."
#| fig-alt: "Plot showing performance values of pca, yeojohnson, or nop, followed by KNN or decision tree."
#| echo: false

ggplot(instance$archive$data[, learner := ifelse(is.na(classif.kknn.k), "classif.rpart", sprintf("classif.kknn\nk = % 2s", (sapply(x_domain, `[[`, "classif.kknn.k"))))],
  aes(x = branch.selection, y = classif.ce)) +
  geom_point() +
  facet_grid(cols = vars(learner)) +
  theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Performance of Various Learners with Different Preprocessing")
```

:::{.callout-tip}
Graphs with alternative path branching can also be created using `ppl()`, see @sec-pipelines-ppl
:::

### Tuning with `po("proxy")` {#sec-pipelines-proxy}

{{< include _optional.qmd >}}

The `r ref("PipeOpProxy", "po(\"proxy\")")` operator is a meta-operator that performs the operation that is stored in its `content` hyperparameter.
This can either be another PipeOp, or an entire Graph.
A PipeOp which can itself contain Graphs that can be tuned over, makes it optimization over different operation possible, similarly to `r ref("PipeOpBranch", "po(\"branch\")")` / `r ref("PipeOpUnbranch", "po(\"unbranch\")")`.
@fig-pipelines-alternatives shows the conceptual difference between `r ref("PipeOpBranch", "po(\"branch\")")` and `r ref("PipeOpProxy", "po(\"proxy\")")`.

```{r eval = TRUE}
#| label: fig-pipelines-alternatives
#| layout-nrow: 2
#| fig-cap: "
#|   Two ways of parametrizing the PipeOps or learners that should be used.
#|   The setups shown in both examples have the same effect: Data is PCA-transformed before being fed to the learner.
#|   (a): Using `po(\"branch\")` it is possible to choose which one out of various alternative paths should be taken.
#|        In this example, the PCA PipeOp is active, the alternatives (Yeo-Johnson transform, or no operation through `po(\"nop\")`) are inactive.
#|        A `po(\"unbranch\")` is necessary to mark the end of the alternative paths.
#|   (b): `po(\"proxy\")` has the hyperparameter `content`, which can be set to a another PipeOp. In this example, it is set to PCA.
#| "
#| fig-alt:
#|   - "po(\"branch\"), followed by, alternatively, po(\"nop\"), PCA, and Yeo-Johnson transform, which are all followed by `po(\"unbranch\")` and a learner.
#|      The PCA branch is active."
#|   - "`po(\"proxy\")`, followed by a learner. The content of the `po(\"proxy\")` is set to a PCA PipeOp."
#| fig-subcap:
#|   - "Usage of `po(\"branch\")`"
#|   - "Usage of `po(\"proxy\")`"
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/branching.svg")
knitr::include_graphics("Figures/proxy.svg")
```

To use `r ref("PipeOpProxy", "po(\"proxy\")")` instead of alternative path branching to perform the above optimization, one would first set up a Graph that contains `r ref("PipeOpProxy", "po(\"proxy\")")` operators as placeholders for the operations (preprocessing, learning) that should be tuned.
Note the different IDs to avoid a name clash.

```{r}
graph_learner = po("proxy", id = "preproc") %>>%
  po("proxy", id = "learner")
graph_learner = as_learner(graph_learner)
```

The tuning space for the `content` hyperparameters can now be a discrete set of the possibilities that should be evaluated.
Using `r ref("p_fct", "paradox::p_fct()")` with a named list of PipeOps that should be inserted works here.
Internally, `r paradox` is creating a transformation function here, see @sec-defining-search-spaces for more details.
For the learner-part, a more complex trafo-function is required, as the selection of the learner depends on more than one search space component:
The choice of the learner itself (`r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` or `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")`), as well as the `k` hyperparameter of the KNN learner.
This trafo-function is passed to the `.extra_trafo` argument of `r ref("ps", "ps()")`.
The transformation accepts the hyperparameter configuration that was generated by the search space as input `x`, and returns the configuration to be assigned to the Graph -- both as named lists.
The help page of `r ref("ps", "ps()")` gives more details on this.
Inside this transformation, the `learner.content` value must be clonsed before modification to avoid altering the original `r ref("Learner")` object inside the search space by reference!
Observe how this optimization, when performed with the same seed, yields the same result as above.

```{r}
search_space = ps(
  preproc.content = p_fct(list(
    nop = po("nop"),
    pca = po("pca", rank. = rank_opt),
    yeojohnson = po("removeconstants") %>>% po("yeojohnson")
  )),
  learner.content = p_fct(list(
    classif.rpart = lrn("classif.rpart"),
    classif.kknn = lrn("classif.kknn", kernel = "rectangular")
  )),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE,
    depends = learner.content == "classif.kknn"),
  .extra_trafo = function(x, param_set) {
    if (!is.null(x$classif.kknn.k)) {
      x$learner.content = x$learner.content$clone(deep = TRUE)
      x$learner.content$param_set$values$k = x$classif.kknn.k
      x$classif.kknn.k = NULL
    }
    x
  }
)

set.seed(1)  # for comparison with the optimization above
instance = tune(
  tuner = tnr("random_search"),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  search_space = search_space,
  term_evals = 10
)

as.data.table(instance$result)[,
  .(preproc.content, learner.content,
    classif.kknn.k = x_domain[[1]]$learner.content$param_set$values$k,
    classif.ce)
]
```

## Recap

`r ref_pkg("mlr3pipelines")` provides `r ref("PipeOp")` objects that provide preprocessing, postprocessing, and ensembling operations that can be created using the `r ref("po", "po()")` constructor function.
PipeOps have an ID and hyperparameters that can be set during construction and can be modified later.

PipeOps are concatenated using the `r ref("concat_graphs", "%>>%")`-operator to form `r ref("Graph")` objects.
Graphs can be converted to `r ref("Learner")` objects using the `r ref("as_learner")` function, after which they can be benchmarked and tuned using the tools provided by `r ref_pkg("mlr3")`.
Various standard Graphs are provided by the `r ref("ppl", "ppl()")` constructor function.

## Exercises

4. Consider the `r ref("PipeOpSelect", "po(\"select\")")` in @sec-pipelines-stack that is used to only keep the columns ending in "`M`".
  If the classification task had more than two classes, it would be more appropriate to list the single class we *do not* want to keep, instead of listing all the classes we do want to keep.
  How would you do this, using the `r ref("Selector")` functions provided by `r ref_pkg("mlr3pipelines")`?
  (Note: The `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` learner used in @sec-pipelines-stack cannot handle more than two classes. To build the entire stack, you will need to use a different learner, such as `r ref("LearnerClassifMultinom", "lrn(\"classif.multinom\")")`.)
5. How would you solve the previous exercise without even explicitly naming the class you want to exclude, so that your Graph works for any classification task?
  Hint: look at the `selector_subsample` in @sec-pipelines-bagging.
6. Use the `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` PipeOp to impute missing values in the `r ref("mlr_tasks_penguins", "tsk(\"penguins\")")` task using learners based on `r ref("ranger::ranger")`.
  Hint 1: you will need to use `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` twice, once for numeric features with `r ref("LearnerRegrRanger", "lrn(\"regr.ranger\")")`, and once for categorical features with `r ref("LearnerClassifRanger", "lrn(\"classif.ranger\")")`.
  Using the `affect_columns` argument of `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` will help you here.
  Hint 2: `r ref("ranger::ranger")` itself does not support missing values, but it is trained on all the features of `r ref("mlr_tasks_penguins", "tsk(\"penguins\")")` that it is not currently imputing, some of which will also contain missings.
  A simple way to avoid problems here is to use `r ref("pipeline_robustify", "ppl(\"robustify\")")` *inside* `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` next to the `r ref("ranger::ranger")` learner.

::: {.content-visible when-format="html"}
```{r citeas, cache = FALSE}
citeas(chapter)
```
:::
