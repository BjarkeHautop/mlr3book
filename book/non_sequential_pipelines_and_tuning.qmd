# Non-sequential Pipelines and Tuning {#sec-pipelines-nonseq}

{{< include _setup.qmd >}}

`r chapter = "Non-sequential Pipelines and Tuning"`
`r authors(chapter)`

```{r pipelines-setup, include = FALSE, cache = FALSE}
library("mlr3oml")
dir.create(here::here("book", "openml"), showWarnings = FALSE, recursive = TRUE)
options(mlr3oml.cache = here::here("book", "openml", "cache"))
```

In @sec-pipelines we looked at simple sequential pipelines that can be built using the `Graph` class and a few `PipeOp` objects.
In this chapter we will take this further and look at non-sequential pipelines that can perform more complex operations.
We will then look at tuning pipelines by combining methods in `r mlr3tuning` and `r mlr3pipelines` and will consider some concrete examples using multi-fidelity tuning (@sec-hyperband) and feature selection (@sec-feature-selection).

We saw the power of the `%>>%`-operator in @sec-pipelines to assemble graphs from combinations of multiple `PipeOp`s and `Learner`s.
Given a single `PipeOp` or `Learner`, the `%>>%`-operator will arrange these objects into a linear `Graph` with each `PipeOp` acting in sequence.
However, by using the `r ref("gunion()")` function, we can instead combine multiple `PipeOp`s, `Graph`s, or a mixture of both, into a parallel `Graph`.
In the following example, we create a `Graph` that centers its inputs and then copies the scaled data to two parallel streams: one replaces the data with columns that indicate whether data is missing, the other imputes missing data using the median (we will return to this in @sec-preprocessing-missing).
The outputs of both streams are then combined into a single dataset using `r ref("PipeOpFeatureUnion")`.

```{r 05-pipelines-modeling-003-evalF, eval = FALSE}
library(mlr3pipelines)

gr = po("scale", center = TRUE, scale = FALSE) %>>%
  gunion(list(
    po("missind"),
    po("imputemedian")
  )) %>>%
  po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-modeling-003-evalT, fig.width = 8, eval = TRUE, echo = FALSE}
#| label: fig-pipelines-parallel-plot
#| fig-cap: 'Simple parallel pipeline plot showing a common data source being scaled then the same data being passed to two `PipeOp`s in parallel whose outputs are then combined and returned to the user.'
#| fig-alt: 'Six boxes where first two are "<INPUT> -> scale", then "scale" has two arrows to "missind" and "imputemedian" which both have an arrow to "featureunion -> <OUTPUT>".'
library(mlr3pipelines)

gr = po("scale", center = TRUE, scale = FALSE) %>>%
  gunion(list(
    po("missind"),
    po("imputemedian")
  )) %>>%
  po("featureunion")
fig <- magick::image_graph(width = 1500, height = 1000, res = 100, pointsize = 24)
gr$plot(horizontal = TRUE)
invisible(dev.off())
magick::image_trim(fig)
```

When applied to the first three rows of the `pima` task we can see how this imputes missing data and adds a column indicating where values were missing.

```{r 05-pipelines-modeling-004, eval = TRUE}
pima_head = tsk("pima")$filter(1:3)
pima_head$data(cols = c("diabetes", "insulin", "triceps"))
result = gr$train(pima_head)[[1]]
result$data(cols = c("diabetes", "insulin", "missing_insulin", "triceps",
  "missing_triceps"))
```

## `po("select")`, `po("featureunion")`, and `affect_columns`

It is common in `Graph`s for an operation to be applied to a subset of features.
In `r mlr3pipelines` this can either (@fig-pipelines-select-affect) be achieved by passing the column subset to the `affect_columns` hyperparameter of a `PipeOp` (assuming it has that hyperparameter), which controls which columns should be affected by the `PipeOp`.
Alternatively, one can use the `r ref("PipeOpSelect", aside = TRUE)` operator to create operations in parallel on specified feature subsets, and uniting the result using `r ref("PipeOpFeatureUnion", aside = TRUE)`.

```{r eval = TRUE}
#| label: fig-pipelines-select-affect
#| layout-nrow: 2
#| fig-cap: "Two methods of setting up `PipeOp`s (`po(op1)` and `po(op2)`) that operate on complementary features (X and ¬X) of an input task."
#| fig-alt: 'Top plot shows the sequential pipeline "po(op1, affected_columns: ¬X") -> po(op2, affected_columns: X"). Bottom plot shows the parallel pipeline that starts with an arrow splitting and then pointing to both  po("select", ¬X) and po("select", X). These respectively point to po(op1) and po(op2), which then both point to po("featureunion").'
#| fig-subcap:
#|   - 'The `affect_columns` hyperparameter can be used to restrict operations to a subset of features. When used pipelines may still be run in sequence.'
#|   - 'Operating on subsets of tasks using concurrent paths by first splitting the inputs with `po("select")` and then combining outputs with `po("featureunion").'
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/affect_pipe.svg")
knitr::include_graphics("Figures/select_pipe.svg")
```

Both methods make use of `r ref("Selector", aside = TRUE)`-functions.
These are helper-functions that indicate to a `PipeOp` which features it should apply to.
`Selectors` may match column names by regular expressions `r ref("selector_grep()")`, or by column type `r ref("selector_type()")`.
`Selectors` can also be used to join variables (`r ref("selector_union()")`), return their set difference (`r ref("selector_setdiff()")`), or select the complement of features from another `Selector` (`r ref("selector_invert()")`).

For example, in @sec-pipelines-pipeops we applied PCA to the bill length and depth of penguins from `penguins_simple` by first selecting these columns using the `Task` method `$select()` and then applying the `PipeOp`.
We can now do this more simply with `selector_grep`, and could go on to use `selector_invert` to apply some other `PipeOp` to other features, below we use `po("scale")` and make use of the `affect_columns` method:

```{r 05-pipelines-multicol-1, eval = TRUE}
sel_bill = selector_grep("^bill")
sel_not_bill = selector_invert(sel_bill)

gr = po("scale", affect_columns = sel_not_bill) %>>%
  po("pca", affect_columns = sel_bill)

result = gr$train(tsk("penguins_simple"))
result[[1]]$data()[1:3, 1:5]
```

The biggest advantage of this method is that it creates a very simple, sequential Graph.
However, one disadvantage of the `affect_columns` method is that it is relatively easy to have unexpected results if the ordering of `PipeOp`s is mixed up.
For example, if we had reversed the order of `po("pca")` and `po("scale")` above then we would have first created columns `"PC1"` and `"PC2"` and then scaled these as they would match the pattern in `sel_not_bill`.
Creating parallel paths with `po("select")` can help mitigate such errors by selecting features given by the `Selector` and creating an independent data processing streams with the given feature subset.
Below we pass the parallel pipelines to `gunion` as a `list` to ensure they receive the same input, and then combine the outputs with `po("featureunion")`.

```{r 05-pipelines-multicol-3-evalF, eval = FALSE}
path_pca = po("select", id = "s_bill", selector = sel_bill) %>>% po("pca")
path_scale = po("select", id = "s_not_bill", selector = sel_not_bill) %>>% po("scale")

gr = gunion(list(path_pca, path_scale)) %>>% po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-multicol-3-evalT, fig.width = 8, eval = TRUE, echo = FALSE}
#| label: fig-pipelines-pcascale
#| fig-cap: Visualization of our `Graph` where features are split into two paths, one with PCA and one with scaling, then combined and returned.
#| fig-alt: 'Seven boxes where first is "<INPUT>" which points to "s_bill -> pca" and "s_not_bill" -> scale", then both "pca" and "scale" point to "featureunion -> <OUTPUT>".'
po_select_bill = po("select", id = "s_bill", selector = sel_bill)
po_select_not_bill = po("select", id = "s_not_bill", selector = sel_not_bill)

path_pca =  po_select_bill %>>% po("pca")
path_scale = po_select_not_bill %>>% po("scale")

gr = gunion(list(path_pca, path_scale)) %>>% po("featureunion")
fig <- magick::image_graph(width = 1500, height = 1000, res = 100, pointsize = 24)
gr$plot(horizontal = TRUE)
invisible(dev.off())
magick::image_trim(fig)
```

The `po("select")` method also has the significant advantage that it allows the same set of features to be used on multiple operations and also allows some features to remain untransformed (`affect_columns` effectively filters out any columns not included in a `Selector`).
`r ref("PipeOpNOP")` performs no operation on its inputs and is thus useful when you only want to perform a transformation on a subset of features and leave the others untouched:

```{r 05-pipelines-multicol-5-evalF, eval = FALSE}
gr = gunion(list(
  po_select_bill %>>% po("scale"),
  po_select_not_bill %>>% po("nop")
)) %>>% po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-multicol-5-evalT, fig.width = 8, eval = TRUE, echo = FALSE}
#| label: fig-pipelines-selectnop
#| fig-cap: Visualization of our `Graph` where features are split into two paths, features that start with 'bill' are scaled and the rest are untransformed.
#| fig-alt: 'Six boxes where first is "<INPUT>" which points to "s_bill -> scale" and "s_not_bill -> nop", then both "scale" and "nop" point to "featureunion -> <OUTPUT>".'
gr = gunion(list(
  po_select_bill %>>% po("scale"),
  po_select_not_bill %>>% po("nop")
)) %>>% po("featureunion")
fig <- magick::image_graph(width = 1500, height = 1000, res = 100, pointsize = 24)
gr$plot(horizontal = TRUE)
invisible(dev.off())
magick::image_trim(fig)
```

```{r 05-pipelines-multicol-6, eval = TRUE}
gr$train(tsk("penguins_simple"))[[1]]$data()[1:3, 1:5]
```

##  Common Patterns and `ppl()` {#sec-pipelines-ppl}

Now you have the tools to create sequential and non-sequential pipelines, you can now create an infinite number of transformations on `Task`, `Learner`, and `Prediction` objects.
In @sec-pipelines-bagging and @sec-pipelines-stack we will work through two complex examples to demonstrate how you can make complex and powerful graphs using the methods and classes we have already looked at.
However, there are many common problems in ML that can be well solved by the same pipelines, and so to make your life easier we have implemented and saved these pipelines in the `r ref("mlr_graphs", aside = TRUE)` dictionary; pipelines in the dictionary can be accessed with the `r ref("ppl()", aside = TRUE)` sugar function.

At the time of writing, this dictionary includes seven `Graph`s (required arguments included):

* `ppl("bagging", graph)`: In `r mlr3pipelines`, bagging (from **b**ootstrapp **agg**regat**ing**), described in detail in @sec-pipelines-bagging, is the process of running a `graph` multiple times on different data samples and then averaging the results. The `iterations` argument controls the number of bagging iterations, `frac` controls the sampling fraction, the averaging method can also be adjusted through the `averager` parameter.
* `ppl("branch", graphs)`: Uses `r ref("PipeOpBranch")` to create different path branches from the given `graphs` where only one branch is evaluated. This is returned to in more detail in @sec-pipelines-branch.
* `ppl("greplicate", graph, n)`: Create a `Graph` that replicates `graph` (which can also be a single `PipeOp`) `n` times. The pipeline avoids ID clashes by adding a suffix to each `PipeOp`, we will see this pipeline in use in @sec-pipelines-bagging.
* `ppl("ovr", graph)`: One-versus-rest classification for converting multiclass classificaiton tasks into several binary classification tasks with one task for each class in the original task. These tasks are then evaluated by the given `graph`, which should be a learner (or a pipeline containing a learner that emits a prediction). The predictions made on the binary tasks are combined into the multiclass prediction needed for the original task.
* `ppl("robustify")`: Perform common preprocessing steps to make any `Task` compatible with a given `Learner`. Optional arguments are the `r ref("Task")` and `r ref("Learner")` in question, as well as individual switches that decide which kind of preprocessing should be done. The "robustify" Graph element queries the metadata provided by the respective objects and performs only the necessary preprocessing. This pipeline is returned to in more detail in @sec-prepro-robustify.
* `ppl("stacking", base_learners, super_learner)`: Stacking, returned to in detail in @sec-pipelines-stack, is the process of using predictions from one or more models (`base_learners`) as features in a subsequent model (`super_learner`)
* `ppl("targettrafo", graph)`: Create a `Graph` that transforms the prediction target of a task and ensures that any transformations applied during training (using the function passed to the `targetmutate.trafo` hyperparameter) are inverted in the resulting predictions (using the function passed to the `targetmutate.inverter` hyperparameter); an example is given in @sec-prepro-scale.


<!-- FIXME: REVIEWED TO HERE -->
## Example: Bagging {#sec-pipelines-bagging}

The basic idea of Bagging, introduced by [@Breiman1996], is to create multiple predictors and then aggregate those to a single, more powerful predictor.
Predictions are aggregated by averaging (regression) or majority vote (classification).
The underlying intuition behind bagging is that averaging a set of weak, but diverse (i.e., only weakly correlated) predictors can reduce the variance of the overall prediction.

This can be achieved by subsampling the data before training a learner, repeating this process a number of times, and then performing a majority vote on the predictions.
A schematic is shown in @fig-pipelines-bagging.


```{r eval = TRUE}
#| label: fig-pipelines-bagging
#| fig-cap: "Graph that performs Bagging by independently subsampling data and fitting individual decision tree learners. The resulting predictions are aggregated by a majority vote PipeOp. Note that the name of the majority vote PipeOp is \"classif.avg\" for naming consistency."
#| fig-alt: "Bagging Graph. Data flows through independent subsampling PipeOps and decision tree learners, to be combined by a majority vote PipeOp."
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/nonlinear_pipeops.svg")
```

Although there is a `"bagging"` entry in `r ref("ppl", "ppl()")` that automatically creates a bagging Graph (@sec-pipelines-ppl), it is instructive to think about how bagging can be constructed from scratch, using the building blocks provided by `mlr3pipelines`.
First, we create a simple pipeline that uses `r ref("PipeOpSubsample", "po(\"subsample\")")` before a learner is trained:

```{r 05-pipelines-non-sequential-009, eval = TRUE}
single_pred = po("subsample", frac = 0.7) %>>% lrn("classif.rpart")
```

This operation can now be copied 10 times using `r ref("pipeline_greplicate", "ppl(\"greplicate\")")`.
`r ref("pipeline_greplicate", "ppl(\"greplicate\")")` allows us to parallelize many copies of an operation by creating a Graph containing `n` copies of the input Graph.
Afterward, the 10 pipelines need to be aggregated to form a single model:

```{r 05-pipelines-non-sequential-010, eval = TRUE}
pred_set = ppl("greplicate", graph = single_pred, n = 10)

bagging = pred_set %>>%
  po("classifavg", innum = 10)
```

The following plot shows the layout of the resulting Graph.

```{r 05-pipelines-non-sequential-012, fig.width = 16, eval = TRUE}
bagging$plot(vertex.label.cex = 1)
```

The bagging pipeline can be converted to a learner using `r ref("as_learner", "as_learner()")`.
The following code compares this pipeline to a single `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` on the "`r ref("mlr_tasks_sonar", "sonar")`" dataset.
This dataset contains sonar response levels in different frequency bands, with the task being to differentiated between metal objects (such as mines) and rocks.
The bagged learner performs noticeably better in this example.
We note, however, that the bagged decision tree is still outperformed by the random forest (`r ref("LearnerClassifRanger", "lrn(\"classif.ranger\")")`).

```{r 05-pipelines-non-sequential-013}
l_bag = as_learner(bagging)
l_bag$id = "bagging"
learner_rpart = lrn("classif.rpart")
rsmp_sonar = rsmp("cv")$instantiate(tsk("sonar"))
grid = benchmark_grid(tsks("sonar"),
  list(l_bag, learner_rpart, lrn("classif.ranger")), list(rsmp_sonar)
)
bmr = benchmark(grid)
bmr$aggregate()
```

We can, however, use `mlr3pipelines` and `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` to come very close to an actual random forest!
The main difference is that a random forest also performs "feature bagging," where only a random subset of available features is considered at each split point.
While this cannot be implemented this directly, the `r ref("PipeOpSelect", "po(\"select\")")` operator and a custom `Selector` can be used to restrict the available features for each tree as an approximation.
We also use the `r ref("pipeline_bagging", "ppl(\"bagging\")")` method mentioned above.
It makes use of the `r ref("Multiplicity")` construct, which is a more efficient way of building massively parallel Graphs.
`r ref("PipeOpClassifAvg", "po(\"classifavg\")")` must therefore be instructed to accept a `r ref("Multiplicity")` object as input, which is done by setting `collect_multiplicity = TRUE`.
Here we use 300 trees, just as the random forest.

```{r 05-bagging-ex}

selector_subsample = function(task) {
  sample(task$feature_names, sqrt(length(task$feature_names)))
}

bagging_quasi_rf = ppl("bagging",
  graph = po("select", selector = selector_subsample) %>>%
    lrn("classif.rpart", minsplit = 1),
  iterations = 300,
  averager = po("classifavg", collect_multiplicity = TRUE)
)

bagging_quasi_rf$param_set$values$subsample.frac = 1
bagging_quasi_rf$param_set$values$subsample.replace = FALSE

l_quasi_rf = as_learner(bagging_quasi_rf)
l_quasi_rf$id = "quasi.rf"

grid = benchmark_grid(tsks("sonar"),
  list(l_quasi_rf, lrn("classif.ranger")), list(rsmp_sonar)
)
bmr = benchmark(grid)
bmr$aggregate(msrs(c("classif.ce", "time_both")))
```

The result shows that the constructed learner behaves and performs very closely to `r ref("ranger::ranger")`.
The `time_both` measure also indicates that our implementation is orders of magnitude slower, because `r ref("ranger::ranger")` is written in C++.
However, constructing this custom learner took much less time, compared to what the authors of `r ref("ranger::ranger")` likely needed.
If the goal is to construct new kinds of learning algorithms that work for a specific purpose, then computational time is often less expensive than developer time!

<!-- FIXME

ADD

  The bagging pipeline shown in @sec-pipelines-bagging is therefore constructed by calling

  ppl("bagging", graph = lrn("classif.rpart"))
-->

## Example: `PipeOpLearnerCV` and Stacking {#sec-pipelines-stack}

Stacking [@Wolpert1992] is another technique that can improve model performance.
The basic idea behind stacking is to use predictions from one model as features for a subsequent model to possibly improve performance.
See @fig-pipelines-stacking for a conceptual illustration.

```{r eval=TRUE, fig.align='center', eval = TRUE}
#| label: fig-pipelines-stacking
#| fig-cap: "Graph that performs Stacking by fitting various models and using their output as features for another model. The `po(\"learner_cv\")` wrapping both a linear model and an SVM will replace the training data by predictions made by these learners. The \"NULL\" (`po(\"nop\")`) operation does not change the training data and makes sure that the original features also remain present. Their combined output is given to the feature union PipeOp, which creates a single training task to be given to the Random Forest learner. "
#| fig-alt: "Stacking Graph. Data flows through independent PipeOps fitting a decision tree, a KNN model, and a LASSO regression model. Their results all flow into a \"Feature Union\" PipeOp, which gives its result to a logistic regression `po(\"learner\")`."
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/stacking.svg")
```

Just as for bagging, it is possible to create a stacking pipeline using `ppl()`, as described in @sec-pipelines-ppl, but we show how to construct it manually as an illustrative example.
Here, the choice is to train an ensemble of different models, the outputs of which are then used as features for a logistic regression model.

To limit overfitting, we must create the stacking features from predictions made for data that was not in the training sample.
Therefore, a `r ref("PipeOpLearnerCV", "po(\"learner_cv\")")` is used, wich performs cross-validation on the training data, fitting a model in each fold.
Each of the models is then used to predict on the out-of-fold data.
As a result, we obtain predictions for every data point in our input data.

We first create various learners that produce the predictions that will be used as features.
These are the "level 0" learners.
Besides the `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` that we have already seen, we also use the k-nearest-neighbor (KNN) learner `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")` and the LASSO learner `r ref("LearnerClassifCVGlmnet", "lrn(\"classif.cv_glmnet\")")`.
We set the `predict_type` to `"prob"` for all learners, so that they produce class probabilities instead of hard class predictions.
We have also set the `kernel` hyperparameter of the KNN learner to `"rectangular"`, which is the simplest distance kernel.
Note that the "`cv`" in the name of the LASSO learner indicates that it performs cross-validation to select the regularization parameter -- it happens independently of the cross-validation performed by `r ref("PipeOpLearnerCV", "po(\"learner_cv\")")`.

```{r 05-pipelines-non-sequential-015, eval = TRUE}
learner_rpart = lrn("classif.rpart", predict_type = "prob")
po_rpart_cv = po("learner_cv",
  learner = learner_rpart,
  resampling.folds = 2, id = "rpart_cv"
)
learner_knn = lrn("classif.kknn",
  kernel = "rectangular",
  predict_type = "prob"
)
po_knn_cv = po("learner_cv",
  learner = learner_knn,
  resampling.folds = 2, id = "knn_cv"
)
learner_glmnet = lrn("classif.cv_glmnet", predict_type = "prob")
po_glmnet_cv = po("learner_cv",
  learner = learner_glmnet,
  resampling.folds = 2, id = "glmnet_cv"
)
```

The level 0 learners are combined using `r ref("gunion", "gunion()")`, and `r ref("PipeOpFeatureUnion", "po(\"featureunion\")")` is used to merge their predictions.
The output produced by an example `$train()` run is instructive:
```{r 05-pipelines-non-sequential-016, eval = TRUE}
level_0 = gunion(list(po_rpart_cv, po_knn_cv, po_glmnet_cv))
combined = level_0 %>>% po("featureunion")

combined$train(tsk("sonar"))
```

:::{.callout-tip}
Each PipeOp has removed the original features and only kept the predictions made by the learners they wrap.
To retain the original features, a `r ref("PipeOpNOP", "po(\"nop\")")` can be added to the list given to `r ref("gunion", "gunion()")`, alongside the level 0 learners.
It pipes the original features through without changing them.
:::

We see that the resulting task contains the predicted probabilities made by each of the level 0 learners.
However, these predictions are redundant, since the probabilities for each class sum to 1.
We will therefore use a `r ref("PipeOpSelect", "po(\"select\")")` to remove the predictions for the "`R`" (Rock) class and keep only the predictions for the "`M`" (Mine) class.
A final PipeOp, containing the learner to be trained on top of the combined features, is appended.

```{r 05-pipelines-non-sequential-017, eval = TRUE}
stack = combined %>>%
  po("select", selector = selector_grep("\\.M")) %>>%
  po("learner", lrn("classif.log_reg"))
```

The resulting layout can be visualized by the Graphs `$plot()` function:
```{r 05-pipelines-non-sequential-018, fig.width = 10, eval = TRUE}
stack$plot(horizontal = TRUE)
```

After training this pipeline, the relative degree by which the `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` weights the level 0 learners can be seen by inspecting its `$model`.
```{r 05-pipelines-non-sequential-019-x, eval = TRUE}
learner_stack = as_learner(stack)
learner_stack$id = "stacking"
learner_stack$train(tsk("sonar"))
learner_stack$graph_model$pipeops$classif.log_reg$learner_model$model
```

It can be observed that the output of the KNN learner influences the overall prediction the most, as it has the largest coefficient.
A benchmark of the individual models confirms that the KNN learner is indeed the best individual model.
The benchmark also indicates that the stacking model performs better than any of the individual models.

```{r 05-pipelines-non-sequential-019-1-background}
grid = benchmark_grid(
  tsks("sonar"),
  list(learner_rpart, learner_knn, learner_glmnet, learner_stack),
  rsmp("holdout")
)
bmr = benchmark(grid)
bmr$aggregate()
```

In real-world applications, stacking can be implemented across multiple levels and on various different representations of the dataset.
On a lower level, different preprocessing methods can be defined in conjunction with several learners.
On a higher level, we can then combine those predictions in order to form a very powerful model.

<!-- FIXME

ADD

The example from @sec-pipelines-stack can thus be written:

  ppl("stacking",
    base_learners = lrns("classif.rpart"),
    super_learner = lrn("classif.rpart")
  )

   -->

## Tuning Graphs {#sec-pipelines-tuning}

Having as many options for preprocessing as provided by `r mlr3pipelines` has many benefits, but it also comes with a drawback:
It enlarges the space of possible hyperparameter configurations considerably.
Not only do preprocessing operations bring their own hyperparameter settings, but the decisions on whether to do preprocessing, and which preprocessing operation to perform, also need to be made.
Tuning ML models with `r mlr3pipelines` can be considered at various levels of complexity:

1. Tuning the hyperparameters of a learner or a PipeOp individually when it is part of a Graph.
2. Jointly tuning the hyperparameters of both learner and its preprocessing operations.
3. Tuning not only the hyperparameters, but also the choice of which operation to perform.

The first level is not much different from tuning individual learners, as described in @sec-optimization.
The only thing to watch out for here is that a learner's hyperparameter names are prefixed with its ID, as shown in @sec-pipelines-inspect.
The second level is demonstrated in the following @sec-pipelines-combined.
The third level is also referred to as the "Combined Algorithm Selection and Hyperparameter optimization" (CASH) [@Thornton2013].
This is demonstrated in @sec-pipelines-branch, which shows how to use alternative path branching.
An alternative way of implementing it is to use `r ref("PipeOpProxy", "po(\"proxy\")")`, demonstrated in the (advanced) @sec-pipelines-proxy.

In this section, we will use the well-known "MNIST" dataset [@lecun1998gradient], which comprises 28 x 28 pixel images of handwritten digits.
It is a classification task with the goal of identifying the digits accurately.
It is often used to demonstrate deep learning with convolutional layers.
However, for this demonstration, we will not use the information about the relative location of pixels.
Instead, we use each pixel's intensity as a separate feature.
We obtain the data from OpenML, which is described in greater detail in @sec-large-benchmarking.
To speed up performance estimation, we subset the given data to 5% of its original size.
```{r 06-pipelines-get-mnist-openml}
library("mlr3oml")
mnist_data = odt(id = 554)
subset = sample(mnist_data$nrow, mnist_data$nrow * 0.05, replace = FALSE)
mnist_task = as_task_classif(mnist_data$data[subset],
    target = "class", id = "mnist")
head(mnist_task$feature_names)
```


### Tuning Combined Spaces {#sec-pipelines-combined}

Instead of using deep learning, we will use the much simpler k-nearest-neighbor (KNN) learner `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")`, again with the simple `"rectangular"` distance kernel.
We investigate if doing a principal component analysis using `r ref("PipeOpPCA", "po(\"pca\")")`, and selecting the highest variance components using the `rank.` hyperparameter, can improve performance.
Because the `k` hyperparameter of the KNN learner also needs to be found, we need to tune it simultaneously.

First, we need to define the Graph that we are tuning.
We have the option of setting each component's hyperparameter being tuned to `to_tune()`, as is done in @sec-optimization, but here we will demonstrate how to define the search space `r ref("ParamSet")` directly.

```{r 05-pipelines-modeling-008}
library("mlr3learners")
graph_learner = po("pca") %>>%
  lrn("classif.kknn", kernel = "rectangular")
graph_learner = as_learner(graph_learner)

library("paradox")
search_space = ps(
  pca.rank. = p_int(
    lower = 2,
    upper = length(mnist_task$feature_names),
    logscale = TRUE
  ),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE)
)
```

We tune this using the `r ref("tune")` function on a 6x6 grid, stepping through both the number of selected principal components (`pca.rank.`) and the KNN's `k` hyperparameter on a log-scale.


```{r debug-dummy, echo=FALSE, eval = TRUE}
instance = list(result_x_domain = list(pca.rank. = 30))
```
```{r 05-pipelines-modeling-009}
library("mlr3tuning")

instance = tune(
  tuner = tnr("random_search"),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  search_space = search_space,
  term_evals = 10
)

instance$result
```

:::{.callout-tip}
Tuning complex pipelines can take a long time:
Not only are the individual training steps often more complex and therefore take longer; the increased search space dimension usually also means that more evaluations need to be performed to get an acceptable level of performance.
It is therefore recommended to make use of parallelization using `r ref_pkg("future")`, which is demonstrated in @sec-parallelization.
:::


Note that the output values are the logarithm of the actual result, which is:
```{r 05-pipelines-modeling-009-2}
instance$result_x_domain
```

The observed performance values are shown in @fig-pipelines-opttrace-1.
It becomes clear that, for different values of `rank.`, different `k` values can be optimal, so jointly tuning both hyperparameters is prudent.

```{r calcbaseline, include = FALSE}
baselineperf = resample(mnist_task, po("pca") %>>% lrn("classif.kknn", kernel = "rectangular"), rsmp("cv", folds = 3))$aggregate(msr("classif.ce"))
baselineperf2 = resample(mnist_task, lrn("classif.kknn", kernel = "rectangular"), rsmp("cv", folds = 3))$aggregate(msr("classif.ce"))
```

```{r fig.align='center', fig.width = 7, fig.height = 3}
#| label: fig-pipelines-opttrace-1
#| fig-cap: "Observed performance values when optimizing both `rank.` of `po(\"pca\")` and `k` of `lrn(\"classif.kknn\")` at the same time.
#|   The x-axis shows `k` values (log scale). Each facet (i.e. individual plot) shows the behavior for a different `rank.` value, displayed at the top.
#|   The red star shows the performance of the untuned model: using `po(\"pca\") %>>% lrn(\"classif.kknn\")` with their defaults: `rank.` set to the number of features, and `k` set to 7."
#| fig-alt: "Plot showing performance values of pca, followed by KNN."
#| echo: false
library("ggplot2")

ggplot(instance$archive$data[, c(rbindlist(x_domain), list(classif.ce = classif.ce))], aes(x = classif.kknn.k, y = classif.ce)) +
  geom_line() +
  ggstar::geom_star(data = data.frame(classif.kknn.k = 7, classif.ce = baselineperf, pca.rank. = length(mnist_task$feature_names)), fill = "red", size = 4, starshape = 1) +
  facet_grid(cols = vars(pca.rank.)) +
  scale_x_log10() +
  theme_minimal() +
  labs(title = "Performance of po(\"pca\") %>>% lrn(\"classif.kknn\")")
```

### Tuning Alternative Paths with `po("branch")` {#sec-pipelines-branch}

While we see that tuning the `r ref("PipeOpPCA", "po(\"pca\")")` has benefits, we have not yet seen whether using PCA at all is beneficial.
It is possible that using the tuned PCA simply does the least damage, or that a much simpler operation would perform equally well or better.

Here we can use the `r ref("PipeOpBranch", "po(\"branch\")")` and `r ref("PipeOpUnbranch", "po(\"unbranch\")")` PipeOps, which make it possible to specify multiple alternative paths.
Data only flows along one of these paths, which can be controlled by a hyperparameter, as is shown in @fig-pipelines-alternatives (a).
This concept makes it possible to tune alternative preprocessing methods or alternative learner models.

`po("(un)branch")` is initialized either with the number of branches, or with a `character`-vector indicating the names of the branches.
If names are given, the "branch-choosing" hyperparameter becomes more readable.
In the following, we set three options:

1. Doing nothing (`r ref("PipeOpNOP", "po(\"nop\")")`)
2. Applying a PCA
3. Removing constant features and applying the Yeo-Johnson transform, using `r ref("PipeOpYeoJohnson", "po(\"yeojohnson\")")`

It is important to "unbranch" again after "branching" to ensure that the outputs are merged into one result objects.

For this demo, we will use the optimal `rank.` value from the previous optimization.

```{r 05-pipelines-non-sequential-003, eval = TRUE}
rank_opt = instance$result_x_domain$pca.rank.

graph = po("branch", c("nop", "pca", "yeojohnson")) %>>%
  gunion(list(
    po("nop"),
    po("pca", rank. = rank_opt),
    po("removeconstants") %>>% po("yeojohnson")
  )) %>>% po("unbranch", c("nop", "pca", "yeojohnson"))
```

The resulting Graph looks as follows:

```{r 05-pipelines-non-sequential-004, fig.width = 11, eval = TRUE}
graph$plot(horizontal = TRUE)
```

The output of this Graph depends on the setting of the `branch.selection` hyperparameter:

```{r 05-pipelines-branch-01}
graph$param_set$values$branch.selection = "pca"  # use the "PCA" path
head(graph$train(mnist_task)[[1]]$feature_names)
graph$param_set$values$branch.selection = "nop"  # use the "No-Op" path
head(graph$train(mnist_task)[[1]]$feature_names)
```

<!-- FIXME:

ADD

  The choice between PCA, Yeo-Johnson transform, and no-op that is shown in  can, for example, be produced by calling:

  ppl("branch", graphs = pos(c("pca", "yeojohnson", "nop")))

 -->

Tuning this hyperparameter can help determine which of the possible options works best in combination with a given learner.
Branching can even be used to tune which of several learners is most appropriate for a given dataset.
We now extend this example so that both the preprocessing (PCA, Yeo-Johnson transform, or no preprocessing), as well as the model to use (KNN or decision tree) can be tuned.
For this, we add another branching pathway to our Graph:

```{r 05-pipelines-branch-02, eval = TRUE}
par(cex = 0.7)
graph_learner = graph %>>%
  po("branch", c("classif.rpart", "classif.kknn"), id = "branch2") %>>%
    gunion(list(
      lrn("classif.rpart"),
      lrn("classif.kknn", kernel = "rectangular")
    )) %>>%
  po("unbranch", c("classif.rpart", "classif.kknn"), id = "unbranch2")
graph_learner = as_learner(graph_learner)
graph_learner$graph$plot()
```

Note that it is necessary to give the two branching operations different IDs to avoid name clashes.

Finally, we would still like to tune the `k` hyperparameter of the KNN learner, as it may depend on the type of preprocessing performed.
However, this hyperparameter is only active when the "`classif.kknn`" path is chosen.
We therefore have to declare a dependency in the search space.
Our search space, which tunes over all options of both branching operators as well as the KNN learner's `k` hyperparameter, is as follows:


```{r 05-pipelines-branch-03}
search_space = ps(
  branch.selection = p_fct(c("nop", "pca", "yeojohnson")),
  branch2.selection = p_fct(c("classif.rpart", "classif.kknn")),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE,
    depends = branch2.selection == "classif.kknn")
)

# set seed for comparison with the alternative optimization method below
set.seed(1)
instance = tune(
  tuner = tnr("random_search"),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  search_space = search_space,
  term_evals = 10
)

cbind(
  as.data.table(instance$result_x_domain),
  classif.ce = instance$result_y
)
```

The results reveal that the tuned KNN performs better than the untuned decision tree, "classif.rpart".
A more thorough investigation could try to tune the decision tree to check if its performance can be brought to the level of the KNN algorithm.
However, increasing the number of options significantly enlarges the search space.
The `grid_search` tuner, which we have used here because of its straightforward interpretability, is not recommended for search spaces that have more than a very small number of dimensions.
Therefore, if this investigation were to be carried further, one would need to use a different optimizer, such as random search or Bayesian optimization, the latter of which is demonstrated in @sec-bayesian-optimization.

```{r fig.align='center', fig.width = 7, fig.height = 3}
#| label: fig-pipelines-opttrace-2
#| fig-cap: "Observed performance values when optimizing both preprocessing (between \"pca\", \"yeojohnson\", and \"nop\", which is no preprocessing) and learner (between KNN and the decision tree \"classif.rpart\") at the same time.
#|   The x-axis shows preprocessing. Each facet (i.e. individual plot) shows the learner being used, together with setting for \"k\", if applicable."
#| fig-alt: "Plot showing performance values of pca, yeojohnson, or nop, followed by KNN or decision tree."
#| echo: false

ggplot(instance$archive$data[, learner := ifelse(is.na(classif.kknn.k), "classif.rpart", sprintf("classif.kknn\nk = % 2s", (sapply(x_domain, `[[`, "classif.kknn.k"))))],
  aes(x = branch.selection, y = classif.ce)) +
  geom_point() +
  facet_grid(cols = vars(learner)) +
  theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Performance of Various Learners with Different Preprocessing")
```

:::{.callout-tip}
Graphs with alternative path branching can also be created using `ppl()`, see @sec-pipelines-ppl
:::

### Tuning with `po("proxy")` {#sec-pipelines-proxy}

{{< include _optional.qmd >}}

The `r ref("PipeOpProxy", "po(\"proxy\")")` operator is a meta-operator that performs the operation that is stored in its `content` hyperparameter.
This can either be another PipeOp, or an entire Graph.
A PipeOp which can itself contain Graphs that can be tuned over, makes it optimization over different operation possible, similarly to `r ref("PipeOpBranch", "po(\"branch\")")` / `r ref("PipeOpUnbranch", "po(\"unbranch\")")`.
@fig-pipelines-alternatives shows the conceptual difference between `r ref("PipeOpBranch", "po(\"branch\")")` and `r ref("PipeOpProxy", "po(\"proxy\")")`.

```{r eval = TRUE}
#| label: fig-pipelines-alternatives
#| layout-nrow: 2
#| fig-cap: "
#|   Two ways of parametrizing the PipeOps or learners that should be used.
#|   The setups shown in both examples have the same effect: Data is PCA-transformed before being fed to the learner.
#|   (a): Using `po(\"branch\")` it is possible to choose which one out of various alternative paths should be taken.
#|        In this example, the PCA PipeOp is active, the alternatives (Yeo-Johnson transform, or no operation through `po(\"nop\")`) are inactive.
#|        A `po(\"unbranch\")` is necessary to mark the end of the alternative paths.
#|   (b): `po(\"proxy\")` has the hyperparameter `content`, which can be set to a another PipeOp. In this example, it is set to PCA.
#| "
#| fig-alt:
#|   - "po(\"branch\"), followed by, alternatively, po(\"nop\"), PCA, and Yeo-Johnson transform, which are all followed by `po(\"unbranch\")` and a learner.
#|      The PCA branch is active."
#|   - "`po(\"proxy\")`, followed by a learner. The content of the `po(\"proxy\")` is set to a PCA PipeOp."
#| fig-subcap:
#|   - "Usage of `po(\"branch\")`"
#|   - "Usage of `po(\"proxy\")`"
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/branching.svg")
knitr::include_graphics("Figures/proxy.svg")
```

To use `r ref("PipeOpProxy", "po(\"proxy\")")` instead of alternative path branching to perform the above optimization, one would first set up a Graph that contains `r ref("PipeOpProxy", "po(\"proxy\")")` operators as placeholders for the operations (preprocessing, learning) that should be tuned.
Note the different IDs to avoid a name clash.

```{r}
graph_learner = po("proxy", id = "preproc") %>>%
  po("proxy", id = "learner")
graph_learner = as_learner(graph_learner)
```

The tuning space for the `content` hyperparameters can now be a discrete set of the possibilities that should be evaluated.
Using `r ref("p_fct", "paradox::p_fct()")` with a named list of PipeOps that should be inserted works here.
Internally, `r paradox` is creating a transformation function here, see @sec-defining-search-spaces for more details.
For the learner-part, a more complex trafo-function is required, as the selection of the learner depends on more than one search space component:
The choice of the learner itself (`r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` or `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")`), as well as the `k` hyperparameter of the KNN learner.
This trafo-function is passed to the `.extra_trafo` argument of `r ref("ps", "ps()")`.
The transformation accepts the hyperparameter configuration that was generated by the search space as input `x`, and returns the configuration to be assigned to the Graph -- both as named lists.
The help page of `r ref("ps", "ps()")` gives more details on this.
Inside this transformation, the `learner.content` value must be clonsed before modification to avoid altering the original `r ref("Learner")` object inside the search space by reference!
Observe how this optimization, when performed with the same seed, yields the same result as above.

```{r}
search_space = ps(
  preproc.content = p_fct(list(
    nop = po("nop"),
    pca = po("pca", rank. = rank_opt),
    yeojohnson = po("removeconstants") %>>% po("yeojohnson")
  )),
  learner.content = p_fct(list(
    classif.rpart = lrn("classif.rpart"),
    classif.kknn = lrn("classif.kknn", kernel = "rectangular")
  )),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE,
    depends = learner.content == "classif.kknn"),
  .extra_trafo = function(x, param_set) {
    if (!is.null(x$classif.kknn.k)) {
      x$learner.content = x$learner.content$clone(deep = TRUE)
      x$learner.content$param_set$values$k = x$classif.kknn.k
      x$classif.kknn.k = NULL
    }
    x
  }
)

set.seed(1)  # for comparison with the optimization above
instance = tune(
  tuner = tnr("random_search"),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  search_space = search_space,
  term_evals = 10
)

as.data.table(instance$result)[,
  .(preproc.content, learner.content,
    classif.kknn.k = x_domain[[1]]$learner.content$param_set$values$k,
    classif.ce)
]
```

## Recap

`r ref_pkg("mlr3pipelines")` provides `r ref("PipeOp")` objects that provide preprocessing, postprocessing, and ensembling operations that can be created using the `r ref("po", "po()")` constructor function.
PipeOps have an ID and hyperparameters that can be set during construction and can be modified later.

PipeOps are concatenated using the `r ref("concat_graphs", "%>>%")`-operator to form `r ref("Graph")` objects.
Graphs can be converted to `r ref("Learner")` objects using the `r ref("as_learner")` function, after which they can be benchmarked and tuned using the tools provided by `r ref_pkg("mlr3")`.
Various standard Graphs are provided by the `r ref("ppl", "ppl()")` constructor function.

## Exercises

4. Consider the `r ref("PipeOpSelect", "po(\"select\")")` in @sec-pipelines-stack that is used to only keep the columns ending in "`M`".
  If the classification task had more than two classes, it would be more appropriate to list the single class we *do not* want to keep, instead of listing all the classes we do want to keep.
  How would you do this, using the `r ref("Selector")` functions provided by `r ref_pkg("mlr3pipelines")`?
  (Note: The `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` learner used in @sec-pipelines-stack cannot handle more than two classes. To build the entire stack, you will need to use a different learner, such as `r ref("LearnerClassifMultinom", "lrn(\"classif.multinom\")")`.)
5. How would you solve the previous exercise without even explicitly naming the class you want to exclude, so that your Graph works for any classification task?
  Hint: look at the `selector_subsample` in @sec-pipelines-bagging.
6. Use the `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` PipeOp to impute missing values in the `r ref("mlr_tasks_penguins", "tsk(\"penguins\")")` task using learners based on `r ref("ranger::ranger")`.
  Hint 1: you will need to use `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` twice, once for numeric features with `r ref("LearnerRegrRanger", "lrn(\"regr.ranger\")")`, and once for categorical features with `r ref("LearnerClassifRanger", "lrn(\"classif.ranger\")")`.
  Using the `affect_columns` argument of `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` will help you here.
  Hint 2: `r ref("ranger::ranger")` itself does not support missing values, but it is trained on all the features of `r ref("mlr_tasks_penguins", "tsk(\"penguins\")")` that it is not currently imputing, some of which will also contain missings.
  A simple way to avoid problems here is to use `r ref("pipeline_robustify", "ppl(\"robustify\")")` *inside* `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` next to the `r ref("ranger::ranger")` learner.

::: {.content-visible when-format="html"}
```{r citeas, cache = FALSE}
citeas(chapter)
```
:::
