# Large-Scale Benchmarking {#sec-large-benchmarking}

{{< include _setup.qmd >}}

`r authors("Large-Scale Benchmarking")`

```{r large_benchmarking-001}
#| include: false
#| cache: false
lgr::get_logger("mlr3oml")$set_threshold("off")
library(mlr3batchmark)
library(batchtools)
library(mlr3oml)
if (!dir.exists(file.path("openml", "manual"))) {
  dir.create(file.path("openml", "manual"), recursive = TRUE)
}
options(mlr3oml.cache = file.path("openml", "cache"))
```

It is often very difficult (if not impossible) to evaluate models using simple mathematical formulae or metrics (like performance measures).
Empirical `r index('benchmark experiments')` are used to evaluate the performance of different algorithms on a wide range of datasets, with the aim of identifying which method performs best.
These empirical investigations are essential for understanding the capabilities and limitations of existing methods and for developing new and improved approaches.
Trustworthy benchmark experiments are often 'large-scale', which means they may make use of many datasets, measures, and models.
Large-scale experiments allow one to make generalization statements about a model's real-world performance, whereas with smaller experiments there are no mathematical guarantees of model performance on datasets beyond those in the experiment.

Large-scale benchmark experiments consist of three primary steps: sourcing the data for the experiment, executing the experiment, and analyzing the results; we will discuss each of these in turn.
In @sec-openml we will begin by discussing `r ref_pkg("mlr3oml")`, which provides an interface between `mlr3` and OpenML [@openml2013], a popular tool for uploading and downloading datasets.
Increasing the number of datasets leads to 'large-scale' experiments that may require significant computational resources.
So in @sec-hpc-exec we will introduce `r ref_pkg("mlr3batchmark")`, which connects `mlr3` with `r ref_pkg("batchtools")` [@batchtools], which provides methods for managing and executing experiments on `r index('high-performance computing', aside = TRUE)` (HPC) clusters.
Finally in @sec-benchmark-analysis we will demonstrate how to make use of `r ref_pkg("mlr3benchmark")` to formally analyze the results from large-scale benchmark experiments.

Throughout this chapter we will use the running example of benchmarking a random forest against logistic regression (this was chosen as this problem has been extensively studied, e.g. @couronne2018random).
We will also assume that you have read @sec-pipelines and @sec-technical.

As a starting example, let us compare our learners across three classification tasks using classification accuracy and holdout resampling.
When conducting large-scale benchmark experiments, the `ppl("robustify")` pipeline (@sec-pipelines) is handy as it performs automatic preprocessing steps including missing value imputation and feature encoding.
As well as loading this pipeline, we also set a featureless baseline as a fallback learner (compare @sec-fallback) and set `"try"` as our encapsulation method, which does not log errors or warnings (we will see why we chose this is useful in @sec-batchtools-monitoring).

```{r large_benchmarking-002}
#| warning: false
# load featureless baseline
lrn_baseline = lrn("classif.featureless", id = "featureless")

# create logistic regression pipeline
lrn_lr = lrn("classif.log_reg")
lrn_lr = as_learner(
  ppl("robustify", learner = lrn_lr) %>>% lrn_lr)
lrn_lr$id = "logreg"
lrn_lr$fallback = lrn_baseline
lrn_lr$encapsulate = c(train = "try", predict = "try")

# create random forest pipeline
lrn_rf = lrn("classif.ranger")
lrn_rf = as_learner(
  ppl("robustify", learner = lrn_rf) %>>% lrn_rf)
lrn_rf$id = "ranger"
lrn_rf$fallback = lrn_baseline
lrn_rf$encapsulate = c(train = "try", predict = "try")

learners = list(lrn_lr, lrn_rf, lrn_baseline)

# create full grid design with holdout resampling
set.seed(123)
design = benchmark_grid(
  tsks(c("german_credit", "sonar", "spam")),
  learners,
  rsmp("holdout")
)

# run the benchmark
bmr = benchmark(design)

# retrieve results
acc = bmr$aggregate(msr("classif.acc"))
acc[, .(task_id, learner_id, classif.acc)]
```

In this small experiment, the random forest appears to outperform the logistic regression on all three datasets.
However, this analysis is not conclusive as we only considered three tasks, used a basic resampling strategy, and the performance differences might not be statistically significant.
So now let us see how to use more advanced methods that will enable us to run a large-scale version of the above experiment to draw some meaningful results.

## Getting Data with OpenML {#sec-openml}

In order to be able to draw meaningful conclusions from benchmark experiments, a good choice of datasets and tasks is essential.
`r index('OpenML', aside = TRUE)` is an open source platform that facilitates the sharing and dissemination of machine learning research data, algorithms, and experimental results, in a standardized format enabling consistent cross-study comparison.
OpenML's design ensures that all data on the platform is `r link("https://www.go-fair.org/fair-principles/", "FAIR")` (**F**indability, **A**ccessibility, **I**nteroperability and **R**eusability), which ensures the data is easily discoverable and reusable.
All entities on the platform have unique identifiers and standardized (meta)data that can be accessed via an open-access REST API or the web interface.

In this section we will cover some of the main features of OpenML and how to use them via the  `r ref_pkg("mlr3oml")` interface package.
In particular we will discuss OpenML datasets, tasks, and task collections, but will not cover algorithms or experiment results here.

### Datasets {#sec-openml-dataset}

Finding data from OpenML is possible via the website or the REST API.
The `r ref("list_oml_data()", aside = TRUE)` function can be used to filter datasets for specific properties, for example by number of features, rows, or number of classes in a classification problem:

```{r}
#| include: false
path_odatasets = file.path("openml", "manual", "odatasets_filter.rds")
```

```{r large_benchmarking-014, eval = !file.exists(path_odatasets)}
odatasets = list_oml_data(
  number_features = c(10, 20), # 10-20 features
  number_instances = c(45000, 50000), # 40,000-50,000 rows
  number_classes = 2 # 2 classes
)[, c("data_id", "name")]
```

```{r}
#| include: false
if (file.exists(path_odatasets)) {
  odatasets = readRDS(path_odatasets)
} else {
  saveRDS(odatasets, path_odatasets)
}
```

We can see that some datasets have duplicated names, which is why each dataset also has a unique ID.
By example, let us consider the 'adult' dataset with ID 1590.
Metadata for the dataset is loaded with the `r ref("mlr3oml::odt()", aside = TRUE)`, which returns an object of class `r ref("OMLData", aside = TRUE)`.

```{r large_benchmarking-003}
library(mlr3oml)
odata = odt(id = 1590)
odata
```

The `r ref("OMLData")` object contains metadata about the dataset but importantly does not (yet) contain the data.
This means that information about the dataset can be queried without having to load the entire data into memory, for example the license and dimension of the data:

```{r large_benchmarking-004}
odata$license
c(nrow = odata$nrow, ncol = odata$ncol)
```

If we want to access the actual data, then calling `$data` will download the data, import it into R, and then store the `data.frame` in the `OMLData` object:

```{r large_benchmarking-005}
# first 5 rows and columns
odata$data[1:5, 1:5]
```

:::{.callout-tip}
After `$data` has been called the first time, all subsequent calls to `$data` will be transparently redirected to the in-memory `data.frame`.
Additionally, many objects can be permanently cached on the local file system by setting the option `mlr3oml.cache` to either `TRUE` or a specific path to be used as the cache folder.
:::

Data can then be converted into backends (see @sec-backends) with the `r ref("as_data_backend()")` function and then into tasks:

```{r large_benchmarking-006}
backend = as_data_backend(odata)
task = as_task_classif(backend, target = "class")
task
```

Some datasets on OpenML contain columns that should neither be used as a feature nor a target.
The column names that are usually included as features are accessible through the field `$feature_names` and we assign them to the `mlr3` task accordingly.
Note that for the dataset at hand this would not have been necessary, as all non-target columns are to be treated as predictors, but we include it for clarity.

```{r}
task$col_roles$feature = odata$feature_names
task
```

### Task {#sec-openml-task}

OpenML tasks are built on top of OpenML datasets and additionally specify the target variable, the train-test splits to use for resampling, and more.
Similarly to `mlr3`, OpenML has different types of tasks, such as regression and classification.
Analogously to filtering datasets, tasks can be filtered with `r ref("list_oml_tasks()", aside = TRUE)`, so if we want to find a task that makes use of the data we have been looking at we would pass the data ID to the `data_id` argument:

```{r}
# first 5 tasks making use of the adult data
adult_tasks = list_oml_tasks(data_id = 1590)
```

There are `r nrow(adult_tasks)` that use our unique dataset so for now we will randomly choose task 359983.
We can load the object using `r ref("mlr3oml::otsk()", aside = TRUE)`, which returns an `r ref("OMLTask", aside = TRUE)` object.

```{r large_benchmarking-009}
otask = otsk(id = 359983)
otask
```

The `r ref("OMLData")` object associated with the underlying dataset can be accessed through the `$data` field.

```{r large_benchmarking-010}
otask$data
```

The data splits associated with the estimation procedure are accessible through the field `$task_splits`.
In mlr3 terms, these are the instantiation of an `mlr3` `r ref("Resampling")` on a specific `r ref("Task")`.

```{r large_benchmarking-011}
otask$task_splits
```

The OpenML task can be converted to both an mlr3 `r ref("Task")` and a `r ref("ResamplingCustom")` instantiated on the task using `r ref("as_task()")` and `r ref("as_resampling()")` respectively:

```{r large_benchmarking-012}
task = as_task(otask)
task

resampling = as_resampling(otask)
resampling
```

`mlr3oml` also allows direct constuction of `mlr3` tasks and resamplings with the standard `tsk` and `rsmp` constructors, e.g.,:

```{r}
tsk("oml", task_id = 359983)
```

Once datasets/tasks are selected, further investigation may be required to see if they are suitable for a given experiment.
Curated task collections are therefore useful to reduce this workload.

### Task Collection {#sec-openml-collection}

The OpenML task collection is a container object bundling existing tasks.
This allows for the creation of `r index("benchmark suites")`, which are curated collections of tasks that satisfy certain quality criteria.
Examples include the OpenML CC-18 benchmark suite [@bischl2021openml], the AutoML benchmark [@amlb2022] and the benchmark for tabular deep learning [@grinsztajn2022why].
`r ref("OMLCollection", aside = TRUE)` objects are loaded with using `r ref("mlr3oml::ocl()", aside = TRUE)`, by example we will look at CC-18, which has ID 99:

```{r}
#| include: false
path_otask_collection = file.path("openml", "manual", "otask_collection99.rds")
```

```{r large_benchmarking-017, eval = !file.exists(path_otask_collection)}
otask_collection = ocl(id = 99)
```

```{r}
#| include: false
if (file.exists(path_otask_collection)) {
  otask_collection = readRDS(path_otask_collection)
} else {
  # need to trigger the download
  otask_collection$task_ids
  saveRDS(otask_collection, path_otask_collection)
}
```

```{r large_benchmarking-019}
otask_collection
```

The task includes 72 classification tasks on different datasets and can be accessed through `$task_ids`:

```{r large_benchmarking-020}
otask_collection$task_ids[1:5] # five 5 tasks in the collection
```

Task collections can be used to quickly define benchmark experiments in `mlr3`.
To quickly define tasks and resampling strategies for the experiment you can use `r ref("as_tasks()")` and `r ref("as_resamplings()")` respectively:

```{r}
tasks = as_tasks(otask_collection)
resamplings = as_resamplings(otask_collection)
```

Alternatively, if we wanted to filter the collection further, say to a binary classification experiment with six tasks, we could run `r ref("list_oml_tasks()")` with the task IDs from the CC-18 collection as argument `task_id` and request the number of classes to be 2.

```{r}
#| include: false
path_binary_cc18 = file.path("openml", "manual", "binary_cc18.rds")
```

```{r large_benchmarking-021, eval = !file.exists(path_binary_cc18)}
binary_cc18 = list_oml_tasks(
  limit = 6,
  task_id = otask_collection$task_ids,
  number_classes = 2
)
```

```{r}
#| include: false
if (!file.exists(path_binary_cc18)) {
  saveRDS(binary_cc18, path_binary_cc18)
} else {
  binary_cc18 = readRDS(path_binary_cc18)
}
```

We now define the learners, tasks, and resamplings for the experiment.
In addition to the random forest and the logistic regression, we also include a featureless learner as a baseline.

```{r large_benchmarking-024}
# load tasks as  a list
otasks = lapply(binary_cc18$task_id, otsk)
# convert to mlr3 tasks and resamplings
tasks = as_tasks(otasks)
resamplings = as_resamplings(otasks)
```

To define the design table, we use `r ref("benchmark_grid()")` and set `paired` to `TRUE`, which is used in situations where each resampling is instantiated on a corresponding task (therefore the `tasks` and `resamplings` below must have the same length) and each learner should be evaluated on every resampled task.

```{r large_benchmarking-025}
large_design = benchmark_grid(
  tasks, learners, resamplings, paired = TRUE
)
large_design
```

Having setup our large experiment, we can now look at how to efficiently carry it out on a cluster.

## Experiment Execution on HPC Clusters {#sec-hpc-exec}

As discussed in @sec-parallelization, parallelization of benchmark experiments is straightforward as they are `r index('embarrassingly parallel')`.
However, for large experiments, parallelization on a local machine will, at best be slow, and at worst will stop you using your computer for any other task, as such `r index('high-performance computing (HPC)')` clusters are generally required.
While physical access to HPC clusters is increasingly simply, the effort required to work on these systems can still be complex.
The R package `r ref_pkg("batchtools")` provides a framework to simplify running large batches of computational experiments in parallel from R.
It is highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimization, simulation, and more.

:::{.callout-tip}
In @sec-parallel-resample we have touched upon different parallelization backends.
The package `r ref_pkg("future")` includes a `"batchtools"` plan, however this does not allow the additional control that comes with working with `r ref_pkg("batchtool")` directly.
:::

An HPC cluster is a collection of interconnected computers or servers providing computational power beyond what a single computer can achieve.
HPC clusters typically consist of multiple compute nodes, each with multiple CPU/GPU cores, memory, and local storage.
These nodes are connected together by a high-speed network and network file system which enables the nodes to communicate and work together on a given task.
These clusters are also designed to run parallel applications that can be split into smaller tasks and distributed across multiple compute nodes to be executed simultaneously.
The most important difference between HPC clusters and a personal computer (PC), is that the nodes cannot be accessed directly, but instead computational jobs are queued by a `r index("scheduling system")` such as Slurm (Simple Linux Utility for Resource Management).
A scheduling system is a software tool that orchestrates the allocation of computing resources to users or applications on the cluster.
It ensures that multiple users and applications can access the resources of the cluster in a fair and efficient manner, and also helps to maximize the utilization of the computing resources.

@fig-hpc contains a rough sketch of an HPC architecture.
Multiple users can log in into the head node (typically via SSH) and add their computational workloads (or `r index('computational job')`) to the queue by sending a command of the form "Execute Computation X using Resources Y for Z amount of time".
The scheduling system controls when these computational jobs are executed.

For the rest of this section we will look at how to use `r ref_pkg("batchtools")` and `r ref_pkg("mlr3batchmark")` for submitting jobs, adapting jobs to clusters, ensuring reproducibility, querying job status, and debugging failures.

```{r large_benchmarking-026}
#| label: fig-hpc
#| fig-cap: "Illustration of an HPC cluster architecture."
#| fig-align: "center"
#| fig-alt: "A rough sketch of the architecture of an HPC cluster. Ann and Bob both have access to the cluster and can log in to the head node. There, they can submit jobs to the scheduling system, which adds them to its queue and determines when they are run."
#| echo: false
knitr::include_graphics("Figures/hpc.drawio.png")
```

### General Setup and Experiment Registry {#sec-registry}

The central concept of `r ref_pkg("batchtools")` is the experiment or '`r index("job")`'.
One replication of a job is defined by applying a (parameterized) algorithm to a (parameterized) problem.
A benchmark experiment in batchtools consists of running many such experiments with different algorithms, algorithm parameters, problems, and problem parameters.
Each such experiment is computationally independent of all other experiments and constitutes the basic level of computation batchtools can parallelize.
For this section we will define a single batchtools experiment as one resampling iteration of one learner on one task, in @sec-custom-experiment-definition we will look at different ways of defining an experiment.

The first step in running an experiment is to create or load an experiment registry with `r ref("batchtools::makeExperimentRegistry()", aside = TRUE)` or `r ref("batchtools::loadRegistry()")` respectively.
This constructs the inter-communication object for all functions in `r ref_pkg("batchtools")` and corresponds to a folder on the file system.
Among other things, the experiment registry stores the algorithms, problems, and job definitions; log outputs and status of submitted, running, and finished jobs; job results; and the cluster function that defines the interaction with the scheduling system in a scheduling-software-agnostic way.

Below, we create a registry in a subdirectory of our working directory -- on a real cluster, make sure that this folder is stored on a shared network filesystem, otherwise the nodes cannot access it.
We also set the registry's `seed` to 1 and the `packages` to `r ref_pkg("mlr3verse")`, which will make these packages available in all our experiments.

```{r include = FALSE}
#| cache: false
if (dir.exists("experiments"))
    unlink("experiments", recursive = TRUE)
```

```{r large_benchmarking-027, message=FALSE, warning=FALSE}
#| cache: false
library(batchtools)

# create registry
reg = makeExperimentRegistry(
  file.dir = "./experiments",
  seed = 1,
  packages = "mlr3verse"
)
```

When printing our newly created registry, we see that there are 0 problems, algorithms or jobs registered.
Among other things, we are informed that the "Interactive" cluster function (see `r ref("batchtools::makeClusterFunctionsInteractive()")`) is used and the working directory for the experiments is printed.

```{r large_benchmarking-028}
reg
```
<!-- FIXME: REVIEWED TO HERE -- WILL LIKELY WANT TO COMBINE SOME OF THESE SECTIONS AS THEY GET SHORTER -->

### Experiment Definition using mlr3batchmark {#sec-mlr3batchmark}

The next step is to populate the registry with problems and algorithms, which we will then use to define the jobs, i.e., the resampling iterations.
This is the first step where `r ref_pkg("mlr3batchmark")` comes into play.
Doing this step with `r ref_pkg("batchtools")` is also possible and gives you more flexibility and is demonstrated in @sec-custom-experiment-definition.
By calling `r ref("batchmark()")`, mlr3 tasks and mlr3 resamplings will be translated to batchtools problems, and mlr3 learners are mapped to batchtools algorithms.
Then, jobs for all resampling iterations are created.

```{r large_benchmarking-029}
#| cache: false
#| output: false
library(mlr3batchmark)
batchmark(large_design, reg = reg)
```

::: {.callout-tip}
All batchtools functions that interoperate with a registry take a registry as an argument.
By default, this argument is set to the last created registry, which is currently the `reg` object defined earlier.
We nonetheless pass it explicitly in this section for clarity, but would not have to do so.
:::

When printing the registry, we confirm that six problems (one for each resampled task) but only a single algorithm is registered.
While a 1-on-1 mapping of the 3 algorithms in the design would have also been possible, the approach in `r ref_pkg("mlr3batchmark")` uses a single algorithm parametrized with the learner identifiers instead for efficiency.
Furthermore, $180 = 3 \times 6 \times 10$ jobs, i.e. one for each resampling iteration, are registered.

```{r large_benchmarking-030}
reg
```

We can summarize the defined experiments using `r ref("batchtools::summarizeExperiments()")`.
There are 10 jobs for each combination of a learner and resampled task, as 10-fold cross-valdation is used as the resampling procedure.

```{r large_benchmarking-031}
summarizeExperiments(
  by = c("task_id", "learner_id"), reg = reg)
```

The function `r ref("batchtools::getJobTable()")` can be used to get more detailed information about the jobs.
Here, we only show a few selected columns for readability and unpack the list columns `algo.pars` and `prob.pars` using `r ref("batchtools::unwrap()")`.
Among other things, we see that each job has a unique `job.id`.
Each row in this job table represents one iteration (column `repl`) of a resample experiment.

```{r large_benchmarking-032}
job_table = getJobTable(reg = reg)
job_table = unwrap(job_table)
job_table = job_table[,
  .(job.id, learner_id, task_id, resampling_id, repl)
]

job_table
```


### Job Submission {#sec-batchtools-submission}

Once the experiments are defined, the next step is to submit them.
Before doing so, it is recommended to test each algorithm individually using `r ref("batchtools::testJob()")`.
We test the job with `job.id = 1` exemplary and specify `external = TRUE` to run the test in an external R session.
The return of the `testJob()` function (which is the return value of the `"run_learner"` algorithm) is a bit technical, because here not the complete objects but only the essential parts are returned to reduce the communication overhead (a named list `learner_state` and a named list `prediction`) - we do not give a detailed description of the list elements here.
We can for example access the training time through the `learner_state` as shown below.

```{r large_benchmarking-033}
#| output: true
result = testJob(1, external = TRUE, reg = reg)
result$learner_state$train_time
```

In case something goes wrong, `r ref_pkg("batchtools")` comes with a bunch of useful debugging utilities covered in @sec-batchtools-monitoring.
Once we are confident that the jobs are defined correctly, we can proceed with their submission, which requires

1. specifying resource requirements for each computational job, and
1. (optionally) grouping multiple jobs into one computational job

Which resources can be configured depends on the cluster function that is set in the registry.
We earlier left it at its default value, which is the "Interactive" cluster function.
In the following we assume that we are working on a Slurm cluster.
Accordingly, we initialize the cluster function with `r ref("batchtools::makeClusterFunctionsSlurm()")` and a predefined `r link("https://github.com/mllg/batchtools/blob/master/inst/templates/slurm-simple.tmpl", "slurm-simple template")`.
A template file is a shell script with placeholders filled in by `batchtools` and contains

1. the command to start the computation via `Rscript` or `R CMD batch`, usually in the last line, and
1. comments which serve as annotations for the scheduler, e.g. to communicate resources or paths on the file system.

The exemplary template should work on many Slurm installations out-of-the-box, but can also easily be customized to also work with more advanced configurations.

```{r large_benchmarking-034}
cf = makeClusterFunctionsSlurm(template = "slurm-simple")
```

To proceed with the examples on a local machine, set the cluster functions to a Socket backend.

```{r large_benchmarking-035}
cf = makeClusterFunctionsSocket()
```

```{r}
#| include: false
cf = makeClusterFunctionsInteractive()
```

:::{.callout-tip}
It is possible to customize the cluster function.
More information is available in the documentation of the `r ref_pkg("batchtools")` package.
:::

We update the value of the `$cluster.functions` and save the registry.
This only has to be done manually when modifying the fields explicitly.
Functions like `r ref("batchmark()")` internally save the registry when required.

```{r large_benchmarking-036}
#| eval: true
#| cache: false
reg$cluster.functions = cf
saveRegistry(reg = reg)
```

The jobs are submitted to the scheduler via `r ref("batchtools::submitJobs()")`.
The most important arguments of this function besides the registry are:

* `ids`, which are either a vector of job IDs to submit, or a data frame with columns `job.id` and `chunk`, which allows grouping multiple jobs into one larger computational job and thereby controlling the granularity.
  This often makes sense on HPCs, as submitting and running computational jobs comes with a considerable overhead and there are often hard limits for the maximum runtime (walltime).
* `resources`, which is a named list specifying the resource requirements for the submitted jobs.

We will `r ref("batchtools::chunk()")` the IDs in such a way that 5 iterations of one resample experiment are run sequentially in one computational job.
The optimal grouping depends on the concrete experiment and scheduling system.

```{r large_benchmarking-037}
ids = job_table$job.id
chunks = data.table(
  job.id = ids, chunk = chunk(ids, chunk.size = 5, shuffle = FALSE)
)
chunks
```

Furthermore, we specify the number of CPUs per computational job to 1, the walltime to 1 hour (3600 seconds), and the RAM limit to 8 GB.
The set of resources depends on your cluster and the corresponding template file.
For a list of resource names that are standardised across most implementations, see `r ref("batchtools::submitJobs()")`.
If you are unsure about the resource requirements, you can start a subset of jobs with conservative resource constraints, e.g. the maximum runtime allowed for your computing site.
Measured runtimes and memory usage can be queried with `r ref("batchtools::getJobTable()")` and finally used to better estimate the required resources for the remaining jobs.

<!-- output to false otherwise we see the interactive cluster function -->
```{r large_benchmarking-038}
#| cache: false
#| output: false
submitJobs(
  ids = chunks,
  resources = list(ncpus = 1, walltime = 3600, memory = 8000),
  reg = reg
)

# wait for all jobs to terminate
waitForJobs(reg = reg)
```


:::{.callout-tip}
A good approach to submit computational jobs is to do this from an R session that is running persistently.
One option is to use `r link("https://github.com/tmux/tmux/wiki", "TMUX (Terminal Multiplexer)")` on the head node to continue job submission (or computation, depending on the cluster functions) in the background.
:::


###  Job Monitoring, Error Handling, and Result Collection {#sec-batchtools-monitoring}



After submitting the jobs, the next phase is to wait for them to finish.
In case you terminated your running R session after job submission, you can load the experiment registry using `r ref("loadRegistry()")` to continue where you left off.

In any large scale experiment many things can and will go wrong, even if we test our jobs beforehand using `r ref("batchtools::testJob()")` as recommended earlier.
The cluster might have an outage, jobs may run into resource limits or crash, subtle bugs in your code could be triggered or any other error condition might arise.
In these situations it is important to quickly determine what went wrong and to recompute only the minimal number of required jobs.

The current status of the computation can be queried with `r ref("getStatus()")`, which lists the number of jobs categorized in multiple status groups:

```{r large_benchmarking-041}
getStatus(reg = reg)
```

To query the ids of jobs in the respective categories, see `r ref("findJobs()")` and, e.g., `findNotSubmitted()` or `findDone()`.

In our case, all experiments finished and none expired or crashed - albeit we took some countermeasures by extending our base learners with fallback learners and integrating them in the pipeline "robustify".
Nonetheless, it makes sense to check the logs for suspicious messages and warnings with `r ref("grepLogs()")` before proceeding with the analysis of the results.
For this purpose, we have set the encapsulation before to `"try"`: in contrast to `"evaluate"` or `"callr"` encapsulation, `"try"` does not capture the output from messages, warnings or errors and store them in the learner's log.
All output is printed to the console and redirected to a log file so that `batchtools` can operate on it with functions like `r ref("getLog()")` or `r ref("grepLogs()")`.

In the following, we extend the design with the debug learner (see @sec-error-handling) erring with a 50% probability.
By just calling `r ref("batchmark()")` with the new design again, the new experiments will be added to the registry on top of the existing jobs.
The `tasks` and `resamplings` below are once again those from the `large_design` from earlier.

```{r}
#| cache: false
#| output: false
extra_design = benchmark_grid(
  learners = lrns("classif.debug", error_train = 0.5),
  tasks = tasks,
  resampling = resamplings,
  paired = TRUE
)

batchmark(extra_design, reg = reg)
```

We can check the new state and submit the jobs which have not been submitted yet (i.e., the newly created jobs):

```{r}
#| cache: false
getStatus(reg = reg)
ids = findNotSubmitted(reg = reg)
```

We queue these jobs as usual by passing their IDs to `submitJobs()`:

```{r}
#| cache: false
#| output: false
submitJobs(ids, reg = reg)
waitForJobs(reg = reg)
```

After these jobs have terminated, we can get a summary of those that failed:

```{r}
error_ids = findErrors(reg = reg)
summarizeExperiments(
  error_ids, by = c("task_id", "learner_id"), reg = reg)
```

Unsurprisingly, all failed jobs have one thing in common: the debug learner.

Finally, it is time to collect the experiment output.
The results for benchmark experiments defined with `r ref("batchmark()")` can be collected with `r ref("reduceResultsBatchmark()")`, which constructs a regular `r ref("BenchmarkResult")`.
It is also possible to retrieve single results with `r ref("loadResult()")`, but the returned values are hard to work with, as they are not optimized for usability, but efficiency.
Here, we only collect results which were not produced by the debug learner.

```{r}
ids = findExperiments(
  algo.pars = learner_id != "classif.debug", reg = reg)
bmr = reduceResultsBatchmark(ids, reg = reg)
bmr$aggregate()
```



### Custom Experiment Definition {#sec-custom-experiment-definition}

{{< include _optional.qmd >}}

While `r ref_pkg("mlr3batchmark")` excels for conducting benchmarks on HPCs, there can be situations in which more fine-grained control over the experiment definition is beneficial or even required.
Here, we will show how to define batchtools jobs that execute an mlr3 benchmark experiment without the help of `r ref_pkg("mlr3batchmark")`.
There is not one single way how to achieve this, and we here show a solution that sacrifices efficiency for simplicity.
Unless you have a specific reason to customize your experiment definition, we recommend using `r ref_pkg("mlr3batchmark")`.
Like before, the first step is to create an experiment registry.

```{r include = FALSE}
#| cache: false
if (dir.exists("experiments-custom"))
  unlink("experiments-custom", recursive = TRUE)
```

```{r large_benchmarking-046}
#| cache: false
reg = makeExperimentRegistry(
  file.dir = "./experiments-custom",
  seed = 1,
  packages = "mlr3verse"
)
```

We can register a problem by calling `r ref("batchtools::addProblem()")`, whose main arguments beside the registry are:

* `name` to uniquely identify the problem,
* `data` to represent the static data part of a problem, and
* `fun`, which takes in the problem `data`, the problem parameters, and the `job` definition, see `r ref("batchtools::makeJob()")` and returns a problem `instance`.

We register all task-resampling combinations of the `large_design` using the task ID as the name.^[The mlr3 task ID is not the same as the OpenML task ID.]
The problem `fun` takes in the static problem `data` and returns it as the problem `instance` as is.
If we were using problem parameters, we could modify the problem instance depending on their values.
In the code below, recall that the `tasks` and `resamplings` were originally used to define the `large_design`.

```{r large_benchmarking-047}
#| output: false
#| cache: false
for (i in seq_along(tasks)) {
  addProblem(
    name = tasks[[i]]$id,
    data = list(task = tasks[[i]], resampling = resamplings[[i]]),
    fun = function(data, job, ...) data,
    reg = reg
  )
}

```

When calling `r ref("batchtools::addProblem()")`, not only is the problem added to the registry object from the active R session, but this information is also synced with the registry folder.

The next step is to register the algorithm we want to run, which we achieve by calling `r ref("batchtools::addAlgorithm()")`.
Besides the registry, it takes in the arguments:

* `name` to uniquely identify the algorithm and
* `fun`, which takes in the problem instance, the algorithm parameters, and the `job` definition.
  It defines the computational steps of an experiment and its return value is the experiment result, i.e. what can later be retrieved using `r ref("loadResult()")`.

The algorithm function receives a list containing the task and resampling as the problem `instance`, the `learner` as the algorithm parameter, and the `job` object.
It then executes the resample experiment defined by these three objects using `r ref("resample()")` and returns a `r ref("ResampleResult")`.
This differs from the `r ref_pkg("mlr3batchmark")`, where one resampling iteration corresponds to one job in `r ref_pkg("batchtools")`.
Here, one `r ref_pkg("batchtools")` job represents a complete resample experiment.

```{r large_benchmarking-048}
#| cache: false
addAlgorithm(
  "run_learner",
  fun = function(instance, learner, job, ...) {
    resample(instance$task, learner, instance$resampling)
  },
  reg = reg
)

reg$algorithms
```

As we have now defined the problems and the algorithm, we can define concrete experiments using `r ref("batchtools::addExperiments()")`.
This function has arguments

* `prob.designs`, a named list of data frames. The name must match the problem name while the column names correspond to parameters of the problem.

* `algo.designs`, a named list of data frames. The name must match the algorithm name while the column names correspond to parameters of the algorithm.

In the code below, we add all resampling iterations for the six tasks as experiments.
By leaving `prob.designs` unspecified, experiments for all existing problems are created per default.
We set the algorithm parameters to all possible `learners`, i.e. the logistic regression, random forest, and featureless learner from `large_design`.
Note that whenever an experiment is added, the current seed is assigned to the experiment and then incremented.

```{r large_benchmarking-049}
#| cache: false
#| output: false
library(data.table)

algorithm_design = list(run_learner = data.table(learner = learners))
print(algorithm_design$run_learner)

addExperiments(algo.designs = algorithm_design, reg = reg)
```

We confirm that the algorithm, problems, and experiments (jobs) were added successfully.

```{r large_benchmarking-050}
summarizeExperiments()
```

@fig-batchtools-illustration summarizes the interplay between the batchtools problems, algorithms, and experiments.

```{r large_benchmarking-051}
#| label: fig-batchtools-illustration
#| fig-cap: "Illustration the batchtools problem, algorithm, and experiment. "
#| fig-align: "center"
#| fig-alt: "A problem consists of a static data part and applies the problem function to this data part (and potentially problem parameters) to return a problem instance. The algorithm function takes in a problem instance (and potentially algorithm parameters), executes one job and returns its result."
#| echo: false
knitr::include_graphics("Figures/tikz_prob_algo_simple.png")
```


We are now ready to submit the jobs to the cluster.
By specifying no job IDs, all experiments are submitted as independent jobs, i.e. one computational job executes one resample experiment.

```{r}
#| output: false
#| cache: false
submitJobs(reg = reg)
waitForJobs(reg = reg)
```

When a cluster jobs finishes, it stores its return value in the registry folder.
We can retrieve the job results using `r ref("batchtools::loadResult()")`.
It outputs the objects returned by the algorithm function, which in our case is a `r ref("ResampleResult")`.

```{r large_benchmarking-054}
rr = loadResult(1, reg = reg)
rr
```

In order to use mlr3's post-processing tools, we need to convert all results into a `r ref("BenchmarkResult")`.
We can do this, by combining all resample results using `r ref("batchtools::reduceResults()")`.

```{r large_benchmarking-055}
bmr = reduceResults(c, reg = reg)
bmr$aggregate()
```

While we took a different route than in @sec-mlr3batchmark to define the experiments and ran them at a different granularity, we arrived at the same result.


## Statistical Analysis {#sec-benchmark-analysis}

Once we successfully executed the benchmark experiment, we can proceed with its analysis.
The package `r ref("mlr3benchmark")` provides infrastructure for applying statistical significance tests on `r ref("BenchmarkResult")` objects.
Currently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported to analyze benchmark experiments with at least two independent tasks and at least two learners.
Before we can use these methods, we have to convert the benchmark result to a `r ref("mlr3benchmark::BenchmarkAggr")` using `r ref("as_benchmark_aggr()")`.
We can then perform a pairwise comparison using `$friedman_posthoc()`.
This method will first perform a global friedman test and only conduct the post-hoc tests if the former is significant.

```{r large_benchmarking-056}
library(mlr3benchmark)
bma = as_benchmark_aggr(bmr, measures = msr("classif.ce"))
bma$friedman_posthoc()
```

These results would indicate a statistically significant difference between the `"featureless"` learner and `"ranger"`, assuming a 95% confidence level.
This table can be summarized in a critical difference plot, which typically shows the mean rank of a learning algorithm on the x-axis along with a thick horizontal line that connects learners which are pairwise not significantly different (while correcting for multiple tests):

```{r large_benchmarking-057}
autoplot(bma, type = "cd")
```

While our experiment did now show a significant difference between the random forest and the logistic regression (they are connected in the plot), the former has a lower rank on average.
This is in line with the large benchmark study conducted by @couronne2018random, where the random forest outperformed the logistic regression in 69% of 243 real world datasets.

As a final note, it is important to be careful when interpreting such test results.
Because our datasets are not an iid sample from a population of datasets, we can only make inference about the data generating processes at hand, i.e. those that generated the datasets we used in the benchmark.

## Conclusion

In this chapter, we have explored how to conduct large scale machine learning experiments using mlr3.
We have shown how to acquire diverse datasets from OpenML through the `r ref_pkg("mlr3oml")` interface package.
Furthermore, we have learned how to execute large-scale experiments using the `r ref_pkg("batchtools")` package and its `r ref_pkg("mlr3batchmark")` integration.
Finally, we have demonstrated how to analyze the results using the `r ref_pkg("mlr3benchmark")` package, thereby extracting meaningful insights from the experiments.

The most important functions and classes we learned about are in @tbl-api-large-benchmarking alongside their R6 classes (if applicable).

| S3 function                         | R6 Class                   | Summary                                                             |
| ------------------------------------| ---------------------------| --------------------------------------------------------------------|
| `r ref("odt()")`                    | `r ref("OMLData")`         | Retrieve an OpenML Dataset                                          |
| `r ref("otsk()")`                   | `r ref("OMLTask")`         | Retrieve an OpenML Task                                             |
| `r ref("ocl()")`                    | `r ref("OMLCollection")`   | Retrieve an OpenML Collection                                       |
| `r ref("list_oml_data()")`          |-                           | Filter OpenML Datasets                                              |
| `r ref("list_oml_tasks()")`         |-                           | Filter OpenML Tasks                                                 |
| `r ref("makeExperimentRegistry()")` |-                           | Create a new registry                                               |
| `r ref("loadRegistry()")`           |-                           | Load an existing registry                                           |
| `r ref("saveRegistry()")`           |-                           | Save an existing registry                                           |
| `r ref("batchmark()")`              |-                           | Register problems, algorithms, and experiments from a design        |
| `r ref("addProblem()")`             |-                           | Register a new Problem                                              |
| `r ref("addAlgorithm()")`           |-                           | Register a  new algorithm                                           |
| `r ref("addExperiments()")`         |-                           | Register experiments using existing algorithms and problems         |
| `r ref("submitJobs()")`             |-                           | Submit jobs to the scheduler                                        |
| `r ref("getJobTable()")`            |-                           | Get an overview of all job definitions                              |
| `r ref("unwrap()")`                 |-                           | Get an overview of all job definitions                              |
| `r ref("getStatus()")`              |-                           | Get the status of the computation                                   |
| `r ref("reduceResultsBatchmark()")` |-                           | Load finished jobs as a benchmark result                            |
| `r ref("reduceResults()")`          |-                           | Combine experiment results                                          |
| `r ref("findExperiments()")`        |-                           | Find specific experiments                                           |
| `r ref("grepLogs()")`               |-                           | Search the log files                                                |
| `r ref("summarizeExperiments()")`   |-                           | Summarize defined experiments                                       |
| `r ref("getLog()")`                 |-                           | Get a specific log file                                             |
| `r ref("findErrors()")`             |-                           | Find ids of failed jobs                                             |

:Core functions for Open in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-large-benchmarking}

### Resources{.unnumbered .unlisted}

- Look at the short `r link("https://doi.org/10.21105/joss.00135", "batchtools paper")`, and please cite this if you use the package.
- Read the `r link("https://www.jstatsoft.org/v64/i11", "Paper on BatchJobs / BatchExperiments")` which are the batchtools predecessors, but the concepts still hold, and most examples work analogously.
- Learn more in the `r link("https://mllg.github.io/batchtools/articles/batchtools.html", "batchtools vignette")`.
- Explore the `r link("https://docs.openml.org/", "OpenML documentation")`.

## Exercises

In this exercise we will conduct an empirical study that compares two machine learning algorithms.
Our null hypothesis is that a single regression tree performs equally well than a random forest.

### Getting Data from OpenML {.unnumbered .unlisted}

1. Load the OpenML collection with ID `r link("269", "https://www.openml.org/search?type=study&study_type=task&id=269")`. It contains regression tasks from the AutoML benchmark [@amlb2022].
2. Find all tasks with less than 4000 observations and convert them to mlr3 tasks.
3. Create an experimental design that compares the random forest in `r ref_pkg("ranger")` with the regression tree from `r ref_pkg("rpart")` on those tasks.
   You can use 3-fold cross-validation instead of the OpenML resamplings to save time.

### Executing the Experiments using batchtools {.unnumbered .unlisted}

1. Create a registry and populate it with the experiments.
1. (Optional) Change the cluster function to either "Socket" or "Multicore" (the latter does not work on Windows).
1. Submit the jobs and once they are finished, collect the results.

### Analyzing the Results {.unnumbered .unlisted}

1. Conduct a global Friedman test and interpret the results.
   As an evaluation measure, use the mean-square error.
1. Inspect the ranks of the results.
