# Preprocessing {#sec-preprocessing}

{{< include _setup.qmd >}}

`r authors("Preprocessing")`

The last chapter (@sec-pipelines) gave an technical introduction to `r mlr3pipelines`, this chapter will show important concepts and typical problems when preprocessing data for machine learning.

In the realm of machine learning, there exist various definitions for preprocessing, data cleaning and feature engineering.
To clarify, preprocessing refers here to everything that happens with the data before it is used to fit the model, while postprocessing encompasses everything that occurs with predictions after the model is fitted.
Data cleaning is an important aspect of preprocessing as it involves the removal of errors, noise, and redundancy in the data.
There is usually little ambiguity in what needs to be done here, and it is generally straightforward.
Feature engineering, on the other hand, covers all other transformations of data before it is fed to the machine learning model.
It involves the creation of useful features from possibly unstructured data, such as written text, sequences or images.
We will focus less on data cleaning as this process does not necessitate `r mlr3pipelines` and is intertwined with exploratory data analysis (EDA).

Although deep learning has shown promising results in automating feature engineering, its effectiveness depends on the complexity and nature of the data being processed, as well as the specific problem being addressed.
Typically it is applicable to natural language processing (NLP) and computer vision (CV) problems, while standard tabular data is lacking in structure for deep learning models to extract meaningful features automatically.
Furthermore, different problems require different features to be extracted, and deep learning models may not always be able to identify the most relevant features for a given problem without human guidance.
The goal of feature engineering is to prepare the data so that a model can be trained on it, and/or to further improve predictive performance.
It is important to note that feature engineering helps mostly for simpler algorithms, while highly complex models usually gain little from it and require little data preparation to be trained.
Common difficulties in data include features with (high) skew distributions, high cardinality categorical features, missing observations, high dimensional dimensionality and imbalanced classes in classification tasks.

In this chapter we will use `r mlr3pipelines` on a data set introduced below that possess most of the mentioned difficulties.
We will not be able to cover every concept and we refer the reader to relevant further resources.
Since feature selection is such an important aspect of preprocessing it was already introduced in @sec-feature-selection.


## Ames Housing Data

The data we will be using is an adapted version of the Ames housing data, initially collected by [@de2011ames].
This data was collected as an *alternative to the Boston Housing data as an end of semester regression project* and is used in various other books on `r link("http://www.feat.engineering/intro-intro.html", "feature engineering")` and `r link("https://www.tmwr.org/ames.html","machine learning")` with R.
Raw and processed versions of the data can be directly loaded from the `AmesHousing` package.

The dataset encompasses data related to 2,930 residential properties situated in Ames, Iowa, sold between 2006 and 2010.
It contains 81 features on various aspects of the house (basement, garage, size, fence, pool, porch, bedrooms, etc.), size and shape of the lot as well as information about condition and quality.
The prediction target is the sale price in USD, hence it is a regression task (@sec-tasks).

::: callout-note
We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset.
The code to recreate this version of the data from the original raw data can be found `r link("https://github.com/ja-thomas/extend_ames_housing", "here")`
:::

We read in the data from the web and quickly peek at the target distribution

```{r preproc-001, echo = TRUE, eval = TRUE, message=FALSE}

library(data.table)
library(ggplot2)

url = "https://raw.githubusercontent.com/ja-thomas/extend_ames_housing/main/data/ames_dirty.csv"

ames = fread(url,
    stringsAsFactors = TRUE
)

ggplot(ames, aes(x = Sale_Price)) +
    geom_histogram()
```

As expected we see a skewed distribution of prices with a few outliers of very expensive properties.

## Data Cleaning and Exploratory Data Analysis

In a first step we explore the data and look for simple problems such as constant or duplicated features.
This can be done quite efficiently with a package like `DataExplorer` or `skimr`.

```{r preproc-0002, echo = TRUE, eval = TRUE, message=FALSE}
library(DataExplorer)
library(skimr)

skim(ames)
plot_intro(ames)
plot_bar(ames)
plot_density(ames)
plot_missing(ames)
plot_correlation(ames, type="continuous")


```

Here we just summarize the important findings:

```{r preproc-002, echo = TRUE, eval = TRUE, message=FALSE}

summary(ames$Misc_Feature_2)
```

`Misc_Feature_2` is a factor with only a single level `othr`.

```{r preproc-003, echo = TRUE, eval = TRUE, message=FALSE}

identical(ames$Condition_2, ames$Condition_3)
```

`Condition_2` and `Condition_3` are identical.

```{r preproc-004, echo = TRUE, eval = TRUE, message=FALSE}

cor(ames$Lot_Area, ames$Lot_Area_m2)
```

Both features represent the lot area, just on different scales.
For all three cases simple removal is sufficient.

```{r preproc-0045, echo = TRUE, eval = TRUE, message=FALSE}

to_remove = c("Lot_Area_m2", "Condition_3", "Misc_Feature_2")
```

Next to constant or identical features, further typical problems that should be checked for are

1) ID columns, i.e., columns that are unique for every observations should be removed or tagged.
2) `NA`s not correctly encoded, e.g. as `"NA"` or `""`
3) Semantic errors in the data, e.g., negative `Lot_Area`
4) Numeric features encoded as categorical for learners that can not handle such features.
5) ...

We will check most of these in the next sections.

Before we continue with feature engineering we create a task, choose a suitable performance measure and resampling strategy.

```{r preproc-006, echo = TRUE, eval = TRUE, message=FALSE}
library(mlr3verse)


ames_task = TaskRegr$new(
    backend = ames,
    target = "Sale_Price",
    id = "ames"
)

ames_task$select(setdiff(ames_task$feature_names, to_remove))

measure = msr("regr.mae")

cv10 = rsmp("cv")

cv10$instantiate(ames_task)

print(ames_task)
```

Lastly we want to compute a simple featureless baseline, i.e., always predicting the median sale price.

```{r preproc-007, echo = TRUE, eval = TRUE, message=FALSE}

baseline_lrn = lrn("regr.featureless", robust = TRUE)

baseline_res = resample(ames_task, baseline_lrn, cv10)
baseline_res$aggregate(measure)
```

We see that simply predicting the median houseprice results in a mean absolute error of a bit more than 56.000$.

## Factor Encoding

In machine learning, categorical features are variables that can take on a limited set of values, such as `Paved_Drive` with possible values of `Dirt_Gravel`, `Partial_Pavement`, `Paved` in the Ames housing data.

However, many machine learning algorithms such as support vector machines (SVM) require numerical inputs to function properly.

As shown in the Introduction (@sec-lrns-add-list), we can easily list learners that directly support categorical features.

```{r preproc-0075, echo = TRUE, eval = TRUE, message=FALSE}
as.data.table(mlr_learners)[task_type == "regr" &
    sapply(feature_types, function(x) "factor" %in% x)]
```

For other learners we need to convert categorical features into numerical ones through encoding.

Categorical features can be distinguished by their cardinality, which refers to the number of levels they contain.
There are three types of categorical features: binary, low-cardinality, and high-cardinality.

Generally, no universal threshold exist when a feature should be considered high-cardinality.

In the EDA section we see the number of levels for each categorical feature.
For the Ames housing data, we assume `Exterior_1st `, `Exterior_2nd`, `MS_SubClass` and `Neighborhood` to be high-cardinality by assuming a threshold of 10.
Generally, this threshold can be considered a hyperparameter and can be tuned jointly with all other hyperparameters.

Some learners which support handling categorical features out of the box, can still crash for high-cardinality features, since they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding.

Low-cardinality features can be handled by one-hot encoding.
One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature.
Theoretically it is sufficient to create one less binary feature than levels, as setting all binary features to zero is also a valid representation.
This is typically called dummy or treatment encoding and required if the learner is a generalized linear (GLM) or additive model (GAM) model.

For high-cardinality features, impact encoding is a reasonable approach.

Impact encoding is a type of encoding that converts categorical features into numeric values based on the impact of the feature on the target.
The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature.
Impact encoding involves the following steps:

1) Group the target variable by the categorical feature.
2) Compute the mean of the target variable for each group.
3) Compute the global mean of the target variable.
4) Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.
5) Replace the categorical feature with the impact scores.

By using impact encoding, we can preserve the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target.
The main advantage, compared to one-hot encoding is that only a single numeric feature is created regardless of the number of levels of the categorical features.
Hence, it is especially useful for high-cardinality features.
Since information from the target is now used to compute the impact scores, it is crucial that the encoding process is embedded in the cross-validation process to avoid label leakage.

The binary features `Alley`, `Central_Air`, `Street` can be encoded simply as 1 or 0, which does not change anything.

First we collapse all levels that occur less than 1% of the time and build a simple pipelines that encode categorical features based on their cardinality using `selector_cardinality_greater_than`.

```{r preproc-009, echo = TRUE, eval = TRUE, message=FALSE}

factor_pipeline =
    po("removeconstants") %>>%
    po("collapsefactors", no_collapse_above_prevalence = 0.01) %>>%
    po("encodeimpact", affect_columns = selector_cardinality_greater_than(10),
        id = "high_card_enc") %>>%
    po("encode", method = "one-hot", affect_columns = selector_cardinality_greater_than(2),
        id = "low_card_enc") %>>%
    po("encode", method = "treatment", affect_columns = selector_type("factor"),
        id = "binary_enc")

factor_pipeline$plot()
```

While many implementations of tree based algorithms, such as `rpart` or `ranger` are able to handle categorical features, `xgboost` can not and requires a preprocessing step.

```{r preproc-010, echo = TRUE, eval = TRUE, message=FALSE}

xgboost_impact = GraphLearner$new(
    factor_pipeline %>>%
        lrn("regr.xgboost", nrounds = 100),
    id = "regr.xgboost_impact"
)

xgboost_one_hot = GraphLearner$new(
    po("encode") %>>%
        lrn("regr.xgboost", nrounds = 100),
    id = "regr.xgboost_one_hot"
)


learners = list(
    baseline = baseline_lrn,
    tree = lrn("regr.rpart"),
    xgboost_impact = xgboost_impact,
    xgboost_one_hot = xgboost_one_hot
)

design = benchmark_grid(ames_task, learners = learners, cv10)
bmr = benchmark(design)
bmr$aggregate(measure = measure)[, .(learner_id, regr.mae)]
```

We see that gradient boosted tree with impact encoding results in the best model, even though the improvement over one-hot encoding is quite small.

For further readings and a benchmark study on different encoding strategies we refer to [@pargent2022regularized].


## Missing Values

In the EDA section we can see that most features contain no missing values, while 7 features contain a substantial amount of missing values.

While both `rpart` and `xgboost` from the previous section could handle missing values automatically, both `ranger` and `lm` that we would like to consider do not.

For simple imputation techniques, missing values can be replaced with the mean, median, mode, or a sample from the empirical distribution of the feature.
For categorical features, missing values can easily be replaced by a new separate level, e.g. called `.MISSING`.
The original information if an observation contained a missing value for each feature might be meaningful for the model.
So we need keep track of the imputation by adding binary indicator features that are `1` if the feature was missing for an observation and `0` if it was present.

[@ding2010investigation] show that for binary classification and tree-based models a *"separate class is clearly
the best method to use when the testing set has missing values and the missingness is related to
the response variable"*.
For numeric features this means that encoding missing values out-of-range, e.g. as two times the largest observed value is a reasonable approach.

```{r preproc-011, echo = TRUE, eval = TRUE, message=FALSE}

impute_histogram = list(
    po("missind",
        type = "integer",
        affect_columns = selector_type("integer")
    ),
    po("imputehist",
        affect_columns = selector_type("integer")
    )
) %>>%
    po("featureunion") %>>%
    po("imputeoor",
        affect_columns = selector_type("factor")
    )

impute_histogram$plot()
```

```{r preproc-012, echo = TRUE, eval = TRUE, message=FALSE}

ranger_impute_histogram = GraphLearner$new(
    impute_histogram %>>%
        lrn("regr.ranger"),
    id = "regr.ranger_imp_histogram"
)

ranger_impute_oor = GraphLearner$new(
    po("imputeoor") %>>%
        lrn("regr.ranger"),
    id = "regr.ranger_imp_oor"
)

design = benchmark_grid(ames_task,
    learners = list(
        ranger_impute_histogram,
        ranger_impute_oor
    ),
    cv10
)
bmr_new = benchmark(design)
bmr$combine(bmr_new)
bmr$aggregate(measure = measure)[, .(learner_id, regr.mae)]
```

We see that the out-of-range imputation worked slighly better for the random forest, but the difference is again very small.
Overall our gradient tree boosting with impact encoding is still the best model, so far.

Many more advanced imputation strategies exist.
For example, in model based imputation, additional machine learning models are trained to predict missing values from other features.
Multiple imputation resamples the data and imputes each value multiple time to attain more robust estimates.
These more advanced techniques very rarely improve the model substantially and the simple imputation techniques introduced before are usually sufficient.

## Pipeline Robustify

`r mlr3pipelines` offers a simple and reusable pipeline for (among other things) imputation and factor encoding called `pipeline_robustify`.

```{r preproc-0135, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}

robustify = mlr3pipelines::pipeline_robustify()
robustify$plot()
```

*Robustify* does the following steps:

1) Constant features are removed as they do not contain any information
2) Character features are cast to categorical features as many learners require factors
3) Date/time features are encoded as numeric features as most learners can not handle these types of features
4) Ordinal features are encoded as categorical features as most learners do not support ordinal features beyond assuming they are categoricals
5) Numeric features are imputed by sampling from the empirical distribution and missingness indicators are added as a reasonable imputatation strategy discussed above
6) Missing values of categorical features are encoded with a new level
7) Factor levels of categorical features are fixed such that during prediction they are the same as during training; possibly dropping empty training factor levels before
8) Since the previous step can introduce new missing values at prediction time due to non existing factor levels they are imputed with random factor levels
9) Categorical features levels are collapsed (starting from the rarest factors in the training data) until there are no more than 1000 levels as to handle high cardinality categorical features without impact encoding
10) Categorical features are encoded by one-hot encoding to retain all information
11) Constant features that might have been created in the previous steps are again removed

```{r preproc-013, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}
lm_preproc = GraphLearner$new(
    robustify %>>%
        lrn("regr.lm"),
    id = "regr.lm_preproc"
)

design = benchmark_grid(ames_task,
    learners = lm_preproc,
    cv10
)
bmr_new = benchmark(design)
bmr$combine(bmr_new)
bmr$aggregate(measure = measure)[, .(learner_id, regr.mae)]
```

We see that while gradient boosting is still the best model, a simple linear model with the robustify preprocessor is not much worse.

## Scaling Features and Targets

Simple transformations of features and the target can be beneficial for certain learners.

Log transformation can help in making the distribution of the target more symmetrical.
This is typically useful when the data is skewed, as some machine learning algorithms assume a normal distribution of the target.
Thus, log transformation can help reduce the impact of outliers and improve the accuracy of the model.
Similarly, log transformation of skew features can help to reduce the influence of outliers and very large models.

For distance based methods such as K-nearest neighbor models or regularized parametric models such as Lasso or Elastic net, normalization is required as otherwise features with larger scale would have an higher impact.
Luckily, most based models internally scale the data if required by the algorithm so most of the time we do not need to manually do this in preprocessing.

```{r preproc-014, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}

log_lm_preproc = ppl("targettrafo", graph = lm_preproc)
log_lm_preproc$param_set$values$targetmutate.trafo = function(x) log(x)
log_lm_preproc$param_set$values$targetmutate.inverter = function(x) list(response = exp(x$response))

log_lm_preproc$plot()

log_lm_preproc = GraphLearner$new(
    log_lm_preproc,
    id = "regr.log_lm_preproc"
)

design = benchmark_grid(ames_task,
    learners = log_lm_preproc,
    cv10
)
bmr_new = benchmark(design)
bmr$combine(bmr_new)
bmr$aggregate(measure = measure)[, .(learner_id, regr.mae)]
```

We see that with the target transformation a simple linear regression is the best model we have seen so far.
While it is quite possible that proper hyperparameter tuning of the gradient boosting algorithm results in a better predictive performance, this requires much more computational resources.

## Feature Extraction

Unfortunately, we do not have any information about the quality or number of kitchen appliances, which can have a significant impact on the sale price.
However, we do have some data on the power consumption in the kitchen over an average day in 2-minute intervals.

```{r preproc-015, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}

energy_data = fread("https://raw.githubusercontent.com/ja-thomas/extend_ames_housing/main/data/energy_usage.csv",
    stringsAsFactors = FALSE
)

plot(as.numeric(energy_data[1, ]), type = "l")
```

Although we should not directly add these 720 features to our data, we can use some information about the curves to gain insights into the kitchen's overall energy usage.
For example, we can look at the maximum used wattage, overall used wattage, number of peaks, and other similar features.
This is what is typically called feature extraction.

To extract some features we write our own simple `PipeOp` that inherits from `PipeOpTaskPreprocSimple` as shown in Chapter (@sec-pipelines).
The operator is quite simple, as we hardcode all operations.
First we extract the functional features save them in a new `data.table` called `ffeats`, apply our extractors `mean`, `min`, `max` and `variance` which are appended to the data.
Finally the original functional features are removed as we do not need them anymore.


```{r preproc-0155, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}
library(R6)

PipeOpFuncExtract = R6Class("PipeOpFuncExtract",

  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,

  private = list(
    .transform_dt = function(dt, levels) {
        ffeat_names = paste0("att", 1:720)
        ffeats = dt[, ..ffeat_names]
        dt[, energy_means := apply(ffeats, 1, mean)]
        dt[, energy_mins := apply(ffeats, 1, min)]
        dt[, energy_maxs := apply(ffeats, 1, max)]
        dt[, energy_vars := apply(ffeats, 1, var)]
        dt[, (ffeat_names) := NULL]
        return(dt)
    }
  )
)


```

Finally we rerun our benchmark with all candidate models and see how much each learner improves with the feature extraction.

```{r preproc-016, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}

ames_task_ext = cbind(ames, energy_data)

ames_task_ext = TaskRegr$new(
    backend = ames_task_ext,
    target = "Sale_Price",
    id = "ames_extended"
)

ames_task$select(setdiff(ames_task$feature_names, to_remove))

func_extractor = PipeOpFuncExtract$new("energy_extract")

ames_task_ext = func_extractor$train(list(ames_task_ext))[[1]]

learners = list(
    baseline = baseline_lrn,
    tree = lrn("regr.rpart"),
    xgboost_impact = xgboost_impact,
    ranger_impute_oor = ranger_impute_oor,
    lm_preproc = lm_preproc,
    log_lm_preproc = log_lm_preproc
)

design = benchmark_grid(list(ames_task_ext, ames_task),
    learners = learners,
    cv10
)

bmr_final = benchmark(design)

```


```{r preproc-017, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}

bmr_final$aggregate(measure = measure)


```

We observe that feature extraction helped all models, except the featureless baseline for obvious reasons.
