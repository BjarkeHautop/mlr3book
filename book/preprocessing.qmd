---
author:
  - name: Author 1
    orcid:
    email:
    affiliations:
      - name: Affiliation 1
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: TODO (150-200 WORDS)
---

# Preprocessing {#sec-preprocessing}

{{< include _setup.qmd >}}

The last chapter (@sec-pipelines) gave an technical introduction to `r mlr3pipelines`, this chapter will show important concepts and typical problems when working with data for machine learning.

In the realm of machine learning, there exist various definitions for preprocessing, data cleaning and feature engineering.
To clarify, preprocessing refers here to everything that happens with the data before it is used to fit the model, while postprocessing encompasses everything that occurs with predictions after the model is fitted.
Data cleaning is an important aspect of preprocessing as it involves the removal of errors, noise, and redundancy in the data.
There is little ambiguity in what needs to be done here, and it is generally straightforward.
Feature engineering, on the other hand, covers all other transformations of data before it is fed to the machine learning model.
It involves the creation of useful features from possibly unstructured data, such as written text, sequences or images.

It's often said that deep learning automates feature engineering, but this mainly refers to feature extraction from written text and images.

The goal of feature engineering is to prepare the data so that a model can be trained on it, and/or to further improve predictive performance.

It's important to note that feature engineering helps mostly for simpler algorithms, while highly complex models gain little from it and require little data preparation to be trained.
Common difficulties in data include features with (high) skew distributions, high cardinality categorical features, missing observations, high dimensional dimensionality and imbalanced classes in classification tasks.

In this chapter we will use `r mlr3pipelines` on a data set introduced below that possess most of the mentioned difficulties.
We won't be able to cover every concept and we refer the reader to relevant further resources.
Since feature selection is such an important aspect of preprocessing it was already introduced in @sec-feature-selection.


## Ames Housing Data

The data we will be using is an adapted version of the Ames housing data, initially collected by [@de2011ames].

This data was collected as an *alternative to the Boston Housing data as an end of semester regression project* and is used in various other books on `r link("http://www.feat.engineering/intro-intro.html", "feature engineering")` and `r link("https://www.tmwr.org/ames.html","machine learning")` with R.
Raw and processed versions of the data can be directly loaded from the `AmesHousing` package.

The dataset encompasses data related to 2,930 residential properties situated in Ames, Iowa, sold between 2006 and 2010.
It contains 81 features on various aspects of the house (basement, garage, size, fence, pool, porch, bedrooms, etc.), size and shape of the lot as well as information about condition and quality.

The prediction target is the sale price in USD, hence it is a regression task (@sec-tasks).

::: callout-note
We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset.
The code to recreate this version of the data from the original raw data can be found `r link("https://github.com/ja-thomas/extend_ames_housing", "here")`
:::

We read in the data from the web and quickly peek at the target distribution

```{r preproc-001, echo = TRUE, eval = TRUE, message=FALSE}

library(data.table)
library(ggplot2)

ames = fread("https://raw.githubusercontent.com/ja-thomas/extend_ames_housing/main/data/ames_dirty.csv", stringsAsFactors = TRUE)

ggplot(ames, aes(x = Sale_Price)) +
    geom_histogram()
```

## Data Cleaning

In a first step we explore the data and look for simple problems such as constant or duplicated features.
This can be done quite efficiently with a package like `DataExplorer`.
The full report can be found in Appendix XXX.

Here we just summarize the important findings:

```{r preproc-002, echo = TRUE, eval = TRUE, message=FALSE}

summary(ames$Misc_Feature_2)
```

`Misc_Feature_2` is a factor with only a single level `othr`.

```{r preproc-003, echo = TRUE, eval = TRUE, message=FALSE}

identical(ames$Condition_2, ames$Condition_3)
```

`Condition_2` and `Condition_3` are identical.

```{r preproc-004, echo = TRUE, eval = TRUE, message=FALSE}

cor(ames$Lot_Area, ames$Lot_Area_m2)
```

Both features represent the lot area, just on different scales.

For all three cases simple removal is sufficient.

```{r preproc-005, echo = TRUE, eval = TRUE, message=FALSE}

ames = ames[, `:=`(Misc_Feature_2 = NULL, Condition_3 = NULL, Lot_Area_m2 = NULL)]
```

::: callout-note
This could also easily be done after creating a `RegressionTask`, either by `task$select()` or `po("select")`
:::

Next to constant or identical features, further typical problems that should be checked for are

1) ID columns, i.e., columns that are unique for every observations should be removed or tagged.
2) `NA`s not correctly encoded, e.g. as `"NA"` or `""`
3) Semantic errors in the data, e.g., negative `Lot_Area`
4) Numeric features encoded as categorical
5) ...

Before we continue with feature engineering we create a task, choose a suitable performance measure and resampling strategy.

```{r preproc-006, echo = TRUE, eval = TRUE, message=FALSE}
library(mlr3verse)
ames_task = TaskRegr$new(backend = ames, target = "Sale_Price", id = "ames")

measure = msr("regr.mae")

cv10 = rsmp("cv")

cv10$instantiate(ames_task)

print(ames_task)
```

Lastly we want to compute a simple featureless baseline, i.e., always predicting the median sale price.

```{r preproc-007, echo = TRUE, eval = TRUE, message=FALSE}

baseline_lrn = lrn("regr.featureless", robust = TRUE)

baseline_res = resample(ames_task, baseline_lrn, cv10)
baseline_res$aggregate(measure)
```

## Factor Encoding

When working with categorical features in machine learning, it is important to properly encode them as numeric features.
Categorical features can be distinguished by their cardinality, which refers to the number of levels they contain.
There are three types of categorical features: binary, low-cardinality, and high-cardinality.

Let us first have a look at the number of levels of each categorical feature.

```{r preproc-008, echo = TRUE, eval = TRUE, message=FALSE}

n_lvls = sapply(ames[, .SD, .SDcols = sapply(ames, is.factor)], function(x) length(levels(x)))
sort(n_lvls)
```

No universal threshold exist when a feature should be considered high-cardinality.

For the Ames housing data, we assume `Exterior_1st `, `Exterior_2nd`, `MS_SubClass` an `Neighborhood` to be high-cardinality.

Some learner support handling categorical features out of the box, but can still crash for high-cardinality features.

The binary features `Alley`, `Central_Air`, `Street` can be encoded simply as 1 or 0, which does not change anything.

Low-cardinality features, on the other hand, should be one-hot encoded.
This means that for each level of the feature, a new binary feature is created.

This can quickly result in a large number of new features, which can be problematic for training a model.

For high-cardinality features, (regularized) impact encoding is a reasonable approach.

```{r preproc-009, echo = TRUE, eval = TRUE, message=FALSE}

low_card_feats = names(n_lvls > 2 & n_lvls <= 10)
high_card_feats = names(n_lvls > 10)

factor_pipeline =
    po("collapsefactors", no_collapse_above_prevalence = 0.01) %>>%
    po("encode", method = "one-hot", affect_columns = selector_name(low_card_feats), id = "low_card_enc") %>>%
    po("encodeimpact", affect_columns = selector_name(high_card_feats), id = "high_card_enc") %>>%
    po("encode", method = "treatment", affect_columns = selector_type("factor"), id = "binary_enc")
```

While many implementations of tree based algorithms, such as `rpart` or `ranger` are able to handle categorical features, `xgboost` cannot and requires a preprocessing step.

```{r preproc-010, echo = TRUE, eval = TRUE, message=FALSE}

xgboost_preproc = GraphLearner$new(factor_pipeline %>>% lrn("regr.xgboost", nrounds = 100), id = "regr.xgboost")

learners = list(
    baseline = baseline_lrn,
    tree = lrn("regr.rpart"),
    xgboost = xgboost_preproc
)

design = benchmark_grid(ames_task, learners = learners, cv10)
bmr = benchmark(design)
plot(bmr, measure = measure)
```


## Missing Values

Referring again to our `DataExplorer` report in Appendix XXX, we can see that most features contain no missing values, while 7 features contain a substantial amount of missings.

While both `rpart` and `xgboost` from the previous section could handle missing values automatically, both `ranger` and `lm` that we would like to consider do not.

For simple imputation techniques, missing values can be replaced with the mean, median, mode, or a sample from the empirical distribution of the feature.
For categorical features, missing values can easily be replaced by a new separate level, e.g. called `.MISSING`.
To keep track of the imputation, binary indicator features are added.

```{r preproc-011, echo = TRUE, eval = TRUE, message=FALSE}

impute_pipeline = list(
    po("missind", type = "integer", affect_columns = selector_type("integer")),
    po("imputehist", affect_columns = selector_type("integer"))
) %>>%
    po("featureunion") %>>%
    po("imputeoor", affect_columns = selector_type("factor"))
```

```{r preproc-012, echo = TRUE, eval = TRUE, message=FALSE}

learners$ranger_preproc = GraphLearner$new(impute_pipeline %>>% lrn("regr.ranger"), id = "regr.ranger")
# learners$lm_preproc = GraphLearner$new(po("fixfactors") %>>% impute_pipeline %>>% factor_pipeline %>>% lrn("regr.lm"), id = "regr.lm")

design = benchmark_grid(ames_task, learners = learners, cv10)
bmr = benchmark(design)
plot(bmr, measure = measure)
```


## Scaling Features and Targets

* Log-scaling of the target for some models
* Log-scaling of features for some models
* Tree-based methods do only consider the order of features
* Normalization for distance based methods
* Polynomials, interactions and basis expansions are mainly interesting for LMs and not really considered here

## Feature Extraction

* Unfortunately we don't have information about the quality and number of kitchen appliances, which can have an effect on the sales price.
* But we have information on power consumption in the kitchen over an average day in 2-Minute intervals
* We cannot directly add these features to our data
* Some information about the curves should give information about the kitchen:
    * Max-used wattage
    * Overall used wattage
    * Number of peaks
    * ...
* Which features to use? Extract a large number of features and use feature selection methods, or include extraction strategies in pipeline definition.
* Some features are easily interpretable and domain knowledge can help to define meaningful extractions.
* More complex features, e.g. wavelets, allow to capture more complex structures, but are not interpretable anymore.

## Multiple Data Sources

* We paint a somewhat unrealistic picture of how ML works in practice
* It is rarely the case that a single data source, such as the ames housing data is present
* Much more often there are many different data sources: Tables, data bases, spreadsheets, ... that need to be consolidated and understood before we can even start to do the above discussed preprocessing steps
* We illustrate that in a very simple example, where information about renovations of the properties is not present in the data, but in a seperate table in a `many-to-one`relation.
* First we need to aggregate information from that table and append it to our data.
