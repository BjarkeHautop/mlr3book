---
author:
  - name: Author 1
    orcid:
    email:
    affiliations:
      - name: Affiliation 1
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: TODO (150-200 WORDS)
---

# Preprocessing {#sec-preprocessing}

{{< include _setup.qmd >}}

## Concepts and Terminology

The last chapter gave an technical introduction to `mlr3pipelines`, this chapter will show important concepts and typical problems when working with data for machine learning.
In the realm of machine learning, there are different definitions for preprocessing, data cleaning and feature engineering.
To clarify, preprocessing refers to everything that happens with the data before it is used to fit the model, while postprocessing encompasses everything that occurs with predictions after the model is fitted.
Data cleaning is a crucial aspect of preprocessing as it involves the removal of errors, noise, and redundancy in the data.
There is little ambiguity in what needs to be done here, and it is generally straightforward.
Feature engineering, on the other hand, covers all other transformations of data before it is fed to the machine learning model.
It involves the creation of useful features from possibly unstructured data, such as written text, sequences or images.

It's often said that deep learning automates feature engineering, but this mainly refers to feature extraction from written text and images.

The goal of feature engineering is to prepare the data so that a model can be trained on it, and/or to further improve predictive performance.

It's important to note that feature engineering helps mostly for simpler algorithms, while highly complex models gain little from it and require little data preparation to be trained.
Common difficulties in data include features with (high) skew distributions, high cardinality categorical features, missing observations, high dimensional dimensionality and imbalanced classes in classification tasks.

In this chapter we will use `mlr3pipelines` on a dataset introduced below that posess most of the mentioned difficulties.
We won't be able to cover every concept and we refer to reader to relevant further resources.
Since feature selection is such an important aspect of preprocessing it was already introduced in Chapter 5.


## Ames Housing Data

The data we will be using is an adapted version of the Ames housing data, initially collected by CITE.

This data is used in various other books on feature engineering and machine learning in R and is nicely prepared in the `AmesHousing` package.

We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset.

* We will use an adapted version of the Ames housing data, a more complex alternative to the Bostong Housing data
* The goal is to predict sales prices of residential properties in Ames, a small city in Iowa
* The original data was collected by Dean De Cock in 2011
* It contains information of 2930 residential properties sold between 2006 in 2010
* Each property is described by 79 features
* In the following we will have a first look at the dataset and do some exploratory data analysis


## Data Cleaning

* ID columns need to be tagged, removed or at least ignored for training
* Duplicated features need to be removed
    * Features that are exactly the same
    * Features that are a (linear) transformation of another feature.
* Constant features
* Errors in data, e.g. due to manual entry, e.g.
    * Year remodeled is before year built
    * Pool area > 0 even if the property does not have a pool
    * `NA`s not correctly encoded, e.g. as `"NA"` or `""`
* Numeric features encoded as categorical features

## Factor Encoding

* For most learners categorical features need to be encoded in numeric features.
* Distinguish between binary, low cardinality and high cardinality categorical features.
* Low or high cardinality refers to the number of levels.
    * Binary: Encode as 1 / 0. Does not change anything except intepretation of coefficients in LM/GAM
    * Low-cardinality: One-hot / dummy encoding.
    * High-cardinality: Regularized target/impact encoding, clustering, hashing.
* Check the learner properties if they can handle categorical features.
* Be careful, a learner, e.g. LM, might be able to handle low-cardinality features, but might crash when high cardinality features are present.
* Most Tree-based algorithms can natively handle even high-cardinality categorical features.
* Optimal encoding can vary between each feature, algorithm and hyperparameter configuration.
* Introduce and tune threshold hyperparameter that decides when to use high-cardinality encoding
* The encoder should also be able to handle new feature levels occuring at test time without crashing.

## Imputation

* Imputation is the process of replacing missing values with artificial substituted values.
* Visualize `NA`s with a missmap
* Missingness can encode important information.
    * Missing completely at random
    * Missing at random: Missingness is related to some other features. EXAMPLE
    * Missing not at random: Missingness is related to the feature itself. EXAMPLE
* Simple imputation techniques replace missings with the mean, median, mode or a sample from empirical distribution of the feature.
* For categorical features, missing values can easily be replaced by a new seperate level.
* To keep track of the imputation, binary indicator features are added.
* Check the learner properties if they can handle missing values
* Some tree-based algorithms can natively handle missing values.
* Model-based imputation trains a machine learning model to predict missing values using the remaining features.
    * The imputation model should be able to handle missings natively.
    * The choice of learner and its hyperparameters add additional complexity to the imputation.
    * Random Forests are a reasonable choice.

## Scaling Features and Targets

* Log-scaling of the target for some models
* Log-scaling of features for some models
* Tree-based methods do only consider the order of features
* Normalization for distance based methods
* Polynomials, interections and basis expansions are mainly interesting for LMs and not really considered here

## Feature Extraction

* Unfortunately we don't have information about the quality and number of kitchen appliances, which can have an effect on the sales price.
* But we have information on power consumptions in the kitchen over an average day in 2-Minute intervals
* We cannot directly add these features to our data
* Some information about the curves should give information about the kitchen:
    * Max-used wattage
    * Overall used wattage
    * Number of peaks
    * ...
* Which features to use? Extract a large number of features and use feature selection methods, or include extraction strategies in pipeline definition.
* Some features are easily interpretable and domain knowledge can help to define meaningful extractions.
* More complex features, e.g. wavelets, allow to capture more complex structures, but are not interpretable anymore.

## Multiple Data Sources

* We paint a somewhat unrealistic picture of how ML works in practice
* It is rarely the case that a single data source, such as the ames housing data is present
* Much more often there are many different data sources: Tables, data bases, spreadsheets, ... that need to be consolidated and understood before we can even start to do the above discussed preprocessing steps
* We illustrate that in a very simple example, where information about renovations of the properties is not present in the data, but in a seperate table in a `many-to-one`relation.
* First we need to aggregate information from that table and append it to our data.
