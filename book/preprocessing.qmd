---
author:
  - name: Author 1
    orcid:
    email:
    affiliations:
      - name: Affiliation 1
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: TODO (150-200 WORDS)
---

# Preprocessing {#sec-preprocessing}

{{< include _setup.qmd >}}

The last chapter (@sec-pipelines) gave an technical introduction to `r mlr3pipelines`, this chapter will show important concepts and typical problems when working with data for machine learning.

In the realm of machine learning, there exist various definitions for preprocessing, data cleaning and feature engineering.
To clarify, preprocessing refers here to everything that happens with the data before it is used to fit the model, while postprocessing encompasses everything that occurs with predictions after the model is fitted.
Data cleaning is an important aspect of preprocessing as it involves the removal of errors, noise, and redundancy in the data.
There is little ambiguity in what needs to be done here, and it is generally straightforward.
Feature engineering, on the other hand, covers all other transformations of data before it is fed to the machine learning model.
It involves the creation of useful features from possibly unstructured data, such as written text, sequences or images.

It's often said that deep learning automates feature engineering, but this mainly refers to feature extraction from written text and images.

The goal of feature engineering is to prepare the data so that a model can be trained on it, and/or to further improve predictive performance.

It's important to note that feature engineering helps mostly for simpler algorithms, while highly complex models gain little from it and require little data preparation to be trained.
Common difficulties in data include features with (high) skew distributions, high cardinality categorical features, missing observations, high dimensional dimensionality and imbalanced classes in classification tasks.

In this chapter we will use `r mlr3pipelines` on a data set introduced below that possess most of the mentioned difficulties.
We won't be able to cover every concept and we refer the reader to relevant further resources.
Since feature selection is such an important aspect of preprocessing it was already introduced in @sec-feature-selection.


## Ames Housing Data

The data we will be using is an adapted version of the Ames housing data, initially collected by [@de2011ames].

This data was collected as an *alternative to the Boston Housing data as an end of semester regression project* and is used in various other books on `r link("http://www.feat.engineering/intro-intro.html", "feature engineering")` and `r link("https://www.tmwr.org/ames.html","machine learning")` with R.
Raw and processed versions of the data can be directly loaded from the `AmesHousing` package.

The dataset encompasses data related to 2,930 residential properties situated in Ames, Iowa, sold between 2006 and 2010.
It contains 81 features on various aspects of the house (basement, garage, size, fence, pool, porch, bedrooms, etc.), size and shape of the lot as well as information about condition and quality.

The prediction target is the sale price in USD, hence it is a regression task (@sec-tasks).

::: callout-note
We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset.
The code to recreate this version of the data from the original raw data can be found `r link("https://github.com/ja-thomas/extend_ames_housing", "here")`
:::

We read in the data from the web and quickly peek at the target distribution

```{r preproc-001, echo = TRUE, eval = TRUE, message=FALSE}

library(data.table)
library(ggplot2)

ames = fread("https://raw.githubusercontent.com/ja-thomas/extend_ames_housing/main/data/ames_dirty.csv", stringsAsFactors = TRUE)

ggplot(ames, aes(x = Sale_Price)) +
    geom_histogram()
```

## Data Cleaning

In a first step we explore the data and look for simple problems such as constant or duplicated features.
This can be done quite efficiently with a package like `DataExplorer`.
The full report can be found in Appendix XXX.

Here we just summarize the important findings:

```{r preproc-002, echo = TRUE, eval = TRUE, message=FALSE}

summary(ames$Misc_Feature_2)
```

`Misc_Feature_2` is a factor with only a single level `othr`.

```{r preproc-003, echo = TRUE, eval = TRUE, message=FALSE}

identical(ames$Condition_2, ames$Condition_3)
```

`Condition_2` and `Condition_3` are identical.

```{r preproc-004, echo = TRUE, eval = TRUE, message=FALSE}

cor(ames$Lot_Area, ames$Lot_Area_m2)
```

Both features represent the lot area, just on different scales.

For all three cases simple removal is sufficient.

```{r preproc-005, echo = TRUE, eval = TRUE, message=FALSE}

ames[, `:=`(Misc_Feature_2 = NULL, Condition_3 = NULL, Lot_Area_m2 = NULL)]
```

::: callout-note
This could also easily be done after creating a `RegressionTask`, either by `task$select()` or `po("select")`
:::

The data contains missing values, but since the

* ID columns need to be tagged, removed or at least ignored for training
* Duplicated features need to be removed
    * Features that are exactly the same
    * Features that are a (linear) transformation of another feature.
* Constant features
* Errors in data, e.g. due to manual entry, e.g.
    * Year remodeled is before year built
    * Pool area > 0 even if the property does not have a pool
    * `NA`s not correctly encoded, e.g. as `"NA"` or `""`
* Numeric features encoded as categorical features

## Factor Encoding

* For most learners categorical features need to be encoded in numeric features.
* Distinguish between binary, low cardinality and high cardinality categorical features.
* Low or high cardinality refers to the number of levels.
    * Binary: Encode as 1 / 0. Does not change anything except intepretation of coefficients in LM/GAM
    * Low-cardinality: One-hot / dummy encoding.
    * High-cardinality: Regularized target/impact encoding, clustering, hashing.
* Check the learner properties if they can handle categorical features.
* Be careful, a learner, e.g. LM, might be able to handle low-cardinality features, but might crash when high cardinality features are present.
* Most Tree-based algorithms can natively handle even high-cardinality categorical features.
* Optimal encoding can vary between each feature, algorithm and hyperparameter configuration.
* Introduce and tune threshold hyperparameter that decides when to use high-cardinality encoding
* The encoder should also be able to handle new feature levels occuring at test time without crashing.

## Imputation

* Imputation is the process of replacing missing values with artificial substituted values.
* Visualize `NA`s with a missmap
* Missingness can encode important information.
    * Missing completely at random
    * Missing at random: Missingness is related to some other features. EXAMPLE
    * Missing not at random: Missingness is related to the feature itself. EXAMPLE
* Simple imputation techniques replace missings with the mean, median, mode or a sample from empirical distribution of the feature.
* For categorical features, missing values can easily be replaced by a new seperate level.
* To keep track of the imputation, binary indicator features are added.
* Check the learner properties if they can handle missing values
* Some tree-based algorithms can natively handle missing values.
* Model-based imputation trains a machine learning model to predict missing values using the remaining features.
    * The imputation model should be able to handle missings natively.
    * The choice of learner and its hyperparameters add additional complexity to the imputation.
    * Random Forests are a reasonable choice.

## Scaling Features and Targets

* Log-scaling of the target for some models
* Log-scaling of features for some models
* Tree-based methods do only consider the order of features
* Normalization for distance based methods
* Polynomials, interections and basis expansions are mainly interesting for LMs and not really considered here

## Feature Extraction

* Unfortunately we don't have information about the quality and number of kitchen appliances, which can have an effect on the sales price.
* But we have information on power consumptions in the kitchen over an average day in 2-Minute intervals
* We cannot directly add these features to our data
* Some information about the curves should give information about the kitchen:
    * Max-used wattage
    * Overall used wattage
    * Number of peaks
    * ...
* Which features to use? Extract a large number of features and use feature selection methods, or include extraction strategies in pipeline definition.
* Some features are easily interpretable and domain knowledge can help to define meaningful extractions.
* More complex features, e.g. wavelets, allow to capture more complex structures, but are not interpretable anymore.

## Multiple Data Sources

* We paint a somewhat unrealistic picture of how ML works in practice
* It is rarely the case that a single data source, such as the ames housing data is present
* Much more often there are many different data sources: Tables, data bases, spreadsheets, ... that need to be consolidated and understood before we can even start to do the above discussed preprocessing steps
* We illustrate that in a very simple example, where information about renovations of the properties is not present in the data, but in a seperate table in a `many-to-one`relation.
* First we need to aggregate information from that table and append it to our data.
