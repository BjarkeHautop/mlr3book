---
author:
  - name: Author 1
    orcid:
    email:
    affiliations:
      - name: Affiliation 1
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: TODO (150-200 WORDS)
---

# Preprocessing {#sec-preprocessing}

{{< include _setup.qmd >}}

The last chapter (@sec-pipelines) gave an technical introduction to `r mlr3pipelines`, this chapter will show important concepts and typical problems when working with data for machine learning.

In the realm of machine learning, there exist various definitions for preprocessing, data cleaning and feature engineering.
To clarify, preprocessing refers here to everything that happens with the data before it is used to fit the model, while postprocessing encompasses everything that occurs with predictions after the model is fitted.
Data cleaning is an important aspect of preprocessing as it involves the removal of errors, noise, and redundancy in the data.
There is little ambiguity in what needs to be done here, and it is generally straightforward.
Feature engineering, on the other hand, covers all other transformations of data before it is fed to the machine learning model.
It involves the creation of useful features from possibly unstructured data, such as written text, sequences or images.

It's often said that deep learning automates feature engineering, but this mainly refers to feature extraction from written text and images.

The goal of feature engineering is to prepare the data so that a model can be trained on it, and/or to further improve predictive performance.

It's important to note that feature engineering helps mostly for simpler algorithms, while highly complex models gain little from it and require little data preparation to be trained.
Common difficulties in data include features with (high) skew distributions, high cardinality categorical features, missing observations, high dimensional dimensionality and imbalanced classes in classification tasks.

In this chapter we will use `r mlr3pipelines` on a data set introduced below that possess most of the mentioned difficulties.
We won't be able to cover every concept and we refer the reader to relevant further resources.
Since feature selection is such an important aspect of preprocessing it was already introduced in @sec-feature-selection.


## Ames Housing Data

The data we will be using is an adapted version of the Ames housing data, initially collected by [@de2011ames].

This data was collected as an *alternative to the Boston Housing data as an end of semester regression project* and is used in various other books on `r link("http://www.feat.engineering/intro-intro.html", "feature engineering")` and `r link("https://www.tmwr.org/ames.html","machine learning")` with R.
Raw and processed versions of the data can be directly loaded from the `AmesHousing` package.

The dataset encompasses data related to 2,930 residential properties situated in Ames, Iowa, sold between 2006 and 2010.
It contains 81 features on various aspects of the house (basement, garage, size, fence, pool, porch, bedrooms, etc.), size and shape of the lot as well as information about condition and quality.

The prediction target is the sale price in USD, hence it is a regression task (@sec-tasks).

::: callout-note
We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset.
The code to recreate this version of the data from the original raw data can be found `r link("https://github.com/ja-thomas/extend_ames_housing", "here")`
:::

We read in the data from the web and quickly peek at the target distribution

```{r preproc-001, echo = TRUE, eval = TRUE, message=FALSE}

library(data.table)
library(ggplot2)

ames <- fread("https://raw.githubusercontent.com/ja-thomas/extend_ames_housing/main/data/ames_dirty.csv", stringsAsFactors = TRUE)

ggplot(ames, aes(x = Sale_Price)) +
    geom_histogram()
```

## Data Cleaning

In a first step we explore the data and look for simple problems such as constant or duplicated features.
This can be done quite efficiently with a package like `DataExplorer`.
The full report can be found in Appendix XXX.

Here we just summarize the important findings:

```{r preproc-002, echo = TRUE, eval = TRUE, message=FALSE}

summary(ames$Misc_Feature_2)
```

`Misc_Feature_2` is a factor with only a single level `othr`.

```{r preproc-003, echo = TRUE, eval = TRUE, message=FALSE}

identical(ames$Condition_2, ames$Condition_3)
```

`Condition_2` and `Condition_3` are identical.

```{r preproc-004, echo = TRUE, eval = TRUE, message=FALSE}

cor(ames$Lot_Area, ames$Lot_Area_m2)
```

Both features represent the lot area, just on different scales.

For all three cases simple removal is sufficient.

```{r preproc-005, echo = TRUE, eval = TRUE, message=FALSE}

ames[, `:=`(Misc_Feature_2 = NULL, Condition_3 = NULL, Lot_Area_m2 = NULL)]
```

::: callout-note
This could also easily be done after creating a `RegressionTask`, either by `task$select()` or `po("select")`
:::

Next to constant or identical features, further typical problems that should be checked for are

1) ID columns, i.e., columns that are unique for every observations should be removed or tagged.
2) `NA`s not correctly encoded, e.g. as `"NA"` or `""`
3) Semantic errors in the data, e.g., negative `Lot_Area`
4) Numeric features encoded as categorical
5) ...

Before we continue with feature engineering we create a task, choose a suitable performance measure and resampling strategy.

```{r preproc-006, echo = TRUE, eval = TRUE, message=FALSE}
library(mlr3verse)

ames_task <- TaskRegr$new(backend = ames, target = "Sale_Price", id = "ames")

measure <- msr("regr.mae")

cv10 <- rsmp("cv")

cv10$instantiate(ames_task)

print(ames_task)
```

Lastly we want to compute a simple featureless baseline, i.e., always predicting the median sale price.

```{r preproc-007, echo = TRUE, eval = TRUE, message=FALSE}

baseline_lrn <- lrn("regr.featureless", robust = TRUE)

baseline_res <- resample(ames_task, baseline_lrn, cv10)
baseline_res$aggregate(measure)
```

## Factor Encoding

When working with categorical features in machine learning, it is important to properly encode them as numeric features.
Categorical features can be distinguished by their cardinality, which refers to the number of levels they contain.
There are three types of categorical features: binary, low-cardinality, and high-cardinality.

Let us first have a look at the number of levels of each categorical feature.

```{r preproc-008, echo = TRUE, eval = TRUE, message=FALSE}

n_lvls <- sapply(ames[, .SD, .SDcols = sapply(ames, is.factor)], function(x) length(levels(x)))
sort(n_lvls)
```

No universal threshold exist when a feature should be considered high-cardinality.

For the ames housing data, we assume `Exterior_1st `, `Exterior_2nd`, `MS_SubClass` an `Neighborhood` to be high-cardinality.

### Binary Features

Binary features can be encoded simply as 1 or 0, which does not change anything except the interpretation of coefficients in linear models (LM) or generalized additive models (GAM).

Low-cardinality features, on the other hand, require one-hot or dummy encoding.
This means that for each level of the feature, a new binary feature is created.
This can quickly result in a large number of new features, so it is important to carefully consider the properties of the learner being used to handle them.

For high-cardinality features, regularized target or impact encoding, clustering, and hashing are some of the techniques that can be used.
It is important to note that while some learners can handle low-cardinality features, they might crash when high-cardinality features are present.

Tree-based algorithms are generally better equipped to handle even high-cardinality categorical features.
However, it is still important to optimize the encoding of each feature based on the algorithm and hyperparameter configuration being used.

Additionally, it is important to introduce and tune a threshold hyperparameter that determines when to use high-cardinality encoding.
Finally, the encoder should be able to handle new feature levels that occur at test time without crashing.


* For most learners categorical features need to be encoded in numeric features.
* Distinguish between binary, low cardinality and high cardinality categorical features.
* Low or high cardinality refers to the number of levels.
    * Binary: Encode as 1 / 0. Does not change anything except intepretation of coefficients in LM/GAM
    * Low-cardinality: One-hot / dummy encoding.
    * High-cardinality: Regularized target/impact encoding, clustering, hashing.
* Check the learner properties if they can handle categorical features.
* Be careful, a learner, e.g. LM, might be able to handle low-cardinality features, but might crash when high cardinality features are present.
* Most Tree-based algorithms can natively handle even high-cardinality categorical features.
* Optimal encoding can vary between each feature, algorithm and hyperparameter configuration.
* Introduce and tune threshold hyperparameter that decides when to use high-cardinality encoding
* The encoder should also be able to handle new feature levels occuring at test time without crashing.

## Imputation

* Imputation is the process of replacing missing values with artificial substituted values.
* Visualize `NA`s with a missmap
* Missingness can encode important information.
    * Missing completely at random
    * Missing at random: Missingness is related to some other features. EXAMPLE
    * Missing not at random: Missingness is related to the feature itself. EXAMPLE
* Simple imputation techniques replace missings with the mean, median, mode or a sample from empirical distribution of the feature.
* For categorical features, missing values can easily be replaced by a new seperate level.
* To keep track of the imputation, binary indicator features are added.
* Check the learner properties if they can handle missing values
* Some tree-based algorithms can natively handle missing values.
* Model-based imputation trains a machine learning model to predict missing values using the remaining features.
    * The imputation model should be able to handle missings natively.
    * The choice of learner and its hyperparameters add additional complexity to the imputation.
    * Random Forests are a reasonable choice.

## Scaling Features and Targets

* Log-scaling of the target for some models
* Log-scaling of features for some models
* Tree-based methods do only consider the order of features
* Normalization for distance based methods
* Polynomials, interections and basis expansions are mainly interesting for LMs and not really considered here

## Feature Extraction

* Unfortunately we don't have information about the quality and number of kitchen appliances, which can have an effect on the sales price.
* But we have information on power consumptions in the kitchen over an average day in 2-Minute intervals
* We cannot directly add these features to our data
* Some information about the curves should give information about the kitchen:
    * Max-used wattage
    * Overall used wattage
    * Number of peaks
    * ...
* Which features to use? Extract a large number of features and use feature selection methods, or include extraction strategies in pipeline definition.
* Some features are easily interpretable and domain knowledge can help to define meaningful extractions.
* More complex features, e.g. wavelets, allow to capture more complex structures, but are not interpretable anymore.

## Multiple Data Sources

* We paint a somewhat unrealistic picture of how ML works in practice
* It is rarely the case that a single data source, such as the ames housing data is present
* Much more often there are many different data sources: Tables, data bases, spreadsheets, ... that need to be consolidated and understood before we can even start to do the above discussed preprocessing steps
* We illustrate that in a very simple example, where information about renovations of the properties is not present in the data, but in a seperate table in a `many-to-one`relation.
* First we need to aggregate information from that table and append it to our data.
