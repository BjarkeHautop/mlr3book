---
author:
  - name: Author 1
    orcid:
    email:
    affiliations:
      - name: Affiliation 1
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: TODO (150-200 WORDS)
---

# Preprocessing {#sec-preprocessing}

{{< include _setup.qmd >}}

The last chapter (@sec-pipelines) gave an technical introduction to `r mlr3pipelines`, this chapter will show important concepts and typical problems when working with data for machine learning.

In the realm of machine learning, there exist various definitions for preprocessing, data cleaning and feature engineering.
To clarify, preprocessing refers here to everything that happens with the data before it is used to fit the model, while postprocessing encompasses everything that occurs with predictions after the model is fitted.
Data cleaning is an important aspect of preprocessing as it involves the removal of errors, noise, and redundancy in the data.
There is little ambiguity in what needs to be done here, and it is generally straightforward.
Feature engineering, on the other hand, covers all other transformations of data before it is fed to the machine learning model.
It involves the creation of useful features from possibly unstructured data, such as written text, sequences or images.

It's often said that deep learning automates feature engineering, but this mainly refers to feature extraction from written text and images.

The goal of feature engineering is to prepare the data so that a model can be trained on it, and/or to further improve predictive performance.

It's important to note that feature engineering helps mostly for simpler algorithms, while highly complex models gain little from it and require little data preparation to be trained.
Common difficulties in data include features with (high) skew distributions, high cardinality categorical features, missing observations, high dimensional dimensionality and imbalanced classes in classification tasks.

In this chapter we will use `r mlr3pipelines` on a data set introduced below that possess most of the mentioned difficulties.
We won't be able to cover every concept and we refer the reader to relevant further resources.
Since feature selection is such an important aspect of preprocessing it was already introduced in @sec-feature-selection.


## Ames Housing Data

The data we will be using is an adapted version of the Ames housing data, initially collected by [@de2011ames].

This data was collected as an *alternative to the Boston Housing data as an end of semester regression project* and is used in various other books on `r link("http://www.feat.engineering/intro-intro.html", "feature engineering")` and `r link("https://www.tmwr.org/ames.html","machine learning")` with R.
Raw and processed versions of the data can be directly loaded from the `AmesHousing` package.

The dataset encompasses data related to 2,930 residential properties situated in Ames, Iowa, sold between 2006 and 2010.
It contains 81 features on various aspects of the house (basement, garage, size, fence, pool, porch, bedrooms, etc.), size and shape of the lot as well as information about condition and quality.

The prediction target is the sale price in USD, hence it is a regression task (@sec-tasks).

::: callout-note
We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset.
The code to recreate this version of the data from the original raw data can be found `r link("https://github.com/ja-thomas/extend_ames_housing", "here")`
:::

We read in the data from the web and quickly peek at the target distribution

```{r preproc-001, echo = TRUE, eval = TRUE, message=FALSE}

library(data.table)
library(ggplot2)

ames <- fread("https://raw.githubusercontent.com/ja-thomas/extend_ames_housing/main/data/ames_dirty.csv", stringsAsFactors = TRUE)

ggplot(ames, aes(x = Sale_Price)) +
    geom_histogram()
```

## Data Cleaning

In a first step we explore the data and look for simple problems such as constant or duplicated features.
This can be done quite efficiently with a package like `DataExplorer`.
The full report can be found in Appendix XXX.

Here we just summarize the important findings:

```{r preproc-002, echo = TRUE, eval = TRUE, message=FALSE}

summary(ames$Misc_Feature_2)
```

`Misc_Feature_2` is a factor with only a single level `othr`.

```{r preproc-003, echo = TRUE, eval = TRUE, message=FALSE}

identical(ames$Condition_2, ames$Condition_3)
```

`Condition_2` and `Condition_3` are identical.

```{r preproc-004, echo = TRUE, eval = TRUE, message=FALSE}

cor(ames$Lot_Area, ames$Lot_Area_m2)
```

Both features represent the lot area, just on different scales.

For all three cases simple removal is sufficient.

```{r preproc-0045, echo = TRUE, eval = TRUE, message=FALSE}

to_remove <- c("Lot_Area_m2", "Condition_3", "Misc_Feature_2")
```


Next to constant or identical features, further typical problems that should be checked for are

1) ID columns, i.e., columns that are unique for every observations should be removed or tagged.
2) `NA`s not correctly encoded, e.g. as `"NA"` or `""`
3) Semantic errors in the data, e.g., negative `Lot_Area`
4) Numeric features encoded as categorical
5) ...

Before we continue with feature engineering we create a task, choose a suitable performance measure and resampling strategy.

```{r preproc-006, echo = TRUE, eval = TRUE, message=FALSE}
library(mlr3verse)


ames_task <- TaskRegr$new(backend = ames, target = "Sale_Price", id = "ames")

ames_task$select(setdiff(ames_task$feature_names, to_remove))

measure <- msr("regr.mae")

cv10 <- rsmp("cv")

cv10$instantiate(ames_task)

print(ames_task)
```

Lastly we want to compute a simple featureless baseline, i.e., always predicting the median sale price.

```{r preproc-007, echo = TRUE, eval = TRUE, message=FALSE}

baseline_lrn <- lrn("regr.featureless", robust = TRUE)

baseline_res <- resample(ames_task, baseline_lrn, cv10)
baseline_res$aggregate(measure)
```

## Factor Encoding

In machine learning, categorical features are variables that can take on a limited set of values, such as `Paved_Drive` with possible values of `Dirt_Gravel`, `Partial_Pavement`, `Paved` in the Ames housing data.

However, many machine learning algorithms require numerical inputs to function properly.

As shown in the Introduction (@sec-lrns-add-list), we can easily list learners that directly support categorical features.


```{r preproc-0075, echo = TRUE, eval = TRUE, message=FALSE}
as.data.table(mlr_learners)[task_type == "regr" &
    sapply(feature_types, function(x) "factor" %in% x)]
```

For other learners we need to convert categorical features into numerical ones is through encoding.

Categorical features can be distinguished by their cardinality, which refers to the number of levels they contain.
There are three types of categorical features: binary, low-cardinality, and high-cardinality.

Let us have a look at the number of levels of each categorical feature.

```{r preproc-008, echo = TRUE, eval = TRUE, message=FALSE}

n_lvls <- sapply(ames[, .SD, .SDcols = sapply(ames, is.factor)], function(x) length(levels(x)))
sort(n_lvls)
```

Generally, no universal threshold exist when a feature should be considered high-cardinality.

For the Ames housing data, we assume `Exterior_1st `, `Exterior_2nd`, `MS_SubClass` an `Neighborhood` to be high-cardinality by assuming a threshold of 10.
Generally, this threshold can be considered a hyperparameter and can be tuned jointly with all other hyperparameters.

Some learners which support handling categorical features out of the box, can still crash for high-cardinality features, since they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding.

Low-cardinality features can be handled by one-hot encoding.
One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature.
Theoretically it is sufficient to create one less binary feature than levels, as setting all binary features to zero is also a valid representation.
This is typically called dummy or treatment encoding and required if the learner is a generalized linear (GLM) or additive model (GAM) model.

For high-cardinality features, impact encoding is a reasonable approach.

Impact encoding is a type of encoding that converts categorical features into numeric values based on the impact of the feature on the target.
The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature.
Impact encoding involves the following steps:

1) Group the target variable by the categorical feature.
2) Compute the mean of the target variable for each group.
3) Compute the global mean of the target variable.
4) Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.
5) Replace the categorical feature with the impact scores.

By using impact encoding, we can preserve the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target.
The main advantage, compared to one-hot encoding is that only a single numeric feature is created regardless of the number of levels of the categorical features.
Hence, it is especially useful for high-cardinality features.
Since information from the target is now used to compute the impact scores, it is crucial that the encoding process is embedded in the cross-validation process to avoid label leakage.

The binary features `Alley`, `Central_Air`, `Street` can be encoded simply as 1 or 0, which does not change anything.

First we collapse all levels that occur less than 1% of the time and build a simple pipelines that encode categorical features based on their cardinality using `selector_cardinality_greater_than`.


```{r preproc-009, echo = TRUE, eval = TRUE, message=FALSE}

factor_pipeline <-
    po("collapsefactors", no_collapse_above_prevalence = 0.01) %>>%
    po("encodeimpact", affect_columns = selector_cardinality_greater_than(10), id = "high_card_enc") %>>%
    po("encode", method = "one-hot", affect_columns = selector_cardinality_greater_than(2), id = "low_card_enc") %>>%
    po("encode", method = "treatment", affect_columns = selector_type("factor"), id = "binary_enc")

factor_pipeline$plot()
```

While many implementations of tree based algorithms, such as `rpart` or `ranger` are able to handle categorical features, `xgboost` cannot and requires a preprocessing step.

```{r preproc-010, echo = TRUE, eval = TRUE, message=FALSE}

xgboost_preproc <- GraphLearner$new(factor_pipeline %>>% lrn("regr.xgboost", nrounds = 100), id = "regr.xgboost")

learners <- list(
    baseline = baseline_lrn,
    tree = lrn("regr.rpart"),
    xgboost = xgboost_preproc
)

design <- benchmark_grid(ames_task, learners = learners, cv10)
bmr <- benchmark(design)
plot(bmr, measure = measure)
bmr$aggregate(measure = measure)[, .(learner_id, regr.mae)]
```


## Missing Values

Referring again to our `DataExplorer` report in Appendix XXX, we can see that most features contain no missing values, while 7 features contain a substantial amount of missings.

While both `rpart` and `xgboost` from the previous section could handle missing values automatically, both `ranger` and `lm` that we would like to consider do not.

For simple imputation techniques, missing values can be replaced with the mean, median, mode, or a sample from the empirical distribution of the feature.
For categorical features, missing values can easily be replaced by a new separate level, e.g. called `.MISSING`.
To keep track of the imputation, binary indicator features are added.

```{r preproc-011, echo = TRUE, eval = TRUE, message=FALSE}

impute_pipeline <- list(
    po("missind", type = "integer", affect_columns = selector_type("integer")),
    po("imputehist", affect_columns = selector_type("integer"))
) %>>%
    po("featureunion") %>>%
    po("imputeoor", affect_columns = selector_type("factor"))


impute_pipeline$plot()
```

```{r preproc-012, echo = TRUE, eval = TRUE, message=FALSE}

learners$ranger_preproc <- GraphLearner$new(impute_pipeline %>>% lrn("regr.ranger"), id = "regr.ranger")
```

`r mlr3pipelines` offers a simple and reusable pipeline for (among other things) imputation and factor encoding called `pipeline_robustify`.

```{r preproc-013, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}

robustify <- mlr3pipelines::pipeline_robustify()
robustify$plot()

learners$lm_preproc <- GraphLearner$new(robustify %>>% lrn("regr.lm"), id = "regr.lm")

design <- benchmark_grid(ames_task, learners = learners, cv10)
bmr <- benchmark(design)
plot(bmr, measure = measure)
bmr$aggregate(measure = measure)[, .(learner_id, regr.mae)]
```

Many more advanced imputation strategies exist.
For example, in model based imputation, additional machine learning models are trained to predict missing values from other features.
Multiple imputation, resamples the data and imputes each value multiple time to attain more robust estimates.
These more advanced techniques very rarely improve the model substantially and the simple imputation techniques introduced before are usually sufficient.



## Scaling Features and Targets

* Log-scaling of the target for some models
* Log-scaling of features for some models
* Tree-based methods do only consider the order of features
* Normalization for distance based methods
* Polynomials, interactions and basis expansions are mainly interesting for LMs and not really considered here

## Feature Extraction

* Unfortunately we don't have information about the quality and number of kitchen appliances, which can have an effect on the sales price.
* But we have information on power consumption in the kitchen over an average day in 2-Minute intervals
* We cannot directly add these features to our data
* Some information about the curves should give information about the kitchen:
    * Max-used wattage
    * Overall used wattage
    * Number of peaks
    * ...
* Which features to use? Extract a large number of features and use feature selection methods, or include extraction strategies in pipeline definition.
* Some features are easily interpretable and domain knowledge can help to define meaningful extractions.
* More complex features, e.g. wavelets, allow to capture more complex structures, but are not interpretable anymore.

## Multiple Data Sources

* We paint a somewhat unrealistic picture of how ML works in practice
* It is rarely the case that a single data source, such as the ames housing data is present
* Much more often there are many different data sources: Tables, data bases, spreadsheets, ... that need to be consolidated and understood before we can even start to do the above discussed preprocessing steps
* We illustrate that in a very simple example, where information about renovations of the properties is not present in the data, but in a seperate table in a `many-to-one`relation.
* First we need to aggregate information from that table and append it to our data.
