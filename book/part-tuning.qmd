# Part II: Tuning and Feature Selection

{{< include _setup.qmd >}}

In this part of the book we are going to look at more advanced methodology that is essential to developing powerful ML models with good predictive ability.
We will demonstrate how to manually make use of these methods using `mlr3` code and also the automated implementations that are included to make your machine learning workflow even more efficient.

```{r, echo=FALSE}
#| fig-align: "center"
#| fig-alt: Diagram showing 13 boxes representing model-agnostic HPO.  On the top are two boxes, one that says  "Search Space" and the other "Tuner", these are connected by a line to "Propose Hyperparameter Configurations". That box has an arrow pointing towards another box "Evaluate by Resampling", which has four blue boxes pointing towards it  "Task", "Learner", "Resampling", and "Measure". "Evaluate by Resampling" has one line below it connected to "Objective", and an arrow to the right connected to "Terminator". This "Terminator" box has an arrow pointing down to "Optimal Hyperparameter Configuration" and right to "Update Tuner". Finally the "Update Tuner" box has a line below it connecting to "Archive" and an arrow above it connecting back to "Propose Hyperparameter Configurations".
knitr::include_graphics("Figures/hpo_loop.png")
```

* @sec-optimization introduces hyperparameter optimization (HPO), which is the process of tuning model hyperparameters to obtain better model performance. Tuning is implemented via the `r mlr3tuning` package, which also includes methods for automating complex tuning processes, including nested resampling.
* The performance of machine learning models can be improved by tuning hyperparameters but also by carefully selected features from the training dataset. @sec-feature-selection introduces manual and automated feature selection with filters and wrappers implemented in `r mlr3filters` and `r mlr3fselect`.
* For readers interested in taking a deep dive into tuning, @sec-optimization-advanced discusses two advanced tuning methods: Hyperband with `r mlr3hyperband` (@sec-hyperband)  and Bayesian Optimization (@sec-bayesian-optimization) with `r mlr3mbo`. This chapter goes into more methodological detail than others to provide more insight into these methods.
