---
author:
  - name: Florian Pfisterer
    orcid: 0000-0001-8867-762X
    email:
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract: TBD
---

```{r special-001, include = FALSE}
set.seed(8)
```

# Fairness

{{< include _setup.qmd >}}

## Fairness {#fairness}

```{r special-048, include = FALSE, cache = FALSE}
library("mlr3fairness")
```

In this chapter, we will explore fairness in automated decision making and how we can build better systems when it comes to systems that make (automated) decisions about individuals.
Such systems range from domains such as banking (credit-scoring) and hiring (applicant scoring) to systems that help with medical decisions.
Methods to help with auditing for or building better models can be found in the `r mlr3fairness` package.
This chapter heavily borrows from the paper accompanying the pacakge [@mlr3fairness].

Automated decision-making systems based on data-driven models are becoming increasingly common, and studies have found that they often outperform human experts in making decisions, especially in high-stakes scenarios, leading to more efficient and accurate predictions.
However, without proper auditing, these models can result in negative consequences for individuals, especially those from underprivileged groups.
The proliferation of such systems in everyday life has made it important to address the potential for biases in these models.
For instance, historical biases and sampling biases in the data used to train these models can lead to replication of such biases in the future, as well as inadequate representation of unprivileged populations, leading to models that perform well in some groups but worse on others.
Biases in the measurement of labels and data, as well as feedback loops, are other sources of biases that need to be addressed.
ML-driven systems are used for highly influential decisions such as loan accommodations, job applications, healthcare, and criminal sentencing.
Therefore, it is cral to develop capabilities to analyze and assess these models not only with respect to their robustness and predictive performance but also with respect to potential biases.

### What is bias?

With bias in the context of fairness, we usually refer to disparities in how a model treats individuals or groups.
In order to understand this better, we will first discuss, which disparities we might want to consider in this context.
Then we can discuss how to detect and quantify those disparities in machine learning models, and finally how they can be translated to so-called *fairness metrics*.

#### Notions of fairness

In this article, we will concentrate on a subset of bias definitions, so-called *group fairness*.
As a scenario, we might for example imagine that we develop a system that makes decisions about whether a patient should be considered for some new treatment.
Our goal might now be, that those decisions are *fair* across groups defined by a *sensitive attribute*, which could e.g., be gender, race or age.
In the following, we will often denote this sensitive group with *A* and for simplicity assume it is binary and can only take two values, e.g. 0 and 1.
We will now present two different perspectives on fairness presented in [@fairmlbook;@wachter20].

The first group of fairness notions, *bias preserving* fairness notions, also called **Separation**, requires that the prediction made by the model is independent of the sensitive attribute given the true label.
In other words, the model should make roughly the same amount of errors (or correct predictions) in each group.
Several metrics fall under this category, such as Equalized Odds, which requires the same true positive and false positive rates across all groups.

The second group, *bias transforming* fairness notions, also called **Independence**, only requires that the prediction made by the model is independent of the sensitive attribute.
This group includes the concept of demographic parity, which requires that the proportion of positive predictions is equal across all groups.

It is important to note that these metrics condense a large variety of societal issues into a single number, and they are therefore limited in their ability to identify biases that may exist in the data. Similar, those metrics do not easily translate to legal principles. For example, if societal biases lead to disparities in an observed quantity (such as SAT scores) for individuals with the same underlying ability, these metrics may not identify existing biases.
These metrics often naturally extend to more complex scenarios, such as multi-class classification, regression, or survival analysis.
Additional fairness notions beyond statistical group fairness include individual fairness, which assesses fairness at an individual level based on the principle of treating similar cases similarly and different cases differently, and causal fairness notions which incorporate causal relationships in the data and propose metrics based on a directed acyclic graph [@fairmlbook;@mitchell21].


#### Choosing fairness notions

As discuessed above, selecting a fairness notion requires careful consideration of the contexts a model will be used in.
Bias preserving metrics, such as equalized odds and equality of opportunity, require that errors made by a model are equal across groups, but might not account for label bias.
Bias transforming methods on the other hand do not depend on labels and can help detect biases arising from different base rates across populations, but enforcing them might induce a shift in the data distribution, possibly leading to feedback loops.

#### Translating notions of fairness into code.

In order to translate the independence requirements stated above into a fairness metric, we investigate differences between groups.
As an example, for a metric $M$, e.g., the true positive rate (TPR), we calculate the difference in the metric across the two groups:

$$
\Delta_{M} = M_{A=0} - M_{A=1}.
$$

To provide an example, with ${P}\left(\hat{Y} = 1 \mid A = \star, Y = 1\right)$ denoted with $TPR_{A=\star}$, we calculate the difference in TPR between the two groups:
$$
\Delta_{TPR} = TPR_{A=0} - TPR_{A=1}.
$$
When $\Delta_{TPR}$ now significantly deviates from $0$, the prediction $\hat{Y}$ violates the requirement for *equality of opportunity* formulated above.

It is important to note that in practice, we might not be able to perfectly satisfy a given metric, e.g., due to stochasticity in data and labels.
Instead, to provide a binary conclusion regarding fairness, a model could be considered fair if $|\Delta_{TPR}| < \epsilon$ for a given threshold $\epsilon > 0$, e.g., $\epsilon = 0.05$.
This allows for small deviations from perfect fairness due to variance in the estimation of $TPR_{A=\star}$ or additional sources of bias.

This approach allows us to construct a fairness `r ref("Measure")` from arbitrary metrics, by supplying a `base_measure`:

```{r special-049}
msr("fairness", base_measure = msr("classif.tpr"))
```

For convenience, we have implemented a variety of `r ref("Measure")`s that is made available when the  `r mlr3fairness` package is loaded.
Fairness measures can then be constructed via `msr()` like any other measure in `r mlr3`.

:::{.callout-tip}
Fairness metrics have a  *fairness* prefix, and simply calling `msr()` without any arguments will return a list of all available measures including fairness metrics.
:::

#### Example: The adult dataset

In the following chunk, we retrieve the binary classification task with id `"adult_train"` from the package.
It contains a part of the `Adult` data set [@].

The task is to predict whether an individual earns more than $50.000 per year.
The column `"sex"` is already set as a binary sensitive attribute with levels `"Female"` and `"Male"`.

```{r special-051}
library("mlr3fairness")
task = tsk("adult_train")
print(task)
```

#### Setting a sensitive attribute

For a given task, we can select one or multiple sensitive attributes.
In `r mlr3`, the sensitive attribute is identified via the column role `pta` short for protected attribute) and can be set as follows:

```{r special-049, eval = FALSE}
task$set_col_roles("sex", add_to = "pta")
```

This example sets the `"sex"` column of our task as a sensitive attribute.
This information is then automatically passed on when the task is used, e.g., when computing fairness metrics.
If more than one sensitive attribute is specified, metrics will be computed based on intersecting groups formed by the columns.

#### Auditing a model for bias

We can now fit any `r ref("Learner")` on this task and score the resulting `r ref("Prediction")`.

```{r special-052}
learner = lrn("classif.rpart", predict_type = "prob")
idx = partition(task)
learner$train(task, idx$train)
prediction = learner$predict(task, idx$test)
```

We then employ a fairness measure, here the discrepancy in accuracy between groups:

```{r special-053}
measure = msr("fairness.acc")
prediction$score(measure, task = task)
```

This now reports the difference in accuracy between the two groups available in the data.
We can similarly use the metric to score a `ResamplingResult` or `BenchmarkResult`.

### Fair Machine Learning

If we now detect that our model is unfair, a natural next step might be to try and mitigate such biases.
The `r mlr3fairness` package comes with several options to adress biases in models, which broadly fall into three categories:
Data pre-processing, employing fair models, or adapting model predictions [@caton-arxiv20a].
Those methods often slightly decrease predictive performance and we might therefore want to try out which of the existing approaches balances predictive performance and fairness.

Pre- and postprocessing schemes can be connected to learners with the help of `r mlr3pipelines`.
If you are not familiar with pipelines, you can familiarize yourself with them in the mlr3pipelines @sec-pipelines chapter.
We provide two examples below, first preprocessing to balance observation weights with `po("reweighing_wts")` and second post-processing predictions to enforce equalized odds by flipping predictions.

```{r special-054, eval = FALSE}
library(mlr3pipelines)
library(mlr3learners)
l1 = po("reweighing_wts") %>>% lrn("classif.rpart")
l2 = po("learner_cv", learner = lrn("classif.rpart")) %>>% po("EOd")
```

Similarly, we can also fit learners that incorporate fairness considerations directly, e.g. a linear model with fairness constraints which also comes with `r mlr3fairness`.

```{r special-055, eval = FALSE}
learner = lrn("classif.fairzlrm")
```

Note, that algorithmic interventions might often enforce fairness in suboptimal ways.
It is therefore important to address biases at their root cause instead of relying solely on algorithmic interventions.

We are now ready to prepare the two interventions using the `benchmark()`function.

```{r special-0541, eval = FALSE}
lrns = list(lrn("classif.rpart"), l1, l2)
grd = benchmark_grid(task, lrns, rsmp("cv", folds = 2L))
bmr = benchmark(grd)
bmr$aggregate(msrs(c("classif.acc", "fairness.eod")))
```

We can now study the result using built-in plot functions, e.g. the `fairness_accuracy_tradeoff` function.

```{r special-0542, eval = FALSE}
fairness_accuracy_tradeoff(bmr, msr("fairness.eod"))
```

This depicts the results of the cross-validation folds along with their aggregate.
Employing the post-processing strategy improves the model for the `fairness.eod` (equalized odds difference) metric at the cost of some accuracy. Employing the reweighing strategy however seems to have a negligible effet.

Combining `r mlr3fairness` with `r mlr3pipelines` and `r mlr3tuning` allows for tuning over complex pipelines and e.g. simultaneously optimizing for performance and fairness. We describe this in more detail in the mlr3fairness paper [@mlr3fairness].

### Further considerations

In additon to implementing fairness metrics and debiasing methods, the `r mlr3fairness` package comes with a variety of additional functionality that might be useful when aiming to achieve fair outcomes:

**Documentation**

Because fairness aspects can not always be investigated based on fairness metrics, it is important to document data collection and the resulting data as well as the models resulting from this data.
Informing auditors about those aspects of a deployed model can lead to better assessments of a model's fairness.
Questionnaires for ML models and data sets have been proposed in literature.
We further add automated report templates using R markdown for data sets and ML models.
In addition, we provide a template for a *fairness report* which includes many fairness metrics and visualizations to provide a good starting point for generating a fairness report inspired by the *Aequitas Toolkit*.
A preview for the different reports can be obtained from the [Reports vignette](https://mlr3fairness.mlr-org.com/articles/reports-vignette.html).

The functions below create a template for creating a data sheet or model cards that help with documenting machine learning models in a specified folder.

```{r special-056, eval = FALSE}
report_datasheet()
report_modelcard()
```

**Visualization**

Similarly, visualizations can help better understand discrepancies between groups or differences between models.
For an in-depth dive into visualizations, please consider the [Visualization vignette](https://mlr3fairness.mlr-org.com/articles/visualization-vignette.html).

To showcase available visualizations, we will again use the `adult` dataset [@] and train a decision tree as well as a random forst on it.

```{r special-057}
task = tsk("adult_train")$filter(1:5000)
learner = lrn("classif.ranger", predict_type = "prob")
learner$train(task)
prediction = learner$predict(tsk("adult_test")$filter(1:5000))
```

```{r special-058}
#| fig-cap: Fairness prediction density plot (left) showing the density of predictions for the positive class splitted into "Male" and "Female" individuals. The metrics comparison plot (right) depicts the model's scores across the specified metrics using bars.
#| fig-alt: Two panel plot including two overlapping densities with a concentration on the right for the model's prediction of the positive class labeled "Male" and "Female". Both densities are relatively equal. The second panel shows three bar charts for the three metrics ("fairness.fpr", "fairness.tpr", "fairness.eod") with bars at roughly 0.035, 0.002, 0.018 for the three metrics respectively.
#| label: fig-fairness
library(patchwork)
p1 = fairness_prediction_density(prediction, task = task) +
  theme(legend.position="bottom")
p2 = compare_metrics(prediction, msrs(c("fairness.fpr", "fairness.tpr", "fairness.eod")), task = task)

(p1 + xlab("") + p2) * 
  theme_bw() *
  scale_fill_viridis_d(end = 0.8, alpha = 0.8) *
  theme(axis.text.x = element_text(angle = 15, hjust = .7))
```

In this example, we can for example see that the prediction density between `Male` and `Female` individuals predicting the positive outcome is relatively similar indicating relatively small biases.
This can also be observed from the different metrics, where relatively small discrepancies in the fairness metric are observed.

### Fairness: Concluding remarks

The functionality introduced above has the goal to help users investigating their models for potential biases and potentially mitigate them. Deciding whether a model is fair however requires additional investigation, such as deciding what the measured quantities actually mean for an individual in the real world and what other biases might exist in the data that might lead to discrepancies in how e.g. covariates or the label are measured.

:::{.callout-note}
Fairness metrics cannot be used to prove or guarantee fairness, and their selection depends on the societal context and the implications of the decisions made. They serve as a diagnostic tool to detect disparities and as a basis for model selection and making fair decisions in practice. However, fairness metrics are reduction of complex societal processes into mathematical objectives and require abstraction steps, which can invalidate the metric. Additionally, practitioners should look beyond the model and consider the data used for training and the process of data and label acquisition. Fairness metrics should be used for exploratory purposes only, and practitioners should not solely rely on them to make decisions about employing an ML model or assessing whether a system is fair.
:::

We hope, that pairing the functionality available in `r mlr3fairness` with additional exploratory data analysis, a solid understanding of the societal context in which the decision is made and integrating additional tools, e.g. from interpretability might help to prevent unfairness in systems deployed in the future.
