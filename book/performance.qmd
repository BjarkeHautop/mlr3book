# Evaluation, Resampling and Benchmarking {#sec-performance}

{{< include _setup.qmd >}}

`r authors("Evaluation, Resampling and Benchmarking")`

<!-- TODO: check links e.g. autoplot etc. -->

In supervised machine learning, a model can only be deployed in practice if it generalizes well to new, unseen data, that is if it has a good `r index("generalization performance", aside = TRUE)`.
Accurate estimation of the generalization performance is crucial for many aspects of machine learning application and research --- whether we want to fairly compare a novel algorithm with established ones, or to find the best algorithm for a particular task after tuning.
The concept of `r index("performance estimation")` provides information on how well a model will generalize to new data and plays an important role in the context of model comparison ( @sec-benchmarking), model selection, and hyperparameter tuning (@sec-optimization).

Assessing the generalization performance of a model beings with selecting a `r index("performance measure")` that is appropriate for our given task and evaluation goal.
As we have seen in @sec-eval, performance measures typically compute a numeric score indicating how well the model predictions match the ground truth (though some technical measure exceptions were seen in @sec-basics-measures-tech).
Once we have decided on a performance measure, the next step is to adopt a strategy that defines how to use the available data to estimate the generalization performance.
Using the same data to train and test a model is a bad strategy as it would lead to an overly optimistic performance estimate, for example a model that is overfitted (fit too closely to the data) could make perfect predictions on training data simply by memorizing it and then only make random guesses for new data.
In @sec-basics-partition we introduced the `partition()` function, which splits data into data for training the model and data for testing the model and estimating the generalization performance, this is known as the holdout strategy (@sec-holdout-scoring) and is where we will begin this chapter.
We will then consider more advanced strategies for assessing the generalization performance (@sec-resampling), look at robust methods for comparing models (@sec-benchmarking), and finally will discuss specialized performance measures for binary classification (@sec-roc).
For an in-depth overview about measures and performance estimation, we recommend @japkowicz2011evaluating.

## Holdout and Scoring {#sec-holdout-scoring}

The goal of ML is to learn a model that can then be used to make predictions about new data.
For this model to be as accurate as possible, we would ideally train it on as much data as is available.
However, data is limited and as we have discussed we cannot train and test a model on the same data.
In practice, one would usually create an `r index('intermediate model', aside = TRUE)`, which is trained on a subset of the available data and then tested on the remainder of the data.
The performance of this intermediate model, obtained by comparing the model predictions to the ground truth, is an estimate of the generalization performance of the final model.

The `r index('holdout', aside = TRUE)` strategy is a simple method to create this split between training and testing datasets, whereby the original data is split into two datasets using a defined ratio.
Ideally, the training dataset should be as large as possible so the intermediate model represents the final model as well possible.
If the training data is too small, the intermediate model is unlikely to perform as well as the final model, resulting in a pessimistically biased performance estimate.
On the other hand, if the training data is so large that the test data is too small, then we will not have a reliable estimate of the generalization performance.
As a rule of thumb, it is common to use 2/3 of the data for training and 1/3 for testing as this provides a reasonable trade-off between bias and variance of the generalization performance estimate (see also @kohavi1995 and @dobbin2011).

In @sec-basics, we used `partition()` to apply the holdout method to a `r ref("Task")` object.
To recap by example let's split the `penguins` task with a 2/3 holdout, which is the default in `mlr3`:

```{r performance-003}
task = tsk("penguins")
splits = partition(task)
learner = lrn("classif.rpart")
learner$train(task, splits$train)
pred = learner$predict(task, splits$test)
```

We can now estimate the generalization performance of a final model by evaluating the quality of the predictions from our intermediate model.
As we have seen in @sec-eval, this is simply a case of choosing one or more measures and passing them to the `$score()` function.
So to estimate the accuracy of our final model we would pass the accuracy measure to our intermediate model:

```{r}
pred$score(msr("classif.acc"))
```

Many performance measures are based on point-wise losses, which means they compute the differences between the predicted values and ground truth values first on an observation level and then aggregate the individual loss values into a single numeric score (composite loss).
For example, the classification accuracy compares whether the predicted values from the `response` column have the same value as the ground truth values from the `truth` column of the `r ref("Prediction")` object.
Hence, for each observation, the point-wise loss takes either value 1 (if `response` and `truth` have the same value) or 0 otherwise.
The `$score()` method summarizes these individual loss values into a composite loss by counting the fraction of observations where the point-wise loss is 1 (i.e., the fraction of observations where `response` and `truth` have the same value).
Other performance measures that are not point-wise instead act on a set of observations, we will return to this in detail when we look at the AUC measure in @sec-roc.
@fig-score illustrates the input-output behavior of the `$score()` method, we will return to this as we turn to more complex evaluation strategies.

```{r performance-017}
#| echo: false
#| label: fig-score
#| fig-cap: "Illustration of the `$score()` method which aggregates predictions of multiple observations contained in a prediction object into a single numeric score"
#| fig-align: "center"
#| fig-alt: A funnel-shaped diagram where the far left box shows the output from a classification prediction object with row_ids, truth, and response columns. Next to this is a box that just says '$score()', which then passes to the right in a funnel shape to a box that says 'classif.acc 0.920354'.
knitr::include_graphics("Figures/predict-score-single.drawio.svg")
```

## Resampling {#sec-resampling}

`r index("Resampling")` strategies repeatedly split all available data into multiple training and test sets, with one repetition corresponding to what is called a resampling iteration in `r mlr3`.
An intermediate model is then trained on each training set and the remaining test set is used to measure the performance in each resampling iteration.
The generalization performance is finally estimated by aggregating the performance scores over multiple resampling iterations (@fig-ml-abstraction).
By repeating the data splitting process often enough, more data points can be used for both training and testing, allowing a more efficient use of all available data for performance estimation.
Furthermore, a high number of resampling iterations can reduce the variance in our scores and thus result in a more reliable performance estimate.
This means that the performance estimate is less likely to be affected by an unlucky split (e.g., a split that does not reflect the original data distribution), which is a known issue of the holdout method.
An important but subtle distinction to note is that resampling strategies create multiple intermediate models trained on different parts of the available data and average their performance, this means they evaluate the performance of the *learning algorithm* that induced these intermediate models and not the performance of the final model itself.
<!-- It is therefore important to train the intermediate models on nearly all data points from the same distribution so that the intermediate models and the final model are similar. -->
The best we can do if we only have access to a limited amount of data is to estimate the performance of the final model by the performance of the learning algorithm.

```{r performance-002, echo=FALSE}
#| label: fig-ml-abstraction
#| fig-cap: "A general abstraction of the performance estimation process. The available data is (repeatedly) split into (a set of) training data and test data (data splitting / resampling process). The learner is trained on each training dataset and produces intermediate models (learning process). Each intermediate model makes predictions based on the features in the test data. The performance measure compares these predictions with the ground truth from the test data and computes a performance value for each test dataset. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process)."
#| fig-align: "center"
#| fig-alt: "A flowchart-like diagram with 3 overlapping boxes. Left box has the caption 'Data splitting / resampling process', upper right box has caption 'Learning process', and lower right box has caption 'Evaluation process'. The process starts in the left box with 'Data' and an arrow to 'Resampling Strategy', which separates into two elements stacked vertically: 'Train Set(s)' above and 'Test Set(s)' below. The 'Train set(s)' element leads to a 'Learner' box, which is inside the larger 'Learning Process' box. A box that says 'Hyperparameters' also sits within the 'Learning Process' and is connected with an arrow also pointing to 'Learner'. An arrow points from the 'Learner' to a stack of 'Intermediate Model(s)'. One thick arrow goes down into the yellow box to a stack of 'Prediction(s)'. An arrow goes from there to 'Performance measure'. The 'Test set(s)' from earlier also have an arrow to 'Performance measure'. From there, a thick arrow goes to 'Performance Value(s)', which has a final dashed arrow to 'Aggregated Performance'."
knitr::include_graphics("Figures/ml_abstraction-2.svg")
```

A variety of resampling strategies exist, each with their respective advantages and disadvantages, which depend on the number of available samples, the task complexity, and the type of model.

A very common strategy is $k$-fold `r index("cross-validation", aside = TRUE)` (CV), which randomly partitions the data into $k$ non-overlapping subsets, called folds (@fig-cv-illustration).
The $k$ models are trained on training data consisting of $k-1$ of the folds, with the remaining fold being used as the test data, this process is then repeated until each fold has acted exactly once as the test data.
The $k$ performance estimates resulting from each fold are aggregated to obtain a more reliable performance estimate (@hastie2001).
Cross-validation guarantees that each observation will be used in one of the test sets throughout the procedure, making efficient use of the available data for performance estimation.
Common values for $k$ are 5 and 10, meaning each training will consist of 4/5 or 9/10 of the original data respectively.
Several variations of cross-validation exist, including repeated $k$-fold cross-validation [Repeated $k$-fold cross-validation]{.aside}\index{resampling!repeated \$k\$-fold cross-validation} where the $k$-fold process is repeated multiple times, and `r index('leave-one-out cross-validation', aside = TRUE)` (LOO-CV) where the number of folds is equal to the number of observations, leading to the test set in each fold consisting of exactly one observation.
LOO-CV sounds reduces variance in the performance estimate but is computationally very expensive due to the need to fit $N$ models.
For linear or polynomial regression models, LOO-CV with mean squared error (MSE) as the performance measure can be computed efficiently via a closed-form formula without the need to fit $N$ models, yet this does not apply in the general case (see @james2013introduction).
Furthermore, LOO-CV is also problematic in imbalanced binary classification tasks as concepts such as stratification (see @sec-strat-group) cannot be applied to LOO-CV.

`r define("Subsampling")` and `r define("bootstrapping")` are two related resampling strategies.
Subsampling randomly selects a given ratio (4/5 and 9/10 are common) of the data for the training dataset where each observation in the dataset is drawn *without replacement* from the original dataset.
The model is trained on this data and then tested on the remaining data, and this process is repeated $k$ times.
This differs from $k$-fold CV as the subsets of training/test data between iterations are not related and each is drawn independently from one another, which means that, across iterations, observations could occur in more than one testing dataset (but only once per dataset).
Bootstrapping follows the same process as subsampling but data is drawn *with replacement* from the original dataset, this means an observation could be selected multiple times (and thus duplicated) in the training data (but never more than once per test dataset).
This means that bootstrapping can result in training sets of the same size as the original data, but at the cost of repeating some observations.
On average, $1 - e^{-1} \approx 63.2\%$ of the data points will be contained in the training set during bootstrapping, referred to as "in-bag" samples (the other 36.8% are known as "out-of-bag" samples).
For both procedures, it is recommended to choose a higher number of repetitions, e.g. $\geq 200$.
Although increasing this value will lead to longer computation times, the benefit of performing more repetitions to obtain a more reliable performance estimate will usually outweigh higher computation times.
Note that terminology regarding resampling strategies is not consistent across the literature, for example subsampling is sometimes referred to as "repeated holdout" \index{repeated holdout|see{subsampling}} or "Monte Carlo cross-validation" \index{Monte Carlo cross-validation|see{subsampling}}, which is why it is advisable to verify formal definitions of resampling techniques applied in literature.

The choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment, but some rules of thumb are available.
If the available data is fairly small ($N \leq 500$), repeated cross-validation with a large number of repetitions can be used to keep the variance of the performance estimates low (10 folds and 10 repetitions is a good place to start).
For the $500 \leq N \leq 50000$ range, 5- to 10-fold cross-validation is generally recommended.
In general, the larger the dataset, the fewer splits are required, yet sample-size issues can still occur, e.g., due to imbalanced data.
Additional recommendations are given by @hpo_practical, which focuses on the model optimization aspect covered in @sec-optimization.
Properties and pitfalls of different resampling techniques, some of which we have summarized here, have been widely studied and discussed in the literature, e.g., @molinaro2005prediction, @kim2009estimating, and @bischl2012resampling.

<!-- Source: https://docs.google.com/presentation/d/1BJXJ365C9TWelojV93IeQJAtEiD3uZMFSfkhzgYH-n8/edit?usp=sharing -->
```{r performance-007, echo=FALSE}
#| label: fig-cv-illustration
#| fig-cap: "Illustration of a 3-fold cross-validation."
#| fig-align: "center"
#| fig-alt: "A diagram illustration 3-fold cross-validation. Each row of the diagram represents one iteration. In each iteration the available data is split into 3 parts, where in each row a different part is marked as the test set. The two remaining parts are the train set, which is used to train a model. Each iteration results in one performance estimate, and all 3 are averaged in the end."
knitr::include_graphics("Figures/cross-validation.svg")
```

In the rest of this section we will go through querying and constructing resampling strategies in `mlr3`, instantiating train-test splits, and then performing resampling on learners.

### Constructing a resampling strategy {#sec-resampling-construct}

All implemented resampling strategies are stored in the `r ref("mlr_resamplings")` dictionary.

```{r performance-008}
as.data.table(mlr_resamplings)
```

The `params` column shows the parameters of each resampling strategy (e.g., the train-test splitting `ratio` or the number of `repeats`) and `iters` displays the number of performed resampling iterations by default.

`r ref("Resampling", index = TRUE, aside = TRUE)` objects can be constructed by passing the strategy 'key' to the sugar function `r ref("rsmp()", aside = TRUE)`.
For example, to construct the holdout strategy with a 4/5 split (2/3 is default):

```{r performance-009}
rsmp("holdout", ratio = 0.8)
```

Parameters for objects inheriting from `Resampling` work in the exact same way as measures and learners and can be set, retrieved, and updated accordingly.
In practice due to the flaws in this strategy, holdout is more likely to be used via the `partition()` function when performing simple experiments (such as those seen throughout the book).
As an example of more sophisticated strategies:

```{r performance-011}
# 3-fold CV
cv3 = rsmp("cv", folds = 3)
# Bootstrapping with 3 repeats and 9/10 ratio
boot100 = rsmp("bootstrap", repeats = 3, ratio = 0.9)
# 2-repeats 5-fold CV
rcv25 = rsmp("repeated_cv", repeats = 2, folds = 5)
```

When a `r ref("Resampling")` object is constructed, it is simply a definition for how the data splitting process will be performed on the task when running the resampling strategy.
However, it is possible to manually instantiate a resampling strategy, i.e., generate all train-test splits, by calling the `$instantiate()`\index{\$instantiate()}[`$instantiate()`]{.aside} method on a given task.
So carrying on our `penguins` example we can instantiate the 3-fold CV object and then view the row indices of the data selected for training and testing each fold using `$train_set()` and `$test_set()` respectively:

```{r performance-012}
cv3$instantiate(task)
# first 5 observations in first training fold
cv3$train_set(1)[1:5]
# first 5 observations in third test fold
cv3$test_set(3)[1:5]
```

When the aim is to fairly compare multiple learners, we need to ensure that all learners being compared use the same training data to build a model and that they use the same test data to evaluate the model performance.
In practice, manually instantiating resampling strategies is rarely required but might be useful for debugging or digging deeper into a model's performance.
Resampling strategies are instantiated automatically for you when using the `resample()`  method, which we will discuss next.

### Resampling experiments {#sec-resampling-exec}

The `r ref("resample()", aside = TRUE)` function takes a given `Task`, `Learner`, and `r ref("Resampling")` object to run the given resampling strategy.
`resample()` repeatedly fits a model on training sets and stores predictions in a `r ref("ResampleResult", aside = TRUE)` object, which contains all information needed to estimate the generalization performance.

```{r performance-013}
rr = resample(task, learner, cv3)
rr
```

We can see the three iterations (one for each fold) returned by the `ResampleResult`.
As with `Prediction` objects, we can evaluate the score *in each iteration* with `$score()`:

```{r performance-014}
acc = rr$score()
acc[, .(iteration, classif.ce)]
```

::: {.callout-tip}
By default `$score()` evaluate the performance in the test sets in each iteration, however you could evaluate the train set performance with `$score(predict_sets = "train")`.
:::

Whilst `$score()` returns the performance in each evaluation, `r index('$aggregate()', aside = TRUE, code = TRUE)`, returns the aggregated score across all resampling iterations.

```{r}
rr$aggregate()
```

By default, the majority of measures will aggregate scores using a `r index("macro average")`, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations.
However, it is also possible to aggregate scores using a `r index("micro average")`, which pools predictions across resampling iterations into one `r ref("Prediction")` object and then computes the measure on this directly:

```{r performance-015}
rr$aggregate(msr("classif.ce", average = "micro"))
```

We can see a small difference between the two methods, which is because classification error is a point-wise loss (@sec-holdout-scoring), however for errors like the AUC that are defined across the set of observations then the difference between micro- and macro-averaging will be larger.
The default type of aggregation method can be found by querying the `$average` field of a `r ref("Measure")` object.

::: {.callout-tip}
As a simple example to explain macro- and micro-averaging, consider the difference between taking the mean of a vector (micro) compared to the mean of two group-wise means (macro):

```{r}
# macro
mean(mean(c(3, 5, 9)), mean(c(1, 5)))
# micro
mean(c(3, 5, 9, 1, 5))
```
:::

The aggregated score returned by `$aggregate()` estimates the generalization performance of our selected learner on the given task using the resampling strategy defined in the `r ref("Resampling")` object.
While we are usually interested in this aggregated score, it can be useful to look at the individual performance values of each resampling iteration (as returned by the `$score()` method) as well, e.g., to see if any of the iterations lead to very different performance results.
@fig-score-aggregate-resampling visualizes the relationship between `$score()` and `$aggregate()` for a small example based on the `"penguins"` task.

```{r performance-017}
#| echo: false
#| label: fig-score-aggregate-resampling
#| fig-cap: "An example of the difference between `$score()` and `$aggregate()`: The former aggregates predictions to a single score within each resampling iteration, and the latter aggregates scores across all resampling folds"
#| fig-align: "center"
#| fig-alt: "A funnel-shaped diagram. Left: Each resampling iteration contains multiple rows of predictions, with 3 iterations total. Middle: $score() reduces those to one performance score per resampling iteration, which leaves 3 scores. Right: $aggregate() reduces predictions across all resampling iterations to a single performance score."
knitr::include_graphics("Figures/predict-score-aggregate-resampling.drawio.svg")
```

To visualize the resampling results, you can use the `r ref("mlr3viz::autoplot.ResampleResult()")` function to plot scores across folds as boxplots or histograms (@fig-resamp-viz).
Histograms can be useful to visually gauge the variance of the performance results across resampling iterations, whereas boxplots are often used when multiple learners are compared side-by-side (see @sec-benchmarking).

```{r performance-035}
#| layout-ncol: 2
#| label: fig-resamp-viz
#| fig-subcap:
#|   - "Boxplot of accuracy scores."
#|   - "Histogram of accuracy scores."
#| message: false
#| fig-alt: "Left: a boxplot ranging from 0.875 to 1.0 and the interquartile range between 0.925 and 0.7. Right: a histogram with five bars in a roughly normal distribution with mean 0.95, minimum 0.875 and maximum 1.0."
rr = resample(task, learner, rsmp("cv", folds = 10))
autoplot(rr, measure = msr("classif.acc"), type = "boxplot")
autoplot(rr, measure = msr("classif.acc"), type = "histogram")
```

### ResampleResult Objects {#sec-resampling-inspect}

As well as being useful for estimating the generalization performance, the `r ref("ResampleResult")` object can also be used for model inspection.
We can use the `$predictions()` method to obtain a list of `r ref("Prediction")` objects corresponding to the predictions from each resampling iteration, which can be used to analyze the predictions of individual intermediate models from each resampling iteration and, e.g., to manually compute a macro averaged performance estimate.

```{r performance-018}
# list of prediction objects
rrp = rr$predictions()
# print first two
rrp[1:2]

# macro averaged performance
mean(sapply(rrp, function(.x) .x$score()))
```

The `$prediction()` method can be used to extract a single `r ref("Prediction")` object that combines the predictions of each intermediate model across all resampling iterations.
The combined prediction object can be used to manually compute a micro averaged performance estimate, for example:

```{r}
pred = rr$prediction()
pred
pred$score()
```

By default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the `r ref("ResampleResult")` object (only the predictions are required to calculate the performance measure).
However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models.
We can configure the `r ref("resample()")` function to keep the fitted intermediate models by setting `store_models = TRUE`.
Each model trained in a specific resampling iteration is then explicitly stored and can be accessed via `$learners[[i]]$model`, where `i` refers to the `i`-th resampling iteration:

```{r performance-021}
rr = resample(task, learner, cv3, store_models = TRUE)
# get the model from the first iteration
rr$learners[[1]]$model
```

In this example, we could then inspect the most important variables in each iteration to help us learn more about the fitted model:

```{r performance-022}
# print 2nd and 3rd iteration
lapply(rr$learners, function(x) x$model$variable.importance)[2:3]
```

Finally, by restricting a task to two features, we can visualize a 2-dimensional prediction surface of individual models in each resampling iteration (@fig-rsmp-viz-surface).
Prediction surfaces like this are useful for model inspection as they can help identify the cause of unexpected performance result.
They are also popular for educational purposes to illustrate the prediction behavior of different learning algorithms, such as the classification tree in this example with its characteristic orthogonal lines.

```{r performance-036}
#| warning: false
#| label: fig-rsmp-viz-surface
#| fig-cap: "Prediction boundaries from a decision tree learner across 4 cross-validation folds where prediction surface is in a solid color and ground truth are the squares."
#| fig-alt: x-axis is 'bill_length', y-axis is 'flipped_length'. Within those are four boxes, each showing almost identical decision boundaries with light green in the top halves, dark purple in bottom left halves, and dark blue in bottom right halves.
task_small = tsk("penguins")
task_small$select(c("bill_length", "flipper_length"))
rr = resample(task_small, learner, rsmp("cv", folds = 4), store_models = TRUE)
autoplot(rr, type = "prediction")
```

### Custom Resampling {#sec-resamp-custom}

{{< include _optional.qmd >}}

Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study with pre-defined folds.

A custom holdout resampling strategy can be constructed using `rsmp("custom")`, where the row indices of the observations used for training and testing must be defined manually when instantiated in a task.
In the example below, we construct a custom holdout resampling strategy by manually assigning row indices to the `$train` and `$test` fields.

```{r performance-023}
resampling = rsmp("custom")
resampling$instantiate(task,
  train = list(c(1:50, 54:333)),
  test = list(51:53)
)
```

The resulting object can be used like all other resampling strategies and we can see this worked as expected by looking at the `row_ids` in the following result:

```{r}
resample(task, learner, resampling)$prediction()
```

A custom cross-validation strategy can be constructed using `rsmp("custom_cv")`.
In this case, we now have to specify either a custom `factor` variable or a `factor` column from the data to determine the folds.
In the example below, we use a smaller version of the `penguins` task and instantiate a custom 2-fold cross-validation strategy using a `factor` variable called `folds` where the first and third rows are used as the test set in Fold 1, and the second and fourth rows are used as the test set in Fold 2:

```{r performance-025}
task_small = tsk("penguins")$filter(c(1, 100, 200, 300))
custom_cv = rsmp("custom_cv")
folds = as.factor(c(1, 2, 1, 2))
custom_cv$instantiate(task_small, f = as.factor(folds))
resample(task_small, learner, custom_cv)$predictions()
```

### Stratification and Grouping {#sec-strat-group}
<!-- FIXME: REVIEWED TO HERE -->
{{< include _optional.qmd >}}

Using column roles (@sec-row-col-roles), it is possible to group or stratify observations according to a particular feature.
We will look at each of these in turn.

#### Grouped Resampling {.unlisted .unnumbered}

Keeping observations together when the data is split can be useful, and sometimes essential, during resampling -- spatial analysis (@sec-spatiotemporal) is a prominent example of when this is essential, as observations belong to natural groups (e.g., countries).
When observations belong to groups, we need to ensure all observations of the same group belong to *either* the training set *or* the test set to prevent potential leakage of information between training and testing.
For example, in a longitudinal study, measurements are taken from the same individual at multiple time points.
Grouping ensures that the model is tested on data from each *person*, and not each observations, thereby ensuring that data in the training set is not correlated with data in the test set.
In this context, the leave-one-out cross-validation strategy can be coarsened to the "leave-one-object-out" cross-validation strategy, where all observations associated with a certain group are left out (@fig-group).

```{r performance-026, echo=FALSE}
#| label: fig-group
#| fig-cap: "Illustration of the train-test splits of a leave-one-object-out cross-validation with 3 groups of observations (highlighted by different colors)."
#| fig-align: "center"
#| fig-alt: "Three images with a vertical dashed line separating them, each image shows a blue box with text 'Train' and white space around it with text 'Test'. The left image shows a blue box with green and red dots inside it and yellow dots outside it, the caption says 'Iteration 1'. The middle image shows a blue box with green and yellow dots inside it and red dots outside it, the caption says 'Iteration 2'. The right image shows a blue box with yellow and red dots inside it and green dots outside it, the caption says 'Iteration 3'."
knitr::include_graphics("Figures/loobject.svg")
```

The `"group"` column role allows us to specify the column in the data that defines the group structure of the observations.
In the following code we construct a leave-one-out resampling strategy, assign the `"group"` role to the the 'year' column of the `penguins` dataset, instantiate the resampling strategy, and finally show how the years are nicely separated in the first fold.

```{r performance-027}
r = rsmp("loo")
task_grp = tsk("penguins")
task_grp$set_col_roles("year", "group")
r$instantiate(task_grp)
table(task_grp$data(rows = r$train_set(1), cols = "year"))
table(task_grp$data(rows = r$test_set(1), cols = "year"))
```

Other cross-validation techniques work in a similar way, where folds are determined at a group-level (as opposed to an observation-level).

#### Stratified Sampling {.unlisted .unnumbered}

Stratified sampling ensures that one or more discrete features within the training and test sets will have a similar distribution as in the original task containing all observations.
This is especially useful when a discrete feature is highly imbalanced and we want to make sure that the distribution of that feature is similar in each resampling iteration (@fig-stratification).
We can also stratify on the target feature to ensure that each intermediate model is fit on training data where the class distribution of the target is representative of the actual task, this is useful to ensure target classes are not under- or over represented in individual resampling iterations, which would skew the estimation of the generalization performance.

```{r performance-028, echo=FALSE}
#| label: fig-stratification
#| fig-cap: "Illustration of a 3-fold cross-validation with stratification for an imbalanced binary classification task with a majority class that is about twice as large as the minority class. In each resampling iteration, the class distribution from the available data is preserved (which is not necessarily the case for cross-validation without stratification)."
#| fig-align: "center"
#| fig-alt: "The figure shows rectangles in yellow and green to represent the majority and minority class respectively. On the left side are rectangles corresponding to the task before it is split; the majority class is clearly larger than the minority class. In the next three boxes we see Iterations 1-3 where the visual size difference between the majority and minority classes is preserved."
knitr::include_graphics("Figures/stratification.svg")
```

Unlike grouping, it is possible to stratify by multiple discrete features using the `"stratum"` column role.
In this case, stratum would be formed out of each combination of the stratified features, e.g., for two stratified features A and B with levels Aa, Ab, Ba, Bb respectively then the created stratum would be AaBa, AaBb, AbBa, AbBb.

The `penguins` task displays imbalance in the `species` column, as can be seen in the output below:

```{r performance-029}
prop.table(table(task$data(cols = "species")))
```

Without specifying a `"stratum"` column role, the `species` column may have quite different class distributions across the training and test sets of a k-fold cross-validation strategy, as can be seen in the example below.

```{r performance-030}
cv10 = rsmp("cv", folds = 10)
cv10$instantiate(task)
rbind("Fold 1" = prop.table(table(task$data(rows = cv10$test_set(1), cols = "species"))),
"Fold 2" = prop.table(table(task$data(rows = cv10$test_set(2), cols = "species"))))
```

We can see across folds how Chinstrap is represented quite differently (`r round(prop.table(table(task$data(rows = cv10$test_set(1), cols = "species")))[2],2)` vs. `r round(prop.table(table(task$data(rows = cv10$test_set(2), cols = "species")))[2],2)`)

When imbalance is severe, minority classes could be left out of the training sets entirely.
Consequently, the intermediate models within these resampling iterations will never predict the missing class, resulting in a misleading performance estimate for any resampling strategy without stratification, which could have severe consequences for a deployed model as it will perform poorly on the minority class in real-world scenarios (e.g., medical diagnosis of rare diseases).
It is important to be aware of the potential consequences of imbalanced class distributions in resampling and use stratification to mitigate highly unreliable performance estimates.
The code below uses `species` as `"stratum"` column role to illustrate that the distribution of `species` in each test set will closely match the original distribution:

```{r performance-031}
task_str = tsk("penguins")
# set species to have both the 'target' and 'stratum' column role
task_str$set_col_roles("species", c("target", "stratum"))
cv10$instantiate(task_str)

rbind("Fold 1" = prop.table(table(task_str$data(rows = cv10$test_set(1), cols = "species"))),
"Fold 2" = prop.table(table(task_str$data(rows = cv10$test_set(2), cols = "species"))))
```

You can view the observations that fall into each stratum using the `$strata` field of a `Task` object, this can be particularly useful when we are interested in multiple strata:

```{r performance-034}
task_str$set_col_roles("year", "stratum")
task_str$strata
table(task$data(cols = c("species", "year")))
```

## Benchmarking {#sec-benchmarking}

`r index("Benchmarking")` in supervised machine learning refers to the comparison of different learners on one or more tasks.
When comparing *multiple learners on a single task* or on a domain consisting of multiple similar tasks, the main aim is often to rank the learners according to a pre-defined performance measure and to identify the best-performing learner for the considered task or domain.
In an applied setting, benchmarking may be used to evaluate whether a deployed model used for a given task or domain (usually the 'state of the art' or 'SOTA' \index{state of the art}) can be replaced by a better alternative.
When comparing *multiple learners on multiple tasks*, the main aim is often more of a scientific nature, e.g., to gain insights into how different learners perform in different data situations or whether there are certain data properties that heavily affect the performance of certain learners (or certain hyperparameters of learners).
It is common (and good) practice for algorithm designers to analyze the generalization performance or runtime of a newly proposed learning algorithm in a benchmark study where it has been compared with existing learners.

### benchmark() {#sec-bm-design}

`r index('Benchmark experiments')` in `mlr3` are conducted with `r ref("benchmark()", aside = TRUE)`, which simply runs `r ref("resample()")` on each task and learner separately, then collects the results.
The provided resampling strategy is automatically instantiated on each task to ensure that all learners are compared against the same training and test data.

To use the `benchmark()` function we first call `r ref("benchmark_grid()")`, which constructs an exhaustive *design* to describe all combinations of the learners, tasks and resamplings to be used in a benchmark experiment, and instantiates the resampling strategies.
By example, below we setup a design to see if a random forest, decision tree, or featureless baseline (@sec-basics-featureless), perform best across two classification tasks.

```{r performance-037}
tasks = tsks(c("german_credit", "sonar"))
learners = lrns(c("classif.rpart", "classif.ranger", "classif.featureless"),
  predict_type = "prob")
resampling = rsmps("cv", folds = 5)

design = benchmark_grid(tasks, learners, resampling)
head(design)
```

The resulting design is essentially just a `data.table`, which can be modified if you want to remove particular combinations

::: {.callout-warning}
## Reproducibility when using benchmark_grid

By default, `benchmark_grid()` instantiates the resamplings on the tasks, which means that concrete train-test splits are generated.
Since this process is random, it is necessary to set a seed **prior to** calling `benchmark_grid()` in order to ensure reproducibility of the data splits.
:::

The constructed benchmark design can then be passed to `r ref("benchmark()")` to run the experiment and the result is a `r ref("BenchmarkResult")` object:

```{r performance-039}
bmr = benchmark(design)
bmr
```

As `benchmark()` is just an extension of `resample()`, we can once again use `$score()`, or `$aggregate()` depending on your use-case, though note that in this case `$score()` will return results over each fold of each learner/task/resampling combination.

```{r performance-040}
head(bmr$score())
bmr$aggregate()[, .(task_id, learner_id, classif.ce)]
```

This would conclude a basic benchmark experiment where you can draw tentative conclusions about model performance, in this case we would possibly conclude the decision tree is a better performing model.
We draw conclusions cautiously here as we have not run any statistical tests or included standard errors of measures, so we cannot definitively say if one model outperforms the other.

As the results of `$aggregate()` are returned in a `data.table`, you can post-process and analyze the results in any way you want.
A common *mistake* is to average the learner performance over all tasks when the tasks vary significantly.
This is a mistake as averaging the performance will miss out important insights into how learners compare on 'easier' or more 'difficult' predictive problems.
A more robust alternative to compare the overall algorithm performance across multiple tasks is to compute the ranks of each learner on each task separately and then calculate the average ranks.
This can provide a better comparison as task specific 'quirks' are taken into account by comparing learners within tasks before comparing them across tasks.
However, using ranks will lose information about the numerical differences of the calculated performance scores.

Ranking the performance scores can be performed most conveniently with the `r mlr3benchmark` package.
We first use `r ref("as_benchmark_aggr")` to aggregate the `r ref("BenchmarkResult")` and then we use the `$rank_data()` method to convert the performance scores to ranks.

```{r performance-042}
library(mlr3benchmark)
bma = as_benchmark_aggr(bmr, measures = msr("classif.ce"))
bma$rank_data()
```

This results in per-task rankings of the three learners.
Unsurprisingly, the random forest ranked first and the baseline last.
For more advanced statistical tests and `r mlr3benchmark` usage, see @sec-large-benchmarking.

### BenchmarkResult Objects {#sec-bm-resamp}

A `r ref("BenchmarkResult")` object is a collection of multiple `r ref("ResampleResult")` objects.

```{r performance-043}
bmrdt = as.data.table(bmr)
bmrdt[1:2, .(task, learner, resampling, iteration)]
```

The contents of a `r ref("BenchmarkResult")` and `r ref("ResampleResult")` (@sec-resampling-inspect) are almost identical and the stored `r ref("ResampleResult")`s can be extracted via the `$resample_result(i)` method, where `i` is the index of the performed benchmark experiment.
This allows us to investigate the extracted `r ref("ResampleResult")` or individual resampling iterations as shown in @sec-resampling.

```{r performance-044}
rr1 = bmr$resample_result(1)
rr1
rr2 = bmr$resample_result(2)
rr2
```

In addition, `BenchmarkResult`s also have the `r ref('as_benchmark_result()', aside = TRUE, index = TRUE)` method, which can be used to convert objects from `ResampleResult` to `BenchmarkResult`, and then optionally combined, which is useful when conducting experiments across multiple machines.

```{r performance-045}
bmr1 = as_benchmark_result(rr1)
bmr2 = as_benchmark_result(rr2)

c(bmr1, bmr2)
```

Boxplots are most commonly used to visualize benchmark experiments as they can intuitively summarize results across tasks and learners simultaneously.
They can also be used to identify potentially unexpected behavior, such as a learner performing reasonably well for most tasks, but yielding noticeably worse scores in one task.
In the case of @fig-benchmark-box, the three learners show consistent relative performance to each other and in an expected order.

```{r performance-046}
#| fig-height: 5
#| fig-width: 6
#| label: fig-benchmark-box
#| fig-cap: "Boxplots of accuracy scores for each learner across resampling iterations and the three tasks. Random forests (`classif.ranger`) consistently performs outperforms the other learners."
#| fig-alt: Nine boxplots, one corresponding to each task/learner combination. In all cases the random forest performs best and the featureless baseline the worst.
autoplot(bmr, measure = msr("classif.acc"))
```

## Evaluation of Binary Classifiers {#sec-roc}

We will now look at specialized performance measures for binary classification implemented in `r mlr3`.
We start introducing the confusion matrix and some useful performance measures that can be derived from it to evaluate binary classifiers when the focus is on accurately predicting the two discrete classes.
We also describe a trade-off between two important measures and illustrate how different thresholds to cut-off predicted probabilities into discrete classes affect these measures.
In @sec-roc-space, we then put special emphasis on a method to visually compare the performance of classifiers.

### Confusion Matrix

For binary classifiers that predict discrete classes, we can compute a `r index("confusion matrix")` which summarizes the following quantities in a two-dimensional contingency table (see also @fig-confusion):

* **True positives (TP)**: Positive instances that are correctly classified as positive.
* **True negatives (TN)**: Negative instances that are correctly classified as negative.
* **False positives (FP)**: Negative instances that are incorrectly classified as positive.
* **False negatives (FN)**: Positive instances that are incorrectly classified as negative.

Different applications may have a particular interest in one (or multiple) of the aforementioned quantities.
Consider spam classification where a model classifies a mail as spam (positive class) or no spam (negative class).
<!-- or diagnostic tests in medicine where we are more interested either in true or false positives rather then just the average proportion of correct classifications. -->
Here, we might accept some FN as long as we have a high number of TN.
This means that some spam mails not classified as such are fine (they can be deleted manually) as long as most of the important mails are correctly identified as no spam.
On the other hand, consider an application at the airport security checkpoint where the aim is to determine whether a travel bag contains a weapon (positive class) or no weapon (negative class).
A classifier used to detect weapons should be able to achieve very high number of TP (as FN are not acceptable), even if this comes at the expense of more FP --- a false alarm once in a while, assuming a weapon is in the travel bag when it is not, is totally fine, as the security staff can manually check the travel bag.
In contrast, overlooking a weapon in a travel bag can have serious consequences.
<!-- On the other hand, a classifier used to detect weapons at an airport security checkpoint (where identifying a weapon is the positive class and no weapon the negative class) should be able to achieve very high number of TP (as FN are not acceptable), even if this comes at the expense of more FP (a false alarm once in a while assuming there is a weapon although this is not the case is totally fine). -->
Similar holds for medical diagnostic tests to identify, e.g., an infectious disease (positive class), where a high number of TP is usually more important than a low number of FP.
Although FP are in general undesirable, it might be less severe to consider a healthy patient infectious than to label an unhealthy patient non-infectious and send him home, which could infect other people.
The importance of a low number of FP increases if a medical diagnostic test is used to initiate medical treatment, as it may be expensive or invasive for a patient.
<!-- For example,  (healthy patients receive a positive medical test indicating a serious disease) -->
<!-- (a medical diagnostic test can be repeated before a patient receives an invasive treatment)  -->
<!-- However, if we received millions of mails per day, we might have a stronger preference towards a low number of false negatives as otherwise our inbox would still see too much spam. -->
<!-- More careful considerations need to be made for medical diagnostic tests, where a positive test for a medical condition can lead to expensive or invasive treatments, which we want to avoid if not truly necessary. -->
<!-- Likewise, if a medical treatment is required, we do not want to falsely assume otherwise based on a test result. -->

<!-- In Chapter @sec-basics, we have already seen how we can obtain the confusion matrix of a `r ref("Prediction")` by accessing the `$confusion` field. -->
In the code example below, we first retrieve the `r ref("mlr_tasks_german_credit")` task for binary classification and construct a random forest learner using `classif.ranger` that predicts probabilities using the `predict_type = "prob"` option.
Next, we use the `r ref("partition()")` helper function we used earlier as a shortcut to a simple holdout split.
We train the learner on the training set and use the trained model to generate predictions on the test set.
Finally, we retrieve the confusion matrix from the resulting `r ref("Prediction")` object by accessing the `$confusion` field (see also @sec-classif-eval):

```{r performance-050}
task = tsk("german_credit")
learner = lrn("classif.ranger", predict_type = "prob")
splits = partition(task, ratio = 0.8)

learner$train(task, splits$train)
pred = learner$predict(task, splits$test)
pred$confusion
```

<!-- These examples illustrate that there is a trade-off between different aspects of classification performance and different types of errors that can occur.  -->
The above exemplary applications illustrate that it depends on the application which quantities from the confusion matrix are important and which may be less important.
<!-- A simple measure such as the classification accuracy does not reflect all of these quantities. -->
While the classification accuracy only takes into account the TP and TN to calculate the average proportion of correctly classified instances, the confusion matrix provides a more holistic picture of the classifier's performance.
However, the absolute numbers (TP, TN, FP, FN) shown in a confusion matrix can be less useful when the classes are imbalanced.
Also the classification accuracy is usually a bad choice in highly imbalanced cases, as it considers only the overall proportion of correctly classified instances without taking into account the total number of positive and negative instances.
<!-- the relative proportions of correctly and incorrectly classified instances within the positive or negative class. -->
<!-- the relative proportions of true and false positives or negatives. -->
For example, consider a case where the occurrence of a disease in the population is 1%.
A model that always predicts "no disease" for every patient would have a classification accuracy of 99%, indicating a well-performing model.
However, the model is useless as it does not identify diseased patients.
<!-- Different fields have therefore derived various  -->
It is therefore useful to also consider other measures from the confusion matrix, as different applications tend to have different notions of what qualities a "good" classification model should exhibit.

### Confusion Matrix-based Measures

Many measures derived from the confusion matrix have different names for historical reasons, originating from different fields.
A good overview of common confusion matrix-based measures can be obtained from the comprehensive table on `r link("https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion", "Wikipedia")` which also provides many common aliases for each measure.
<!-- Some common performance measures that are based on the confusion matrix and  -->
Below, we introduce some of these measures which mainly quantify the discrimination performance, i.e., the ability of a classifier to separate the two classes (see also @fig-confusion for their definition based on TP, FP, TN and FN):

* **True Positive Rate (TPR)**, **Sensitivity** or **Recall**: How many of the true positives did we predict as positive?
* **True Negative Rate (TNR)** or **Specificity**: How many of the true negatives did we predict as negative?
* **False Positive Rate (FPR)**, or 1 - **Specificity**: How many of the true negatives did we predict as positive?
* **Positive Predictive Value (PPV)** or **Precision**: If we predict positive how likely is it a true positive?
* **Negative Predictive Value (NPV)**: If we predict negative how likely is it a true negative?
* **Accuracy (ACC)**: The proportion of correctly classified instances out of the total number of instances.
* **F1-score**: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as $2 \times \frac{Precision \times Recall}{Precision + Recall}$.

<!-- While the classification accuracy only takes into account the TP and TN to calculate the average proportion of correctly classified instances, the confusion matrix additionally shows FP and FN and provides a more holistic picture of the classifier's performance. -->
<!-- Note that TP and FN sum up to the total number of positive instances in the data, while FP and TN sum up to the total number of negative instances in the data. -->
<!-- These measures are motivated by the need to evaluate binary classifiers in different applications by more than accuracy alone, as we have previously done. -->

```{r performance-049}
#| echo: false
#| label: fig-confusion
#| fig-cap: "Binary confusion matrix of ground truth class vs. predicted class."
#| fig-align: "center"
#| fig-alt: "Binary confusion matrix of ground truth class vs. predicted class."
knitr::include_graphics("Figures/confusion_matrix.svg")
```

The `r ref_pkg("mlr3measures")` package allows to compute several common confusion matrix-based measures using the `r ref("mlr3measures::confusion_matrix", "confusion_matrix")` function:

```{r performance-051}
mlr3measures::confusion_matrix(truth = pred$truth,
  response = pred$response, positive = task$positive)
```

If a binary classifier predicts probabilities instead of discrete classes, we could arbitrarily set a threshold to cut-off the probabilities and assign them to the positive and negative class (see @sec-thresholding).
When it comes to classification performance, it is generally difficult to achieve a high TPR and low FPR simultaneously because there is often a trade-off between the two rates.
Increasing the threshold for identifying the positive cases, leads to a higher number of negative predictions and fewer positive predictions.
As a consequence, the FPR is usually better (lower), but at the cost of a worse (lower) TPR.
For example, in the special case where the threshold is set too high and no instance is predicted as positive, the confusion matrix shows zero true positives (no instances that are actually positive and correctly classified as positive) and zero false positives (no instances that are actually negative but incorrectly classified as positive).
Therefore, the FPR and TPR are also zero since there are zero false positives and zero true positives.
<!-- However, the TPR is also zero since there are no true positives, which means that the model fails to identify any positive cases, even if there are positive cases in the dataset.  -->
<!-- Consider a binary classifier that predicts whether an email is spam (positive class) or not (negative class). -->
<!-- Increasing the threshold for identifying positive cases means that the model will require more evidence (i.e., a higher predicted probability) before classifying an email as spam.  -->
<!-- This can result in fewer emails being classified as spam (fewer positive predictions) and more emails being classified as not spam (more negative predictions). -->
<!-- This can increase the model's specificity (the proportion of true negatives among all negative predictions) which leads to a better (lower) FPR. -->
<!-- However, it may also reduce the model's sensitivity and TPR (i.e., the proportion of true positives among all positive predictions). -->
Conversely, lowering the threshold for identifying positive cases may never predict the negative class and can increase (improve) TPR, but at the cost of a worse (higher) FPR.
For example, below we set the threshold to `0.99` and `0.01` for the `r ref("mlr_tasks_german_credit")` task to illustrate the two special cases explained above where zero positives and where zero negatives are predicted and inspect the resulting confusion matrix-based measures (some measures can not be computed due to division by 0 and therefore will produce `NaN` values):

```{r performance-052}
pred$set_threshold(0.99)
mlr3measures::confusion_matrix(pred$truth, pred$response, task$positive)
pred$set_threshold(0.01)
mlr3measures::confusion_matrix(pred$truth, pred$response, task$positive)
```

### ROC Analysis {#sec-roc-space}

`r index("ROC")` (Receiver Operating Characteristic) analysis is widely used to evaluate binary classifiers.
Although extensions for multiclass classifiers exist (see e.g., @hand2001simple), we will only cover the much easier binary classification case here.
ROC analysis aims at evaluating the performance of classifiers by visualizing the aforementioned trade-off between the TPR and the FPR which can be obtained from a confusion matrix.

We first consider the simple case of hard classifiers that only predict discrete classes.
Each classifier that predicts discrete classes, will be a single point in the ROC space (see @fig-roc, panel (a)).
The best classifier lies on the top-left corner where the TPR is 1 and the FPR is 0.
Classifiers on the diagonal predict class labels randomly (possibly with different class proportions).
For example, if each positive instance will be randomly classified with 25% as to the positive class, we get a TPR of 0.25.
If we assign each negative instance randomly to the positive class, we get a FPR of 0.25.
In practice, we should never obtain a classifier clearly below the diagonal.
Swapping the predicted classes of a classifier would results in points in the ROC space being mirrored at the diagonal baseline.
A point in the ROC space below the diagonal might indicate that the positive and negative class labels have been switched by the classifier.

We now consider the case of soft classifiers that predict probabilities instead of discrete classes.
Using different thresholds to cut-off predicted probabilities and assign them to the positive and negative class may lead to different confusion matrices.
In this case, we can characterize the behavior of a binary classifier for different thresholds by plotting the TPR and FPR values --- this is the ROC curve.
For example, we can use the previous `r ref("Prediction")` object, compute all possible TPR and FPR combinations if we use all predicted probabilities as possible threshold, and visualize them to manually create a ROC curve:

```{r performance-053}
#| fig-cap: "Manually constructed ROC-curve based on the `german_credit` dataset and the `classif.ranger` Random Forest learner."
thresholds = sort(pred$prob[,1])

rocvals = data.table::rbindlist(lapply(thresholds, function(t) {
  pred$set_threshold(t)
  data.frame(
    threshold = t,
    FPR = pred$score(msr("classif.fpr")),
    TPR = pred$score(msr("classif.tpr"))
  )
}))

head(rocvals)

library(ggplot2)
ggplot(rocvals, aes(FPR, TPR)) +
  geom_point() +
  geom_path(color = "darkred") +
  geom_abline(linetype = "dashed") +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(
    title = NULL,
    x = "1 - Specificity (FPR)",
    y = "Sensitivity (TPR)"
  ) +
  theme_bw()
```

A natural performance measure that can be derived from the ROC curve is the area under the curve (AUC).
The higher the AUC value, the better the performance, whereas a random classifier would result in an AUC of 0.5  (see @fig-roc, panel (b) for an illustration).
The AUC can be interpreted as the probability that a randomly chosen positive instance is ranked higher (in the sense that it gets a higher predicted probability of belonging to the positive class) by the classification model than a randomly chosen negative instance.

::: {.callout-tip}
For multiclass classification tasks, generalizations of the AUC measure are also available.
See e.g. `"classif.mauc_au1p"`.
:::

```{r performance-054, echo = FALSE}
#| label: fig-roc
#| fig-cap: "Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better as both lie on a parallel line to the baseline. C2 is clearly dominated by C1, C3 as it is further away from the best classifier at (TPR = 1, FPR = 0). Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2."
#| fig-align: "center"
#| fig.height: 3.5
#| fig.width: 8
#| fig-alt: "Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better as both lie on a parallel line to the baseline. C2 is clearly dominated by C1, C3 as it is further away from the best classifier at (TPR = 1, FPR = 0). Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2."
#library(gridExtra)
library(ggplot2)
# devtools::install_github("thomasp85/patchwork")
library(patchwork)

set.seed(123)
fun = function(x, lambda) 1 - exp(-lambda*x) #ecdf(rexp(1000000, rate = 5))
funinv = function(x, lambda) 1 + log(x)/lambda
x = c(seq(2e-5, 1, length = 1000))
lambda1 =  -1*log(1 - 0.75)/0.125
lambda2 =  -1*log(1 - 0.625)/0.25
#lambda3 =  -1*log(1 - 0.875)/0.25
d1 = data.frame(x = x, y = fun(x, lambda = lambda1))
d2 = data.frame(x = x, y = fun(x, lambda = lambda2))
d3 = data.frame(x = x, y = funinv(x, lambda = lambda1))#fun(x, lambda = lambda3))

# mean(d1$y)
# mean(d2$y)
# mean(d3$y)

rd = data.frame(x = c(0, 1), y = c(0, 1))
classif = data.frame(x = c(0, 1, 1, 0, 0.125, 0.25, 0.25), y = c(1, 0, 1, 0, 0.75, 0.625, 0.875),
  classifier = c("best", "worst", "random", "random", "C1", "C2", "C3"))
classif = droplevels(classif[-2, ])

p = ggplot(rd, aes(x = x, y = y)) +
  # geom_area(mapping = aes(x = x, y = y), fill = "red", alpha = 0.5) +
  coord_fixed(ratio = 1) +
  ylab(expression(TPR)) + xlab(expression(FPR)) +
  theme_bw()

p1 = p +
  geom_line(colour = 2, lty = 2, linewidth = 0.75) +
  geom_text(aes(x = 0.5, y = 0.5, hjust = 0.5, vjust = -0.5, label = "baseline (random classifiers)"), colour = 2, size = 3, angle = 45) +
  geom_point(data = classif, aes(x = x, y = y, colour = classifier, shape = classifier), size = 3) +
  geom_text(data = classif[classif$classifier == "random",],
    aes(x = x, y = y, hjust = c(1.1, -0.1), vjust = c(0.5, 0.5)),
    label = c("always predict positive class", "never predict positive class"),
    colour = 2, size = 3) +
  geom_text(data = classif[grepl("^C", classif$classifier), ],
    aes(x = x, y = y, hjust = c(0.5, 0.5, 0.5), vjust = c(-1, -1, -1)),
    label = c("C1", "C2", "C3"),
    colour = c("C1" = "black", "C2" = "black", "C3" = "black"), #c("C1" = "gray70", "C2" = "gray50", "C3" = "gray30"),
    size = 3) +
  ggtitle("(a)") +
  scale_color_manual("classifier",
    values = c("best" = 3, "random" = 2,
      "C1" = "black", "C2" = "black",  "C3" = "black"
        ))

dall = rbind(
  cbind(d1, AUC = round(mean(d1$y), 2), classifier = "C1"),
  cbind(d2, AUC = round(mean(d2$y), 2), classifier = "C2"),
  cbind(d3, AUC = round(mean(d3$y), 2), classifier = "C3"),
  cbind(classif[c(3, 1, 2), 1:2], AUC = 1, classifier = "best"),
  cbind(rd, AUC = 0.5, classifier = "random")
)
dall$AUC = factor(dall$classifier, levels = c("best", "random", "C1", "C2", "C3"))
#dall$AUC = factor(dall$AUC, levels = sort(unique(dall$AUC), decreasing = TRUE))

lab = c("best \n(AUC = 1)", "random \n(AUC = 0.5)", "C1 (AUC = 0.9)", "C2 (AUC = 0.75)", "C3 (AUC = 0.9)")

p2 = p +
  geom_text(aes(x = 0.5, y = 0.5, hjust = 0.5, vjust = -0.5, label = "baseline"), colour = 2, size = 3, angle = 45) +
  geom_line(data = dall, aes(x = x, y = y, lty = AUC, col = AUC), linewidth = 0.75) + ggtitle("(b)") +
  geom_point(data = classif[grepl("^C", classif$classifier), ], aes(x = x, y = y, shape = classifier), size = 3) +
  geom_text(data = classif[grepl("^C", classif$classifier), ],
    aes(x = x, y = y, hjust = c(0.5, 0.5, 0.5), vjust = c(-1, -1, -1)),
    label = c("C1", "C2", "C3"),
    colour = c("C1" = "black", "C2" = "black", "C3" = "black"),
    size = 3) +
  ylim(c(0, 1)) +
  guides(shape = "none") +
  scale_color_manual("ROC curve",
    values = c(
      "best" = 3,
      "random" = 2,
      "C1" = "gray70", "C2" = "gray70", "C3" = "gray70"),
    labels = lab) +
  scale_linetype_manual("ROC curve",
    values = c(
      "best" = 3,
      "random" = 2,
      "C1" = 3, "C2" = 4, "C3" = 5),
    labels = lab) +
  NULL

#ggarrange(p1, p2, nrow = 1, ncol = 2)
# p1 + geom_function(fun = function(x) fun(x, lambda = lambda1), mapping = aes(col = "0.91")) +
#   geom_function(fun = function(x) fun(x, lambda = lambda2)) +
#   geom_function(fun = function(x) funinv(x, lambda = lambda1))

p1 + p2 & theme(plot.margin = grid::unit(c(0, 0, 0, 0), "mm"))
#p1 + p2 & theme(legend.position = "bottom")

```

For `r mlr3` prediction objects, the ROC curve can be constructed with the previously seen `r ref("autoplot.PredictionClassif")` from `r mlr3viz`.
The x-axis showing the FPR is labelled "1 - Specificity" by convention, whereas the y-axis shows "Sensitivity" for the TPR.

```{r performance-055}
#| fig-cap: "ROC-curve based on the `german_credit` dataset and the `classif.ranger` Random Forest learner."
autoplot(pred, type = "roc")
```

We can also plot the precision-recall (PR) curve which visualize the PPV vs. TPR.
The main difference between ROC curves and PR curves is that the number of true-negatives are not used to produce a PR curve.
PR curves are preferred over ROC curves for imbalanced populations.
This is because the positive class is usually rare in imbalanced classification tasks.
Hence, the FPR is often low even for a random classifier.
As a result, the ROC curve may not provide a good assessment of the classifier's performance, because it does not capture the high rate of false negatives (i.e., misclassified positive observations).
See also @davis2006relationship for a detailed discussion about the relationship between the PRC and ROC curves.

```{r performance-056}
#| fig-cap: "Precision-Recall curve based on the `german_credit` dataset and the `classif.ranger` Random Forest learner."
autoplot(pred, type = "prc")
```

Another useful way to think about the performance of a classifier is to visualize the relationship of the set threshold with the performance metric at the given threshold.
For example, if we want to see the FPR and accuracy across all possible thresholds:

```{r performance-057}
#| layout-ncol: 2
#| fig-subcap:
#|   - "Threshold vs. FPR plot on `german_credit` with `classif.ranger`."
#|   - "Threshold vs. accuracy plot on `german_credit` with `classif.ranger`."
autoplot(pred, type = "threshold", measure = msr("classif.fpr"))
autoplot(pred, type = "threshold", measure = msr("classif.acc"))
```

This visualization would show us that changing the threshold from the default 0.5 to a higher value like 0.7 would greatly reduce the FPR, while reducing accuracy by only a few percentage points.
Depending on the problem at hand, this might be perfectly desirable trade-off.

These visualizations are also available for `r ref("ResampleResult")`.
Here, the predictions of individual resampling iterations are merged prior to calculating a ROC or PR curve (micro averaged):

```{r performance-058}
#| layout-ncol: 2
#| fig-subcap:
#|   - "ROC-curve across resampling iterations."
#|   - "Precision-Recall curve across resampling iterations."
rr = resample(
  task = tsk("german_credit"),
  learner = lrn("classif.ranger", predict_type = "prob"),
  resampling = rsmp("cv", folds = 5)
)
autoplot(rr, type = "roc")
autoplot(rr, type = "prc")
```

We can also visualize a `r ref("BenchmarkResult")` to compare multiple learners on the same `r ref("Task")`:

```{r performance-059}
#| layout-ncol: 2
#| fig-subcap:
#|   - "ROC-curve comparing two learners."
#|   - "Precision-Recall curve comparing two learners."
design = benchmark_grid(
  tasks = tsk("german_credit"),
  learners = lrns(c("classif.rpart", "classif.ranger"), predict_type = "prob"),
  resamplings = rsmp("cv", folds = 5)
)
bmr = benchmark(design)
autoplot(bmr, type = "roc")
autoplot(bmr, type = "prc")
```

## Conclusion

In this chapter, we learned how to estimate the generalization performance of a model via resampling.
We also learned about benchmarking to fairly compare the estimated generalization performance of different learners across multiple tasks.
Performance calculations underpin these concepts, and we have seen some of them applied to classification tasks, with a more in-depth look at the special case of binary classification and ROC analysis.
We also learned how to visualize confusion matrix-based performance measures with regards to different thresholds as well as resampling and benchmark results with `r mlr3viz`.
<!-- that cut-off the predicted probabilities and assign the predictions to the positive and negative class  -->
The discussed topics belong to the fundamental concepts of supervised machine learning.
@sec-optimization builds on these concepts and applies them for tuning (i.e., to automatically choose the optimal hyperparameters of a learner) through nested resampling (@sec-nested-resampling).
In @sec-special, we will also take a look at specialized tasks that require different resampling strategies.
Finally, @tbl-api-performance provides an overview of some important `r mlr3` functions and the corresponding R6 classes that were most frequently used throughout this chapter.


| S3 function                 | R6 Class              | Summary                                      |
| --------------------------- | --------------------- | -------------------------------------------- |
| `r ref("rsmp()")`           | `r ref("Resampling")` | Assigns observations to train- and test sets |
| `r ref("resample()")`       | `r ref("ResampleResult")` | Evaluates learners on given tasks using a resampling strategy |
| `r ref("benchmark_grid()")` | -                     | Constructs a design grid of learners, tasks, and resamplings  |
| `r ref("benchmark()")`      | `r ref("BenchmarkResult")` | Evaluates learners on a given design grid |

:Core S3 'sugar' functions for resampling and benchmarking in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-performance}

<!-- No gallery posts according to BB
### Resources {.unnumbered .unlisted}

- Learn more about advanced resampling techniques in: `r link("https://mlr-org.com/gallery/basic/2020-03-30-stratification-blocking/", "Resampling - Stratified, Blocked and Predefined")`.
- Check out the blog post `r link("https://mlr-org.com/gallery/basic/2020-03-18-iris-mlr3-basics/", "mlr3 Basics on “Iris” - Hello World!")` to see minimal examples on using  resampling and benchmarking on the iris dataset.
- Use resampling and benchmarking for the `r link("https://mlr-org.com/gallery/basic/2020-08-14-comparison-of-decision-boundaries/", "comparison of decision boundaries of classification learners")`.
- Learn how to effectively pick thresholds by applying tuning and pipelines (Chapters [-@sec-optimization] and [-@sec-pipelines]) in `r link("https://mlr-org.com/gallery/optimization/2020-10-14-threshold-tuning/index.html", "this post on threshold tuning")`.
-->

## Exercises

1. Apply the "bootstrap" resampling strategy on the `mtcars` task and evaluate the performance of the `classif.rpart` decision tree learner.
Use 100 replicates and an a sampling ratio of 80%.
Calculate the MSE for each iteration and visualize the result.
Finally, calculate the aggregated performance score.

2. Use the `spam` task and 5-fold cross-validation to benchmark Random Forest (`classif.ranger`), Logistic Regression (`classif.log_reg`), and XGBoost (`classif.xgboost`) with regards to AUC.
Which learner appears to do best? How confident are you in your conclusion?
How would you improve upon this?

3. A colleague claims to have achieved a 93.1% classification accuracy using the `classif.rpart` learner on the `penguins_simple` task.
You want to reproduce their results and ask them about their resampling strategy.
They said they used a custom 3-fold cross-validation with folds assigned as `factor(task$row_ids %% 3)`.
See if you can reproduce their results.
