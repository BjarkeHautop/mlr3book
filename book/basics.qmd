---
author:
  - name: Natalie Foss
    orcid:
    email: nfoss2@uwyo.edu
    affiliations:
      - name: University of Wyoming
  - name: Lars Kotthoff
    orcid: 0000-0003-4635-6873
    email: larsko@uwyo.edu
    affiliations:
      - name: University of Wyoming
abstract:
    We describe and explain the basic building blocks of mlr3 and how to train and evaluate simple machine learning models.
    The chapter introduces the different types of tasks that mlr3 supports and how to work with them, learners and how to train models, how to set parameters, how to make predictions using trained models, and how to evaluate the quality of the predictions in a principled fashion.
    Only the basic concepts are introduced, but we give pointers on where to learn more in the rest of the book, and overviews of other concepts.
    After reading this chapter, you will be able to use mlr3 for most machine learning workflows.
---

# Fundamentals of mlr3 {#sec-basics}

{{< include _setup.qmd >}}

In this chapter, we will introduce the `r mlr3` objects and corresponding `r ref_pkg("R6")` classes that implement the essential building blocks of `r index("machine learning")`.
These building blocks include the data (and the methods of creating training and test sets), the machine learning `r index('algorithm')` (and its training and prediction process), the configuration of a machine learning algorithm through its `r index('hyperparameters')`, and evaluation measures to assess the quality of predictions.

In the simplest definition, `r define('machine learning')` is the process of using computer models to learn relationships from data.
`r define('Supervised learning')` is a subfield of machine learning in which datasets consist of observations (rows in tabular data) that are labeled, which means that each data point includes `r index('features')` (columns in tabular data) and a quantity that we are trying to predict, also called a `r index('target')`.
A classic example might be trying to predict a car's miles per gallon (the target) based on properties (the features) such as horsepower and the number of gears (we will return to this particular example later).
In `r mlr3`, we refer to datasets, and their associated metadata as `r index('tasks')` (@sec-tasks).
The term 'tasks' is used to refer to the machine learning task (i.e., mathematical problem) that we are trying to solve.
Tasks are defined by the features used for prediction and the targets to predict, so there can be multiple tasks associated with any given dataset.
For example, predicting miles per gallon (mpg) from horsepower is one task, predicting horsepower from miles per gallon is another task, and predicting number of gears from model is yet another task, and so on.

Supervised learning can be further divided into `r define('regression')` -- which is prediction of numeric target values, e.g. predicting a car's mpg -- and `r define('classification')` -- which is prediction of categorical values/labels, e.g., predicting a car's model.
Other tasks are also encompassed by supervised learning, and these are returned to in @sec-special - we will also consider `r index('unsupervised learning')`  tasks in that chapter.
For any supervised learning task, the goal is to build a model that captures the relationship between features and target and often to `r index('train')` a model to be able to make predictions for new and previously unseen data.
Such models are induced by passing `r index('training data')` to `r index('machine learning algorithms')`, including `r index('decision trees')`, `r index('support vector machines')`, `r index('neural networks')`, and many more.
Machine learning algorithms are called `r define('learners')` in `r mlr3` (@sec-learners) as given data, they learn models.
Each learner has a parameterized space that potential models are drawn from and during the training process, these parameters are fitted to best match the data.
For example, the parameters could be the weights given to individual features when training a linear regression model.
During training, all machine learning algorithms are `r define('fitted')`/`r define('trained')` by optimizing a loss-function that quantifies the mismatch between ground truth target values in the training data and the predictions of the model.
Learners are distinct from algorithms as they also contain information about hyperparameters, which are user-controlled or tuned (@sec-optimization) parameters that are not learnt during training but instead affect *how* the algorithm is trained.

For a model to be most useful, it should generalize beyond the training data to make 'good' predictions (@sec-predicting) on new and previously 'unseen' (by the model) data.
The simplest way to determine if a model will generalize to make 'good' predictions for new data, is to split data into `r define('training data')` and `r define('test data')` -- where the model is trained on the training data and then the separate test data is used to evaluate models in an unbiased way by assessing to what extent the model has learned the true relationships that underlie the data (@sec-performance).
This evaluation procedure estimates a model's `r define('generalization error')`, i.e., how well we expect the model to perform in general.
There are many ways to evaluate models (@sec-eval) and to split data for estimating generalization error (@sec-resampling).

This whistlestop tour of machine learning provides the basic knowledge required to use software in `mlr3` and is summarized in @fig-ml-abstraction.
In the rest of this book we will also provide introductions to methodology when relevant and in @sec-special we will also provide introduction to applications in other tasks.
However for detailed texts about machine learning, including different algorithms, we recommend @hastie2001, @james_introduction_2014, and @bishop_2006.

In the next few sections we will look at the building blocks of `r mlr3` using regression as an example, we will then consider how to extend this to classification in @sec-classif, for other tasks see @sec-special.

```{r basics-fig-1, echo=FALSE}
#| label: fig-ml-abstraction
#| fig-cap: "General overview of the machine learning process."
#| fig-align: "center"
#| fig-alt: "A flowchart starting with the task (data), which splits into training- and test sets. The training set is used with the learner to fit a model, which is then used with the test set to make predictions. A performance measure is applied to the predictions and results in a performance estimate. Resampling refers to the repeated application of this process."
knitr::include_graphics("Figures/ml_abstraction.svg")
```


## Tasks {#sec-tasks}

`r index('Tasks')` are objects that contain the (usually tabular) data and additional metadata that define a machine learning problem.
The `r index('metadata')` contain, for example, the name of the target feature for supervised machine learning problems.
This information is used automatically by operations that can be performed on a task so that for example the user does not have to specify the prediction target every time a model is trained.

### Constructing Tasks {#sec-tasks-built-in}

`r mlr3` includes a few predefined machine learning tasks in the `r ref("mlr_tasks")` `Dictionary`.

```{r basics-001}
mlr_tasks
```

To get a task from the dictionary, use the `r ref("tsk()")`\index{tsk()} function and assign the return value to a new variable.
Below we retrieve the task `r ref("mlr_tasks_mtcars")`, which uses the `r ref("datasets::mtcars")` dataset:

```{r basics-002}
task_mtcars = tsk("mtcars")
task_mtcars
```

:::{.callout-tip}
`task_mtcars` is a variable storing an object that inherits from class `mlr_tasks_mtcars`, so if you want to access the help page you need to use `task_mtcars$help()`.
However, for the underlying class you can use more standard help methods: `?mlr_tasks_mtcars`, or `task_mtcars$help()`.
:::

To create your own regression task, you will need to construct a new instance of the `r ref("TaskRegr")`.
The simplest way to do this is with the function `r ref("as_task_regr()")` to convert a `data.frame` type object to a regression task, specifying the target feature by passing this to the `target` argument.
By example, we will imagine that `mtcars` was not already available as a predefined task in `r mlr3`.
In the code below we load the `datasets::mtcars` dataset, print its properties, subset the data to only include columns `"mpg"`, `"cyl"`, `"disp"`, print the modified data's properties, and then setup a regression task called `"cars"` (`id = "cars"`) in which we will try to predict miles per gallon (`target = "mpg"`) from number of cylinders (`"cyl"`) and displacement (`"disp"`):

```{r basics-006}
library(mlr3)
data("mtcars", package = "datasets")
mtcars_subset = subset(mtcars, select = c("mpg", "cyl", "disp"))
str(mtcars_subset)
task_mtcars = as_task_regr(mtcars_subset, target = "mpg", id = "cars")
```

The data can be in any tabular format, e.g. a `data.frame()`, `data.table()`, or `tibble()`.
The `target` argument specifies the prediction target column.
The `id` argument is optional and specifies an identifier for the task that is used in plots and summaries; if omitted the variable name of the data will be used as the `id`.

:::{.callout-tip}
As many ML models do not work properly with arbitrary `r link("https://en.wikipedia.org/wiki/UTF-8", "UTF8 names")`, `r mlr3` defaults to throw an error if any of the column names passed to `r ref("as_task_regr()")` (and other task constructors) contain a non-ASCII character or do not comply with R's variable naming scheme.
Therefore, we recommend converting names with `r ref("make.names()")` if possible, but if not then you can bypass this check in` r mlr3` by setting `options(mlr3.allow_utf8_names = TRUE)` (but do not be surprised if an underlying package implementation throws up a related error).
:::

Printing a task provides a short summary, in this case we can see the task has `r task_mtcars$nrow` observations and `r task_mtcars$ncol` columns (32 x 3), of which `mpg` is the target, there are no special properties, and there are `r length(task_mtcars$feature_names)` features stored in double-precision floating point format.

```{r}
task_mtcars
```

We can plot the task using the `r mlr3viz` package, which gives a graphical summary of the distribution of the target and feature values:

```{r basics-008, message=FALSE}
#| fig-cap: "Overview of the mtcars dataset."
#| fig-alt: Diagram shows six plots, three are line plots showing the relationship between continuous variables, and three are scatter plots showing relationships between other variables.
library("mlr3viz")
autoplot(task_mtcars, type = "pairs")
```

### Retrieving Data {#sec-retrieve-data}

We have looked at how to create tasks to store data and metadata, now we will look at how to retrieve the stored data.

Various fields can be used to retrieve metadata about a task. The dimensions, for example, can be retrieved using `$nrow` and `$ncol`:

```{r basics-009}
c(task_mtcars$nrow, task_mtcars$ncol)
```

The names of the feature and target columns are stored in the `$feature_names` and `$target_names` slots, respectively.

```{r basics-010}
c(Features = task_mtcars$feature_names, Target = task_mtcars$target_names)
```

While the columns of a task have unique `character`-valued names, their rows are identified by unique natural numbers, called row IDs.
They can be accessed through the `$row_ids` field:

```{r basics-011}
head(task_mtcars$row_ids)
```

Row IDs are not used as features when training or predicting but are metadata that allows to access individual observations.
Note that Row IDs are not the same as row numbers.
This is best demonstrated by example, below we create a regression task from random data, print the original row ids, which correspond to row numbers 1-5, then we filter three rows (we will return to this method just below) and print the new row IDs, which no longer correspond to the row numbers.

```{r}
task = as_task_regr(data.frame(x = runif(5), y = runif(5)), target = "y")
task$row_ids
task$filter(c(4, 1, 3))
task$row_ids
```

This design decision allows tasks and learners to transparently operate on real database management systems, where uniqueness is the only requirement for primary keys (and not the actual row ID value).

The data contained in a task can be accessed through `$data()`, which returns a `r ref("data.table")` object.
This method has optional `rows` and `cols` arguments to specify subsets of the data to retrieve.

```{r basics-012}
# retrieve all data
task_mtcars$data()
# retrieve data for rows with IDs 1, 5, and 10 and feature columns
task_mtcars$data(rows = c(1, 5, 10), cols = task_mtcars$feature_names)
```

:::{.callout-tip}
You can work with row numbers instead of row IDs by adding a step to extract the corresponding row ID:

```{r basics-016, eval = FALSE}
# select the 2nd row of the task by extracting the second row_id:
task$data(rows = task$row_ids[2])
```
:::

You can always use 'standard' R methods to extract summary data from a task, for example to summarize the underlying data:

```{r basics-013}
summary(as.data.table(task_mtcars))
```

### Task Mutators {#sec-tasks-mutators}

Finally, after a task has been created, you may want to perform operations on the task such as filtering down to subsets of rows and columns, which is often useful for manually creating train and test splits or to fit models on a subset of given features.
Above we saw how to access subsets of the underlying dataset using `$data()`, however this will not change the underlying task.
Therefore, we provide `r define('mutators')`, which modify the given `Task` in place, this can be seen in examples below.

Subsetting by features (columns) is possible with `$select()` with the desired feature names passed as a character vector.
Subsetting by observations (rows) is performed with `$filter()` by passing the row IDs as a numeric vector.

```{r basics-014}
task_mtcars_small = tsk("mtcars") # initialize with the full task
task_mtcars_small$select(c("am", "carb")) # keep only these features
task_mtcars_small$filter(2:4) # keep only these rows
task_mtcars_small$data()
```

As `R6` uses reference semantics (@sec-r6), you need to use `$clone()` if you want to copy a task and then mutate it further:

```{r basics-015}
# the wrong way
task_mtcars_small = tsk("mtcars")$filter(1:2)$select("cyl")
task_mtcars_wrong = task_mtcars_small
task_mtcars_wrong$data()
task_mtcars_wrong$filter(1)
# original data affected
task_mtcars_small$data()

# the right way
task_mtcars_small = tsk("mtcars")$filter(1:2)$select("cyl")
task_mtcars_right = task_mtcars_small$clone()
task_mtcars_right$data()
task_mtcars_right$filter(1)
# original data unaffected
task_mtcars_small$data()
```

To add extra rows and columns to a task, you can use `$rbind()`\index{$rbind()} and `$cbind()`\index{$cbind()} respectively:

```{r basics-017}
task_mtcars_small$cbind( # add another column
  data.frame(disp = c(150, 160))
)
task_mtcars_small$rbind( # add another row
  data.frame(mpg = 23, cyl = 5, disp = 170)
)
task_mtcars_small$data()
```

## Learners {#sec-learners}

Objects of class `r ref("Learner")` provide a unified interface to many popular machine learning algorithms in R.
The `r ref("mlr_learners")` dictionary contains all the learners available in `mlr3`, we will discuss the available learners in @sec-lrns-add, for now we will just use a decision tree regression learner as an example to discuss the `Learner` interface.
As with tasks, you can access learners from the dictionary with a single sugar function, in this case `r ref("lrn()")` `r aside("lrn()")`.

```{r basics-023}
lrn("regr.rpart")
```

All `Learner` objects include the following metadata, which can be seen in the output above:

* `$feature_types`: the type of features the learner can handle.
* `$packages`: the packages required to be installed to use the learner.
* `$properties`: special properties the model can handle, for example the "missings" properties means a model can handle missing data, and "importance" means it can compute the relative importance of each feature.
* `$predict_types`: the types of prediction that the model can make (@sec-predicting).
* `$param_set`: the set of model hyperparameters (@sec-param-set).

To run a machine learning experiment, learners pass through two stages (@fig-basics-learner):

* `r define('Training')`: A training `Task` is passed to the learner's `$train()`  function which trains and stores a `r index('model')`, i.e., the learned relationship of the features to the target.
* `r define('Predicting')`: New data, often a different partition of the original dataset, is passed to the `$predict()` method of the trained learner to predict the target values.

```{r basics-022, echo=FALSE, fig.align="center"}
#| label: fig-basics-learner
#| fig-cap: Overview of the different stages of a learner. Top - data (split into features and a target) is passed to an (untrained learner). Bottom - new data is passed to the train learner (model) which makes predictions for the 'missing' target column.
#| fig-alt: Diagram shows two boxes, the first is labelled "$train()" and shows data being passed to a Learner. The second is labelled "$predict()" and shows "Inference Data" being passed to the "Learner" which now include a "$model", an arrow then shows predictions being made.
knitr::include_graphics("Figures/learner.svg")
```

### Training {#sec-training}

In the simplest use-case, models are trained by passing a task to a learner with the `r define("$train()", code = TRUE)` method:

```{r}
# load mtcars task
task = tsk("mtcars")
# load a regression tree
learner_rpart = lrn("regr.rpart")
# pass the task to the learner via $train()
learner_rpart$train(task)
```

After training, the fitted model is stored in the `r define("$model", code = TRUE)` field for future inspection and prediction:

```{r}
# inspect the trained model
learner_rpart$model
```

We see that the learner has identified features in the task that are predictive of the target (`mpg`) and uses them to partition observations in the tree.
The textual representation of the model depends on the type of learner.
For more information on any model see the learner help page, which can be accessed in the same way as tasks with the `help()` field, e.g., `learner_rpart$help()`.

#### Partitioning data

When performing simple examples to assess the quality of a model's predictions, you will likely want to partition your dataset to get a fair and unbiased estimate of a model's generalization error.
In the next chapter we will look at resampling and benchmark experiment which will go into more detail about unbiased estimation but for now we will just discuss the simplest method of splitting data using the `r ref("partition()", aside = TRUE)` function.
This function randomly splits the given task into two disjoint sets: a training set (67% of the total data, the default) and a test set (33% of the total data, the data not part of the training set).

```{r basics-025}
# changing default to a 70:30 train:test split
splits = partition(task_mtcars, ratio = 0.7)
splits
```

Now when training we will tell the model to only use the training data by passing the row IDs from `partition` to the `row_ids` argument of `$train()`:

```{r basics-025-1}
learner_rpart$train(task_mtcars, splits$train)
```

Now we can use our trained learner to make predictions on new data.

### Predicting {#sec-predicting}

Predicting from trained models is as simple as passing your data to the `r define("$predict()", code=TRUE)` method of the trained `Learner`.

Carrying straight on from our last example, we will call the `$predict()` method from our trained learner and again will use the `row_ids` argument, but this time to pass the IDs of our `r index("test set")`:

```{r basics-030}
predictions = learner_rpart$predict(task_mtcars, splits$test)
```

The `$predict()` method returns an object inheriting from `r ref("Prediction")`, in this case `r ref("PredictionRegr")` as this is a regression task.

```{r}
predictions
```

The `row_ids` column corresponds to the row IDs of the predicted observations.
The `truth` column contains the ground truth data, which the object extracts from the task, in this case: `task_mtcars$truth(splits$test)`.
Finally, the `response` column contains the values predicted by the model.
The `r ref("Prediction")` object can easily be converted into a `data.table` or `data.frame` using `as.data.table()`/`as.data.frame()` respectively.

All data in the above columns can be accessed directly, for example to get the first two predicted responses:

```{r basics-access-pred}
predictions$response[1:2]
```

Similarly to plotting `Task`s, `r mlr3viz` provides an `r ref("ggplot2::autoplot()", text = "autoplot()")` method for `r ref("Prediction")` objects.

```{r basics-035, message = FALSE, warning = FALSE}
#| fig-cap: "Comparing predicted and ground truth values for the mtcars dataset."
#| fig-alt: "A scatter plot of predicted values on one axis and ground truth values on the other. A trend line is fit to show that in general there is good agreement between predicted and ground truth values."
library(mlr3viz)
predictions = learner_rpart$predict(task_mtcars, splits$test)
autoplot(predictions)
```

In the examples above we made predictions by passing a task to `$predict()`, instead if you would rather pass a `data.frame` type object directly then you can use `r define("$predict_newdata()", code=TRUE)`:

```{r basics-032}
mtcars_new = data.table(cyl = c(5, 6), disp = c(100, 120),
  hp = c(100, 150), drat = c(4, 3.9), wt = c(3.8, 4.1),
  qsec = c(18, 19.5), vs = c(1, 0), am = c(1, 1),
  gear = c(6, 4), carb = c(3, 5))
predictions = learner_rpart$predict_newdata(mtcars_new)
predictions
```


#### Changing the Prediction Type {.unnumbered .unlisted}

Whilst predicting a single numeric quantity is the most common prediction type in regression, it is not the only prediction possible.
Several regression models can also predict standard errors, which are computed during training.
To predict these, the `predict_type` field of a `r ref("LearnerRegr")` must be changed from "response" (the default) to "se" before training, and most simply during construction.
The `rpart` learner we used above does not support predicting standard errors, so in the example below we will use a linear regression model implemented in `r ref("mlr3learners::LearnerRegrLm")`, note how the output now includes standard errors.

```{r basics-033}
library(mlr3learners)
learner_lm = lrn("regr.lm", predict_type = "se")
learner_lm$train(task_mtcars, splits$train)
learner_lm$predict(task_mtcars, splits$test)
```

The last element of `Learners` we will consider for now are hyperparameters, which are important for ensuring the predictions a `Learner` make are actually good (we will return to what 'good' means in @sec-eval).

### Hyperparameters {#sec-param-set}

`Learner`s encapsulate a machine learning algorithm and its `r index('hyperparameters')`, which are free parameters that can be set by the user to affect *how* the algorithm is run.
Hyperparameters may affect how a model is trained or how it makes predictions and deciding which hyperparameters to set can require expert knowledge though often there is an element of trial and error.
In this book we do not make any recommendation for which hyperparameters to set (manually or with tuning) but in @sec-tuning-spaces we briefly discuss predefined '`r index('tuning spaces')`'.
Hyperparameters are hugely important to a model performing well and therefore setting hyperparameters manually is rarely a good idea.
In practice, automated hyperparameter optimization is more common, which we will return to in @sec-optimization.
For this chapter we will refer to manual setting of hyperparameters for the sake of brevity.
We will first look at `r ref_pkg("paradox")` and `r ref("ParamSet")` objects which are used to store learner hyperparameters, and then we will look at getting and setting these values.

#### Paradox and parameter sets

We will continue our running example with a decision tree regression learner.
To access the hyperparameters in the decision tree, we use `r define("$param_set", code=TRUE)`:

```{r basics}
learner_rpart$param_set
```

The output above is a `r ref("paradox::ParamSet")` object from the package `r ref_pkg("paradox")`.
These objects provide information on hyperparameters including their name (`id`), data types (`class`), acceptable ranges for hyperparameter values (`lower`, `upper`), the number of levels possible if the data type is categorical (`nlevels`), the default value from the underlying package (`default`), and finally the set value if different from the default (`value`).
The second column references classes defined in `r ref_pkg("paradox")` that determine the class of the parameter and the possible values it can take.
@tbl-parameters-classes lists the possible hyperparameter types, all of which inherit from `r ref("paradox::Param")`.

| Hyperparameter Class | Description                          |
| :-----------------:  | :----------------------------------: |
| `r ref("ParamDbl")`  | Real-valued (Numeric) Parameters     |
| `r ref("ParamInt")`  | Integer Parameters                   |
| `r ref("ParamFct")`  | Categorical (Factor) Parameters      |
| `r ref("ParamLgl")`  | Logical / Boolean Parameters         |
| `r ref("ParamUty")`  | Untyped Parameters                   |

: Hyperparameter Classes and the type of hyperparameter they represent. {#tbl-parameters-classes}

Let's carry on the example above and consider some specific hyperparameters.
From the decision tree `ParamSet` output we can infer the following:

* `cp` must be a "double" (`ParamDbl`) taking values between 0 (`lower`) and 1 (`upper`) with a default of 0.01 (`default`).
* `keep_model` must be a "logical" (`ParamLgl`) taking values `TRUE` or `FALSE` with default `FALSE`
* `xval` must be an "integer" (`ParamInt`) taking values between 0 and `Inf` with a default of 10 and a set value of `0`.

In rare cases (we try to minimize it as much as possible), we alter hyperparameter values in construction.
When we do this the reason will always be given in the learner help page.
In the case of `regr.rpart`, we change the `xval` default to `0` because `xval` controls internal cross-validations and if a user accidentally leaves this at 10 then model training can take a long time.

With the changed hyperparameters, we have a more complex (and more reasonable) model.

#### Getting and setting hyperparameter values

Now we have looked at how parameter sets are stored, we can now think about getting and setting parameters.
Returning to our decision tree, say we are interested in growing a tree with depth 1, which means a tree where data is split once into two terminal nodes.
From the parameter set output, we know that the `maxdepth` parameter has a default of 30 and that it takes integer values.
There are a few different ways we could change this hyperparameter.
The simplest way to set a hyperparameter is in construction of the learner by simply passing the hyperparameter name and new value to `lrn()`:

```{r}
learner_rpart = lrn("regr.rpart", maxdepth = 1)
```

We can view the set of non-default hyperparameters (i.e., those changed by the user) by using `$param_set$values`:

```{r basics-027}
learner_rpart$param_set$values
```

Now we can see that `maxdepth = 1` (as we discussed above `xval = 0` is changed in construction).
This `values` field simply returns a `list` of set hyperparameters, so another way to update hyperparameters is by updating an element in the list:

```{r}
learner_rpart$param_set$values$maxdepth = 2
learner_rpart$param_set$values
```

Finally, to set multiple values at once we recommend either setting these in construction or using `$set_values`.

```{r}
learner_rpart = lrn("regr.rpart", maxdepth = 3, xval = 1)
learner_rpart$param_set$values
# or with set_values
learner_rpart$param_set$set_values(xval = 2, cp = 0.5)
learner_rpart$param_set$values
```

:::{.callout-warning}
As `learner_rpart$param_set$values` returns a `list`, some users may be tempted to set hyperparameters by passing a new `list` to `$values` -- this would work but **we do not recommend it**.
This is because passing a `list` will wipe any existing hyperparameter values if they are not included in the list.
So by example:
```{r}
rpart_params = lrn("regr.rpart")
# values at construction
rpart_params$param_set$values
# passing maxdepth the wrong way
rpart_params$param_set$values = list(maxdepth = 1)
# we have removed xval by mistake
rpart_params$param_set$values
# now with set_values
rpart_params = lrn("regr.rpart")
rpart_params$param_set$set_values(maxdepth = 1)
rpart_params$param_set$values
```
:::

All methods will ensure your new values fall within the allowed parameter range:

```{r, error=TRUE}
lrn("regr.rpart", cp = 2, maxdepth = 2)
```

#### Parameter dependencies

{{< include _optional.qmd >}}

More complex hyperparameter spaces may include parameter dependencies, which occur when setting a hyperparameter is conditional on the value of another hyperparameter.
One such example is an SVM classifier, implemented in `r ref("mlr3learners::LearnerClassifSVM")`.
The parameter set of this model has an additional column called 'parents', which tells us there are parameter dependencies in the learner.

```{r}
lrn("classif.svm")$param_set
```

To view exactly what the dependency is we can use `$deps`, this returns a `data.table` which can queried in the usual way.
So to see the dependencies of the SVM and to inspect the conditions we could do the following:

```{r}
lrn("classif.svm")$param_set$deps
lrn("classif.svm")$param_set$deps[1, cond]
lrn("classif.svm")$param_set$deps[4, cond]
```

This tells us that the parameter `cost` should only be set if the `type` parameter is set to `"C-classification"`.
Similarly, the `coef0` parameter should be set only if `"polynomial"`, `"radial"`, or `"sigmoid"`.

```{r, error=TRUE}
# errors as type is not C-classification
lrn("classif.svm", type = "eps-classification", cost = 0.5)
# works because type is C-classification
lrn("classif.svm", type = "C-classification", cost = 0.5)
```

## Evaluation {#sec-eval}

An important step of modeling is evaluating the performance of the trained model.
We have seen how to inspect the model and plot its predictions above, but a more rigorous way that allows to compare different types of models more easily is to compute a performance measure.
`r mlr3` offers many performance measures, which can be created with the `r ref("msr()")` function.
Measures are stored in the dictionary `r ref("mlr_measures")`, and a measure has to be supported by `r mlr3` to be used, just like learners.
For example, we can list all measures that are available for regression tasks:

```{r basics-036}
mlr_measures$keys("regr")
```

For example, `regr.mape` is the mean absolute percent error, `regr.rmse` is the root of the mean squared error, and `regr.sse` is the sum of squared errors.
The documentation for each measure, which contains its formula and more details, is available through the `$help()` function of the measure object.

Measure objects can be created with a single performance measure (`r ref("msr()")`) or multiple (`r ref("msrs()")`):

```{r basics-037}
measure = msr("regr.rmse")
measures = msrs(c("regr.rmse", "regr.sse"))
```

At the core of all performance measures is a quantification of the difference between the predicted value and the ground truth value (except for unsupervised tasks, see @sec-special).
This means that in order to assess performance, we usually need the ground truth data -- observations for which we do not know the true value cannot be used to assess the quality of the predictions of a model.
This is why we make predictions on the data the model did not use during training (the test set).

As we have seen above, `r mlr3`'s `r ref("Prediction")` objects contain both predictions and ground truth.
The `r ref("Measure")` objects define how prediction and ground truth are compared, and how differences between them are quantified.
We choose root mean squared error (`r ref("mlr_measures_regr.rmse", text = "regr.rmse")`) as our performance measure for this example.
Once the measure is created, we can pass it to the `$score()` method of the `r ref("Prediction")` object to quantify the predictive performance of our model.

```{r basics-038}
task = tsk("mtcars")
splits = partition(task)
predictions = lrn("regr.rpart")$train(task, splits$train)$predict(task, splits$test)
measure = msr("regr.rmse")
measure
predictions$score(measure)
```

:::{.callout-note}
`$score()` can be called without a measure; in this case the default measure for the type of task is used.
Regression defaults to mean squared error (`r ref("mlr_measures_regr.mse", text = "regr.mse")`).
:::

It is possible to calculate multiple measures at the same time by passing multiple measures to `$score()`.
For example, to compute both root mean squared error `r ref("mlr_measures_regr.rmse", text = "regr.rmse")` and mean squared error `r ref("mlr_measures_regr.mse", text = "regr.mse")`:

```{r basics-039}
measures = msrs(c("regr.rmse", "regr.mse"))
predictions$score(measures)
```

`r mlr3` also provides measures that do not quantify the quality of the predictions of a model, but other information we may be interested in, for example the time it took to train the model and make predictions:

```{r basics-measures-time}
measures = msrs(c("time_train", "time_predict"))
predictions$score(measures, learner = learner_lm)
```

Note that these measures require a trained learner in addition to the predictions.

Some measures have hyperparameters themselves, for example `r ref("mlr_measures_selected_features", text = "selected_features")`.
This measure gives information on the features the model used and is only supported by learners that have the "selected_features" property.
It requires a task and a learner in addition to the predictions.
The `lm` model does not support showing selected features; we use the `rpart` learner again and the full `mtcars` task.

```{r basics-measures-hp}
task_mtcars = tsk("mtcars")
splits = partition(task_mtcars)
learner_rpart = lrn("regr.rpart", minsplit = 10)

learner_rpart$train(task_mtcars, splits$train)
predictions = learner_rpart$predict(task_mtcars, splits$test)
measure = msr("selected_features")
predictions$score(measure, task = task_mtcars, learner = learner_rpart)
```

The hyperparameter of the measure specifies whether the number of selected features should be normalized by the total number of features.
The default is `FALSE`, giving the absolute number of features that, in this case, the trained decision tree uses.
We can change the hyperparameter in the same way as for learners, for example:

```{r basics-measures-hp-set}
measure = msr("selected_features", normalize = TRUE)
predictions$score(measure, task = task_mtcars, learner = learner_rpart)
```

We have now seen the basic building blocks of `r mlr3` -- creating and partitioning a task, instantiating a learner and setting its hyperparameters, training a model and inspecting it, making predictions, and assessing the quality of the model with a performance measure.
So far, we have focused on regression, where we want to predict a numeric quantity.
The rest of this chapter looks at other task types.
The general procedure is the same, but some details are different.


## Classification {#sec-classif}

Classification predicts a discrete, categorical target instead of the continuous numeric quantity for regression.
The models that learn to classify data are different from regression models, and regression learners are not applicable for classification problems (although for some learners, there are both regression and classification versions).
`r mlr3` distinguishes between the different tasks and learner types through different R6 classes and different prefixes -- regression learners and measures start with `regr.`, whereas classification learners and measures start with `classif.`.

### Classification Tasks {#sec-classif-tsks}

The `r ref("mlr_tasks")` dictionary that comes with `r mlr3` contains several classification tasks (`r ref("TaskClassif")`).
We can show only the classification tasks by converting the dictionary to a `data.table` and filtering on the `task_type`:

```{r basics-040}
as.data.table(mlr_tasks)[task_type == "classif"]
```

We will use the `r ref("datasets::penguins", text = "penguins")` dataset as a running example:

```{r basics-041}
task_penguins = tsk("penguins")
task_penguins
```

Just like for regression tasks, printing it gives an overview of the task, including the number of observations and features, and their types.

The target variable, `r task_penguins$target_names`, is of type factor and has the following three classes or levels:

```{r basics-042}
unique(task_penguins$data(cols = "species"))
```

Classification tasks (`r ref("TaskClassif")`) can also be plotted using `r ref("ggplot2::autoplot()", text = "autoplot()")`.
Apart from the "pairs" plot type that we show here, "target" and "duo" are available.
We refer the interested reader to the documentation of `r ref("mlr3viz::autoplot.TaskClassif")` for an explanation of the other options.
To keep the plot readable, we select only the first two features of the dataset.

```{r basics-043, warning = FALSE, message = FALSE}
#| fig-cap: "Overview of part of the penguins dataset."
#| fig-alt: Diagram showing the distribution of target and feature values for a subset of the penguins data.
library("mlr3viz")

task_penguins_small = task_penguins$clone()
# select the first feature, otherwise the individual plots are too small
task_penguins_small$select(head(task_penguins_small$feature_names, 1))
autoplot(task_penguins_small, type = "pairs")
```

### Classification Learners {#sec-classif-lrns}

Classification learners (`r ref("LearnerClassif")`) are a different R6 class than regression learners (`r ref("LearnerRegr")`), but also inherit from the base class `r ref("Learner")`.
We can instantiate a classification learner in the same way as a regression learner, by retrieving it from the `r ref("mlr_learners")` dictionary using `r ref("lrn()")`.
Note the "`classif.`" prefix to denote that we want a learner that classifies observations:

```{r basics-044}
learner_rpart = lrn("classif.rpart")
learner_rpart
```

Just like regression learners, classification learners have hyperparameters we can set to change their behavior, and printing the learner object gives some basic information about it.
Training a model and making predictions works in the same way as for regression:

```{r basics-classification-train-predict}
splits = partition(task_penguins)
learner_rpart$train(task_penguins, splits$train)
learner_rpart$model
predictions = learner_rpart$predict(task_penguins, splits$test)
predictions
```

Just like predictions of regression models, we can plot classification predictions with `r ref("ggplot2::autoplot()", text = "autoplot()")`:

```{r basics-plot-pred-classif, message = FALSE, warning = FALSE}
#| fig-cap: "Comparing predicted and ground truth values for the penguins dataset."
#| fig-alt: "A stacked bar plot of predicted values in one bar and ground truth values in the other. The number of observations for a particular class is approximately, but not quite, the same for predicted and ground truth values."
library("mlr3viz")
autoplot(predictions)
```

#### Changing the Prediction Type {#sec-classif-pred_type}

Classification problems support two types of predictions: the default "response", i.e. the class label, and "prob", which gives the probability for each class label.
Not all learners support predicting probabilities.

The prediction type for a learner can be changed by setting `$predict_type`.
After retraining the learner, all predictions have class probabilities (one for each class) in addition to the response, which is the class with the highest probability:

```{r basics-046}
learner_rpart$predict_type = "prob"
learner_rpart$train(task_penguins, splits$train)
predictions = learner_rpart$predict(task_penguins, splits$test)
predictions
```

More information on how the probabilities are used to determine the predicted label and how to change this in @sec-thresholding.

:::{.callout-tip}
@sec-lrns-add-list shows how to list learners that support the probability prediction type.
:::

### Classification Evaluation {#sec-classif-eval}

Evaluation measures for classification problems that are supported by `r mlr3` can be found in the `mlr_measures` dictionary:

```{r basics-047}
mlr_measures$keys("classif")
```

For example, `classif.auc` is the area under the receiver operator characteristic (ROC) curve (see @sec-roc), `classif.acc` is the accuracy, and `classif.logloss` is the logarithmic loss.
The documentation for each measure, which contains its formula and more details, is available through the `$help()` function of the measure object.

Some of these measures require the [predictition type](#sec-classif-pred_type) to be "prob" (e.g. `classif.auc`).
As the default is "response", using those measures requires to change the prediction type, as shown above.
You can check what prediction type a measure requires by looking at `$predict_type`.

```{r basics-048}
measure = msr("classif.acc")
measure$predict_type
```

Once we have created a classification measure, we can give it to the `$score()` method to compute its value for a given `r ref("PredictionClassif")` object:

```{r basics-classif-score}
predictions$score(measure)
```

#### Confusion Matrix

A popular way to show the quality of prediction of a classification model is a confusion matrix.
It gives a quick overview of what observations are misclassified, and how they are misclassified.
The rows in a confusion matrix are the predicted class and the columns are the true class.
All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified.
More information can be found on `r link("https://en.wikipedia.org/wiki/Confusion_matrix", "Wikipedia")`.

`r mlr3` supports confusion matrices through the `$confusion` property of the `r ref("PredictionClassif")` object:

```{r basics-049}
predictions$confusion
```

In this case, our classifier does fairly well classifying the penguins.

### Binary Classification and Positive Classes {#sec-binary-classif}

Classification problems with a two-class target are called binary classification tasks.
Binary Classification is special in the sense that one of these classes is denoted *positive* and the other one *negative*.
You can specify the *positive class* for a classification task object during task creation.
If not explicitly set during construction, the positive class defaults to the first level of the target feature.

```{r basics-050}
# during construction
data("Sonar", package = "mlbench")
task_sonar = as_task_classif(Sonar, target = "Class", positive = "R")

# switch positive class to level 'M'
task_sonar$positive = "M"
```

### Thresholding {#sec-thresholding}

Models trained on binary classification tasks that predict the probability for the positive class usually use a simple rule to determine the predicted class label -- if the probability is more than 50%, predict the positive label; otherwise, predict the negative label.
In some cases, you may want to adjust this threshold, for example, if the classes are very unbalanced (i.e., one is much more prevalent than the other).
For example, in the "german_credit" dataset, the credit risk is good for far more observations.

Training a classifier on this data overpredicts the majority class, i.e. the more prevalent class is more likely to be predicted for any given observation.

```{r basics-thresholding-1}
#| fig-cap: "Comparing predicted and ground truth values for the german_credit dataset."
#| fig-alt: "A stacked bar plot of predicted values in one bar and ground truth values in the other. The more prevalent class is predicted much more often than it is present in the ground truth data."
task_credit = tsk("german_credit")
splits = partition(task_credit)
learner = lrn("classif.rpart", predict_type = "prob")
learner$train(task_credit)
predictions = learner$predict(task_credit)
predictions$confusion
autoplot(predictions)
```

Changing the prediction threshold allows to address this without having to adjust the hyperparameters of the learner or retrain the model.

```{r basics-thresholding-2}
#| fig-cap: "Comparing predicted and ground truth values for the german_credit dataset with adjusted threshold."
#| fig-alt: "A stacked bar plot of predicted values in one bar and ground truth values in the other. The more prevalent class is now predicted less often and the agreement with the ground truth data is better."
predictions$set_threshold(0.7)
predictions$confusion
autoplot(predictions)
```

:::{.callout-tip}
Thresholds can be tuned automatically with respect to prediction performance with the `r mlr3pipelines` package using `r ref("mlr_pipeops_tunethreshold", text = "PipeOpTuneThreshold")`.
This is covered in @sec-pipelines.
:::



#### Thresholding For Multiple Classes {.unnumbered .unlisted}

For classification tasks with more than two classes you can also adjust the prediction threshold, which is 0.5 for each class by default.
Thresholds work slightly differently with multiple classes:

* The probability for a data point is divided by each class threshold resulting in `n` ratios for `n` classes.
* The highest ratio is selected (ties are random by default).

Lowering the threshold for a class means that it is more likely to be predicted and raising it has the opposite effect.
The `r ref("datasets::zoo", text = "zoo")` dataset illustrates this concept nicely.
When trained normally some classes are not predicted at all:


```{r basics-thresholding-3}
#| fig-cap: "Comparing predicted and ground truth values for the zoo dataset."
#| fig-alt: "A stacked bar plot of predicted values in one bar and ground truth values in the other. Some classes are predicted more often than in the ground truth data, some less often."
task = tsk("zoo")
learner = lrn("classif.rpart", predict_type = "prob")
learner$train(task)
preds = learner$predict(task)

autoplot(preds)
```

The classes `amphibian` and `insect` are never predicted.
On the other hand, the classes `mollusc` and `reptile` are predicted more often than they appear in the truth data.
We can address this by lowering the threshold for `amphibian` and `insect`.
`$set_threshold()` can be given a named list to set the threshold for all classes at once:

```{r basics-thresholding-4}
#| fig-cap: "Comparing predicted and ground truth values for the zoo dataset with adjusted thresholds."
#| fig-alt: "A stacked bar plot of predicted values in one bar and ground truth values in the other. The agreement between predicted and ground truth data is better with the adjusted prediction thresholds."
# c("mammal", "bird", "reptile", "fish", "amphibian", "insect", "mollusc.et.al")
new_thresh = c(0.5, 0.5, 0.5, 0.5, 0.4, 0.4, 0.5)
names(new_thresh) = task$class_names

autoplot(preds$set_threshold(new_thresh))
```

We can again see that adjusting the thresholds results in better predictive performance, without having to retrain a model.


## Column Roles {#sec-row-col-roles}

{{< include _optional.qmd >}}

We have seen that certain columns are designated as "targets" and "features" during task creation; `r mlr3` calls this "roles".
Target refers to the column(s) we want to predict and features are the predictors (also called co-variates or descriptors) for the target.
Besides these two, there are other possible roles for columns.
The roles affect the behavior of the task for different operations.

The `task_mtcars_small` task, for example, has the following column roles:

```{r basics-018}
task_mtcars_small$col_roles[c("feature", "target")]
```

There are seven column roles.
We can list all supported column roles by printing the names of the field `$col_roles`:

```{r basics-019}
# supported column roles, see ?Task
names(task_mtcars_small$col_roles)
```

* `"feature"`: Regular feature used in the model fitting process.
* `"target"`: Target variable. Most tasks only accept a single target column.
* `"name"`: Row names / observation labels. To be used in plots. Can be queried with `$row_names`. Not more than a single column can be associated with this role.
* `"order"`: Data returned by `$data()` is ordered by this column (or these columns). Columns must be sortable with `order()`.
* `"group"`: During resampling, observations with the same value of the variable with role `"group"` are marked as "belonging together". For each resampling iteration, observations of the same group will be exclusively assigned to be either in the training set or in the test set. Not more than a single column can be associated with this role.
* `"stratum"`: Stratification variables. Multiple discrete columns may have this role.
* `"weight"`: Observation weights. Not more than one numeric column may have this role.

Columns can have multiple roles.
It is also possible for a column to have no role at all, in which case they are ignored.
This is, in fact, how `$select()` and `$filter()` operate: They unassign the `"feature"` (for columns) or `"use"` (for rows) role without modifying the data which is stored in an immutable `r ref("DataBackend")`:

```{r basics-020}
task_mtcars_small$backend
```

There are two main ways to manipulate the col roles of a `Task` directly:

1. Use the `r ref("Task")` method `$set_col_roles()` (recommended).
1. Directly modify the field `$col_roles`, which is a named list of vectors of column names.
   Each vector in this list corresponds to a column role, and the column names contained in that vector have the corresponding role.

Just as `$select()`/`$filter()`, these are in-place operations, i.e. the task object itself is modified.
To retain an unmodified version of a task, use `$clone()`.

```{r basics-100}
new_task = task_mtcars_small$clone()
```


### Feature Role Example {#sec-feat-role-ex}

Changing the column or row roles, whether through `$select()`/`$filter()` or directly, does not change the underlying data, it just updates the view on it.
In a previous example we filtered the `"cyl"` column out of the task.
Because the underlying data are still there (and accessible through `$backend`), we can add the `"cyl"` column back into the task by setting its column role to `"feature"`.

```{r basics-021}
task_mtcars_small$set_col_roles("cyl", roles = "feature")
task_mtcars_small$feature_names  # cyl is now a feature again
task_mtcars_small$data()
```

### Weights Role Example {#sec-weight-role-ex}

In some cases you may wish to weigh data points (rows) differently. For example, if your classification task has severe class imbalance (where the minority class is the class you are more interested in predicting accurately), weighting the minority class rows more heavily may improve the model's performance on that class.

For this example we will work with the built-in `breast_cancer` dataset. There are many more instances of the benign tumor class than the malignant tumor class. We are interested in predicting the malignant tumor class accurately so we will weight these instances.

```{r}
task_cancer = tsk("breast_cancer")
summary(task_cancer$data()$class)
```

```{r}
cancer_data = task_cancer$data()
# adding a column where the weight is 2 when the class == "malignant", and 1 otherwise
cancer_data$weights = ifelse(cancer_data$class == "malignant", 2, 1)
task_cancer = as_task_classif(cancer_data, target = "class")
task_cancer$set_col_roles("weights", roles = "weight")
task_cancer$col_roles[c("feature", "target", "weight")]
```

## Supported Algorithms {#sec-lrns-add}

`mlr3` supports many algorithms (some through multiple implementations) as `Learner`s.
These are primarily accessed through `r mlr3`, `r mlr3learners` and `r mlr3extralearners` package, however all packages that implement new tasks (@sec-special) also include a handful of simple algorithms.

The list of learners included in `r mlr3` is deliberately small to avoid large sets of dependencies:

* Featureless learner (`regr.featureless`/`classif.featureless`), which are implemented in `r mlr3` and are baseline learners used for model comparison or as `r index('fallback learners')` (@sec-fallback). The former predicts the mean of the target values in the training set for all new observations, the latter predicts the most frequent label.
* Debug learners (`regr.debug`/`classif.debug`), which are implemented in `r mlr3` and used only to debug code (@sec-error-handling).
* Classification and regression trees (CART) (`regr.rpart`/`classif.rpart`).

The `r mlr3learners` package contains select implementations of the most popular ML methods:

* Linear (`regr.lm`) and logistic (`classif.log_reg`) regression.
* Penalized Generalized Linear Models (`regr.glmnet`/`classif.glmnet`) and with built-in optimization of the penalization parameter (`regr.cv_glmnet`/classif.cv_glmnet`).
* Weighted $k$-Nearest Neighbors regression (`regr.kknn`/`classif.kknn`).
* Kriging / Gaussian Process Regression (`regr.km`).
* Linear (`classif.lda`) and Quadratic (`classif.qda`) Discriminant Analysis.
* Naïve Bayes Classification (`classif.naive_bayes`).
* Support-Vector machines (`regr.svm`/`classif.svm`).
* Gradient Boosting (`regr.xgboost`/`classif.xgboost`).
* Random Forests for regression and classification (`regr.ranger`/`classif.ranger`).

The majority of other learners are all in `r mlr3extralearners`.
You can find an up-to-date list of learners here: `r link("https://mlr-org.com/learners.html")`.

The dictionary `r ref("mlr_learners")` contains learners that are supported in loaded packages.
You can list all learners by converting the `r ref("mlr_learners")` dictionary into a `data.table`:

```{r basics-learners-list}
as.data.table(mlr_learners)
```

The resulting `data.table` contains a lot of metadata that is useful for identifying learners with particular properties.
For example, we can list all learners that support regression problems:

```{r basics-learners-list-regr}
as.data.table(mlr_learners)[task_type == "classif"]
```

We can filter by multiple conditions, for example to list all regression learners that and can predict standard errors:

```{r basics-learners-regr-se}
as.data.table(mlr_learners)[task_type == "regr" &
    sapply(predict_types, function(x) "se" %in% x)]
```

## Exercises

1. Set the seed to `124` then train a classification tree model with `classif.rpart` and default hyperparameters on 80% of the data in the predefined `sonar` task. Evaluate the model's performance with the classification error measure on the remaining data. Also think about why we need to set the seed in this example.
2. Calculate the true positive, false positive, true negative, and false negative rates of the predictions made by the model in exercise 1.
3. Change the threshold of the model from exercise 1 such that the false positive rate is lower than the false negative rate.
What is one reason you might do this in practice?
