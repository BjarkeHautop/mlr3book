# Pipelines {#sec-pipelines}

<!--
---
author:
  - name: Martin Binder
    email: martin.binder@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
  - name: Florian Pfisterer
    orcid: 0000-0001-8867-762X
    email: florian.pfisterer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
  - name: Bernd Bischl
    orcid: 0000-0001-6002-6980
    email: berind.bischl@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract: "
  mlr3 provides a rich interface that abstracts many concepts used for training, predicting, and tuning individual machine learning algorithms.
  However, many real-world machine learning applications involve more than just fitting a single model at a time:
  It is often beneficial or even necessary to preprocess data for feature engineering and compatibility with learners.
  In many cases it is also useful to combine predictions of multiple models in ensembles.

  This chapter introduces mlr3pipelines, a dataflow programming language that can be used to define machine learning processes from simple building blocks.
  After a short demo of the possibilities offered by mlr3pipelines, we describe the individual pipeline operators and how they can be combined, at first in simple and then in more complicated graphs.
  We then show how mlr3pipelines can be used to optimize not only the hyperparameters of learners, but also the hyperparameters of preprocessing operations and even the layout of the dataflow graph itself.
  We give an overview over the most common patterns encountered in graphs and conclude with a section on advanced details and usage.
"
---
-->

{{< include _setup.qmd >}}

```{r pipelines-setup, include = FALSE, cache = FALSE}
library("mlr3pipelines")
# absolute PITA having to do this all the time:
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
lgr::get_logger("mlr3tuning")$set_threshold("warn")
lgr::get_logger("mlr3oml")$set_threshold("warn")
options(warnPartialMatchArgs = FALSE)
options(warnPartialMatchAttr = FALSE)
options(warnPartialMatchDollar = FALSE)
options(mlr3.exec_chunk_size = 1)
options(width = 73, digits = 3)

knitr::opts_chunk$set(fig.width=7, fig.height=5, eval = TRUE)
library("data.table")

library("mlr3oml")
dir.create(here::here("book", "openml"), showWarnings = FALSE, recursive = TRUE)
options(mlr3oml.cache = here::here("book", "openml", "cache"))
.cl = parallel::makePSOCKcluster(parallel::detectCores())
.oldplan = future::plan(future::cluster, workers = .cl)

# disable all output everywhere
parallel::clusterCall(.cl, function() {
  Sys.setenv(OMP_NUM_THREADS=1)
  Sys.setenv(OMP_THREAD_LIMIT=1)
  data.table::setDTthreads(1)
  RhpcBLASctl::blas_set_num_threads(1)
  RhpcBLASctl::omp_set_num_threads(1)
  lgr::get_logger("bbotk")$set_threshold("warn")
  lgr::get_logger("mlr3")$set_threshold("warn")
  lgr::get_logger("mlr3tuning")$set_threshold("warn")
  options(warnPartialMatchArgs = FALSE)
  options(warnPartialMatchAttr = FALSE)
  options(warnPartialMatchDollar = FALSE)
})
```

`r authors("Pipelines")`

Machine learning (ML) toolkits often try to abstract away the processes inside ML algorithms.
These toolkits provide a layer of abstraction, allowing users to swap one algorithm for another without having to worry about specifics of each algorithm.
The benefit of using `r ref_pkg("mlr3")`, for example, is that one can use any `r ref("Learner")`, `r ref("Task")`, or `r ref("Resampling")` object and use them for typical ML operations, mostly independently of what algorithms or datasets they represent.
In the following code snippet, it would be trivial to swap in a different learner than `r ref("LearnerRegrRpart", "lrn(\"regr.rpart\")")` without having to worry about implementation details:
```{r 05-pipelines-in-depth-002, output = FALSE}
set.seed(1)
task = as_task_regr(cars, target = "dist")
learner_rpart = lrn("regr.rpart")
resampling = rsmp("holdout")
resample(task, learner_rpart, resampling)
```

However, this modularity breaks down as soon as the learning process encompasses more than just model fitting, like data preprocessing, building ensemble-models or even more complicated meta-models.
This is where `r ref_pkg("mlr3pipelines")` [@mlr3pipelines] steps in: it takes modularity one step further than `r mlr3` and makes it possible to build individual steps within a `r ref("Learner")` out of building blocks that manipulate data.
Individual, frequently encountered building blocks, such as missing value imputation or majority vote ensembling, are provided as an (R6-)object, which we call a `r define("PipeOp")`.
These PipeOps can be connected using directed edges inside a `r define("Graph")` (or "Pipeline") to represent the flow of data between operations.

:::{.callout-tip}
The `r ref("Graph")` class represents an object similar to a directed acyclic graph (DAG), since the input of a PipeOp can not depend on its output and hence cycles are not allowed.
However, the resemblance to a DAG is not perfect, since the `r ref("Graph")` class allows for multiple edges between nodes.
A term such as "directed acyclic multigraph" would be more accurate, but we will stick to the term "Graph" for simplicity.
:::

Some examples of operations that can be performed with `r mlr3pipelines` are:

* Data manipulation and preprocessing operations, e.g. PCA, feature filtering, missing value imputation.
* Task subsampling for speed or for handling of imbalanced classes.
* Ensemble methods and aggregation of predictions.
* Simultaneous model selection and hyperparameter tuning of both learners and preprocessing operators, by using `r mlr3tuning`. This is sometimes facetiously referred to as "CASH", standing for "Combined Algorithm Selection and Hyperparameter optimization" [@Thornton2013].

During model training, the PipeOps in a Graph operate on a given dataset and transform it.
PipeOps that come later in a Graph get the already transformed data as input.
The Graph can therefore be thought of as a network representing function composition.
However, besides transforming data, PipeOps also generate a *state*, similar to how Learners train model parameters.
This state is used to inform the PipeOp's operation during prediction.

As a simple example, consider scaling features to have unit variance `po("scale", center = FALSE)` (@fig-pipelines-state).
During training, it calculates and divides by the standard deviation of each feature.
For prediction, it would be wrong to divide by the standard deviation of the *prediction* dataset; instead the same scaling factors as during training should be used.
The PipeOp therefore stores the scaling factors of each feature in its state, to be used during prediction.
Each PipeOp stores its own state independently of other PipeOps.
A very simple sequential Graph that does various preprocessing operations before fitting a learner is displayed in @fig-pipelines-examples (a).
@fig-pipelines-examples (b) shows a more elaborate pipeline that does alternative path branching.

```{r fig.align='center', eval = TRUE}
#| label: fig-pipelines-state
#| fig-cap: "`$train()` of the \"Scaling\" PipeOp both transfors data (rectangles) as well as creates a state: the scaling factors, necessary to transform data during prediction."
#| fig-alt: "Traning data is transformed by a Scaling PipeOp, which also sets the state inside the PipeOp."
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/state_graphic.svg")
```

When evaluating the performance of an ML process consisting e.g. of preprocessing, followed by model fitting, it is strongly advised to always put the entire preprocessing operation *inside* the resampling loop, as is done here -- `r mlr3pipelines` encourages this more accurate approach.
Making a PCA on the entirety of a dataset, followed by resampling a learner on it, would effectively leak information from the training set to the test set, leading to biased results.

## `r mlr3pipelines` by Example {#sec-pipelines-intro}

In the following, we provide a brief example of `r mlr3pipelines` to give you a quick idea of what it is capable of.
The dataflow programming concept implemented by `r mlr3pipelines` is very intuitive, and seeing a few code snippets will already give you a clear idea of how to use it.
In subsequent chapters, we will elaborate on each of the concepts used here.

```{r eval = TRUE}
#| label: fig-pipelines-example
#| layout: "[50, 50]"
#| layout-valign: bottom
#| fig-cap: "
#|   Representations of different pipelines.
#|   (a): A simple sequential pipeline that does some preprocessing before feeding data to a learner.
#|   (b): A pipeline that offers three alternative preprocessing paths:
#|   Data can either be scaled, PCA-transformed, or not transformed at all (\"null\") before being fed into the \"classif.rpart\" learner.
#|   By tuning the hyperparameter of the \"branch\" PipeOp, it is possible to discover which preprocessing operation leads to the best performance."
#| fig-subcap:
#|   - Sequential pipeline.
#|   - Alternative path branching pipeline.
#| fig-alt:
#|   - Sequential pipeline that does scaling, factor encoding, and median imputation before fitting a model.
#|   - "Pipeline that feeds data into a \"branch\" operator, followed by, alternatively, \"null\", \"pca\", and \"scale\". Outputs of these are combined into \"unbranch\", followed by \"classif.rpart\"."
#| echo: false
knitr::include_graphics("Figures/single_pipe.svg")

# This just produces a plot, not visible to the user.
library("mlr3pipelines")

graph = po("branch", c("nop", "pca", "scale")) %>>%
  gunion(list(
    po("nop", id = "null1"),
    po("pca"),
    po("scale")
  )) %>>%
  po("unbranch", c("nop", "pca", "scale")) %>>%
  po("learner", lrn("classif.rpart"))

graph$plot()
```

PipeOps can be easily created using the `r ref("po", "po()")` constructor function.
Similar to learners, they have hyperparameters that can be set during construction as well as by using the `$param_set$values` field, as demonstrated in @sec-param-set.
The following sets the `scale.` hyperparameter, which corresponds to the `scale.` argument of the `r ref("prcomp", "prcomp()")` function that underlies `r ref("PipeOpPCA", "po(\"pca\")")`:

```{r pipeop-quickstart-library, eval = TRUE}
library("mlr3pipelines")

pca = po("pca", scale. = TRUE)
```

PipeOps can be combined to form Graphs.
The simplest way to combine PipeOps is to use the `r ref("concat_graphs", "%>>%")`-operator.
To create a Graph that trains a model, combine PipeOps with a `r ref("Learner")`.
The `$plot()` method can be used to display the structure of the created Graph.

```{r pipeop-quickstart-graphs-1, eval = TRUE}
graph = pca %>>% lrn("classif.rpart")

graph$plot(horizontal = TRUE)
```

A Graph in which the last operation is a `r ref("Learner")` can itself be transformed into a learner, which performs the operations represented by the Graph in order, by using `r ref("as_learner", "as_learner()")`.

```{r pipeop-quickstart-graphlearner-1, eval = TRUE}
graph_learner = as_learner(graph)

graph_learner$train(tsk("iris"))
```
Notice how the decision variables in the model are not the columns of the original dataset, but rather the principal components extracted by `r ref("PipeOpPCA", "po(\"pca\")")`.
```{r pipeop-quickstart-graphlearner-1b, eval = TRUE}
graph_learner$model$classif.rpart$model
```

`r ref("resample", "resample()")`, `r ref("benchmark", "benchmark()")`, and `r ref("tune", "tune()")` work for this object just as for any other `r ref("Learner")`.
```{r pipeop-quickstart-graphlearner-2}
rr = resample(tsk("iris"), graph_learner, rsmp("cv"))
rr$aggregate()
```

## `PipeOp`: Pipeline Operators {#sec-pipelines-pipeops}

The most fundamental unit of functionality within `r mlr3pipelines` is the `r ref("PipeOp")`, short for "pipeline operator".
It represents a transformative operation on input (for example, a training dataset), resulting in output.
Like a learner, it possesses a `$train()` and a `$predict()` function; however, unlike a learner, the `$train()` function can also return a result.
The training phase typically generates a particular model of the data, which is saved as internal state.
The prediction phase then operates on the prediction data based on the trained model.
Therefore, just like a Learner, a PipeOp has "parameters" (i.e., the state) that are trained, but also "hyperparameters" that users can set.
An illustration of this behavior is the *principal component analysis* operation (`r ref("PipeOpPCA", "po(\"pca\")")`), which we have already seen in the previous section:
```{r pipeop-intro-1, eval = TRUE}
pca = po("pca")
pca
```
When printed, various aspects of this PipeOp can be observed:
The first line tells us the ID of the PipeOp ("`pca`") and that it is not trained yet, therefore lacking a `$state`.
The second line would tell us about hyperparameter values.
However, this is an empty list here, as we are using the default settings of `r ref("PipeOpPCA", "po(\"pca\")")`.
The remaining lines provide information about the input and output types of this PipeOp.
The PCA PipeOp takes one input (named "`input`") of type "`Task`", both during training and prediction (thus "`[Task,Task]`).
It produces one output (named "`output`"), also of type "`Task`" during both phases.

Training is conducted using the `$train()`-method.
Unlike the learner's `$train()` method, we can imagine the PipeOp's `$train()` as potentially having multiple inputs and outputs.
This is implemented through `list`s: Both the `$train()` input and output of a PipeOp are always `list`s; the number of elements of these lists depends on the operation.
For example, the `r ref("PipeOpPCA", "po(\"pca\")")` PipeOp only transforms a single dataset and is thus invoked with a list containing a single element, the training data task.
It returns a modified task with features replaced by their principal components.

```{r 05-pipelines-in-depth-003, eval = TRUE}
poin = list(tsk("iris"))
poout = pca$train(poin)
poout
poout[[1]]$head()
```

During training, the PCA transforms incoming data by rotating it in such a way that features become uncorrelated and are ordered by their contribution to total variance.
The rotation matrix is also saved to be applied to new data during the "prediction phase".
This allows for "prediction" with single rows of new data, where a row's scores on each of the principal components (the components of the *training* data!) are computed.

Similarly to `$train()`, the `$predict()` function operates on `list`s:

```{r 05-pipelines-in-depth-004, eval = TRUE}
single_line_iris = tsk("iris")$filter(1)
single_line_iris$data()
poin = list(single_line_iris)
poout = pca$predict(poin)
poout[[1]]$data()
```

The internal state that is trained in the `$train()` call is stored in the `$state` field (shown in @fig-pipelines-state). It is analogous to the `$model` field of a learner, and its content depends on the class of PipeOp being used.

```{r 05-pipelines-in-depth-005, eval = TRUE}
pca$state
```

### Creating `PipeOp`s

Each PipeOp is an instance of an `R6` class, with many classes provided by the `r mlr3pipelines` package itself.
They can be retrieved from the `r ref("mlr_pipeops")` dictionary, most easily by using the `r ref("po", "po()")` convenience function.
A shortcut for creating lists of PipeOps from a vector of names is `r ref("pos", "pos()")`.
When retrieving PipeOps from the `r ref("mlr_pipeops")` dictionary, it is also possible to provide additional constructor arguments, such as an ID or hyperparameter settings.

```{r 05-pipelines-in-depth-2-009, eval = TRUE}
po("pca", rank. = 3, id = "pca2")
```

Note how the printed output for this PipeOp differs from the one shown for `r ref("PipeOpPCA", "po(\"pca\")")` above:
It has a different ID, and the fact that the `rank.` setting differs from the default is also shown.

Some PipeOps, in fact, require construction arguments, for example when they are operators that wrap another `mlr3`-object.
```{r 05-pipelines-pipeops-006, eval = TRUE}
po_learner = po("learner", learner = lrn("classif.rpart"))
```

Calling `po()` by itself prints all available PipeOps.
You can use `as.data.table(po())` to get a more detailed list with additional meta-data.
The current list of all PipeOps contained in `r mlr3pipelines` with links to their documentation can also be found on the `r link("https://mlr-org.com/pipeops.html", "mlr website")`.
```{r 06-pipelines-pipeops-006-1, eval = TRUE}
po()
```

## `Graph`: Networks of `PipeOp`s {#sec-pipelines-graphs}

### Basics: Building a `Graph`

PipeOps are used to represent individual computational steps in ML pipelines.
These pipelines themselves are defined by `r ref("Graph")` objects.
A Graph is a collection of PipeOps with "edges" that guide the flow of data.

The most convenient way of building a Graph is to connect a sequence of PipeOps using the **`r ref("concat_graphs", "%>>%")`** ("double-arrow") operator.
When given two PipeOps, this operator creates a Graph that first executes the left-hand PipeOp, followed by the right-hand one.
It can also be used to connect a Graph with a PipeOp, or with another Graph.
The following example creates a Graph that first adds a `Petal.Area` feature to a given dataset, and then performs scaling and centering of all numeric features.
```{r 05-sequential-01, eval = TRUE}
po_area = po("mutate",
  mutation = list(Petal.Area = ~Petal.Width * Petal.Length)
)
po_scale = po("scale")
gr = po_area %>>% po_scale
print(gr)
```

The printer provides information about the layout of the Graph:
For each PipOp (column "ID"), it displays information about its state, as well as a list of its successors ("sccssors", i.e. which PipeOps are connected to its output and come directly after it) and its predecessors ("prdcssors", which PipeOps are connected to its input).
For this simple Graph, we see that the output of the PipeOp with the ID "`mutate`" is given to the one with ID "`scale`".
While the printer of a Graph gives some information about its layout, the most intuitive way of visualizing it is using the `$plot()` function.

```{r 05-pipelines-in-depth-017, eval = TRUE}
gr$plot(horizontal = TRUE)
```

### `Graph`s are Nodes with Edges

Internally, Graphs are collections of PipeOps with edges connecting them.
The collection of PipeOps inside a Graph can be accessed through the `$pipeops` field.
The set of edges in the Graph can be examined through the `$edges` field.
It is a `data.table` listing the "source" (`src_id`, `src_channel`) and "destination" (`dst_id`, `dst_channel`) of data flowing along each edge.

```{r 05-pipelines-in-depth-018-2, eval = TRUE}
gr$pipeops
gr$edges
```

Besides using the `r ref("concat_graphs", "%>>%")`-operator to create Graphs, it is also possible to create them explicitly.
A Graph is empty when first created, and PipeOps can be added using the `$add_pipeop()` method.
The `$add_edge()` method is used to create connections between them.
The above Graph can therefore also be created in the following way:

```{r 05-pipelines-in-depth-016, eval = TRUE}
gr = Graph$new()
gr$add_pipeop(po_area)
gr$add_pipeop(po_scale)
gr$add_edge("mutate", "scale")  # address by PipeOp-ID
```

:::{.callout-warning}
Although it is also possible to modify individual PipeOps and edges in a Graph through the `$pipeops` and `$edges` fields, this is not recommended, as no error checking is performed and it may put the Graph in an invalid state.
Only do this if you know what you are doing.
:::

### Using a `Graph`

A Graph itself has a `$train()` and a `$predict()` method that accept some data and propagate this data through the network of PipeOps, by calling their respective `$train()` and `$predict()` methods.
The return value is the output of the PipeOp(s) without outgoing edges.
Just like for PipeOps, the output is a list.

```{r 05-pipelines-in-depth-019, eval = TRUE}
result = gr$train(tsk("iris"))
result
result[[1]]$head()
result = gr$predict(single_line_iris)
result[[1]]$head()
```

### Debugging a `Graph` with Intermediate Results

When Graphs are evaluated, they do not keep intermediate results for memory efficiency, unless the `$keep_results` flag is set first.
Inspecting these results may help understanding the inner workings of Graphs, in particular when they produce unexpected results.

```{r 05-pipelines-in-depth-021-x, eval = TRUE}
gr$keep_results = TRUE
result = gr$predict(single_line_iris)
intermediate = gr$pipeops$mutate$.result
intermediate
intermediate[[1]]$data()
```

## Sequential `Learner`-Pipelines {#sec-pipelines-sequential}

Possibly the most common application for `r mlr3pipelines` is to use it to perform basic preprocessing tasks, such as missing value imputation or factor encoding, and to then feed the resulting data into a `r ref("Learner")`.
A Graph representing this workflow manipulates data and fits a `Learner`-model during training, and uses the fitted model with data that has been similarly preprocessed during prediction.
Conceptually, the process may look as shown in @fig-pipelines-pipeline.

```{r 05-pipelines-modeling-002, eval = TRUE}
#| label: fig-pipelines-pipeline
#| fig-cap: "Conceptualization of training and prediction process inside a sequential learner-pipeline. During training (top row), the data is passed along the preprocessing operators, each of which modifies the data and creates a `$state`. Finally, the learner receives the data and a model is created. During prediction (bottom row), data is likewise transformed by preprocessing operators, using their respective `$state` information in the process. The learner then receives data that has the same format as the data seen during training, and makes a prediction."
#| fig-alt: "Traning data is transformed by a sequential pipeline during training, being passed along a scaling, factor encoding, and median imputation PipeOp and finally given to a learner. Prediction data is passed along the same pipeline, this time containing state and model objects, to create a prediction."
#| echo: false

knitr::include_graphics("Figures/pipe_action.svg")
```

While a `r ref("Learner")` is not a `r ref("PipeOp")` by itself, it can be readily converted into one using `r ref("as_pipeop", "as_pipeop()")`, or alternatively `r ref("PipeOpLearner", "po(\"learner\")")`, which creates a `r ref("PipeOpLearner")`-wrapper-class.
```{r 05-pipelines-modeling-0, eval = TRUE}
learner_rpart = lrn("classif.rpart")
po_rpart = as_pipeop(learner_rpart)
po_rpart = po("learner", learner_rpart)  # yields the same result
```

However, this conversion is rarely necessary, as the `%>>%`-operator automatically converts Learners to PipeOps.
The following code creates a Graph that adds a `Petal.Area` feature, followed by fitting a `"classif.rpart"` decision tree model.

```{r 05-pipelines-modeling-1, eval = TRUE}
po_area = po("mutate",
  mutation = list(Petal.Area = ~Petal.Width * Petal.Length)
)
gr = po_area %>>% learner_rpart  # could just as well use po_rpart
gr$plot(horizontal = TRUE)
```

To use a Graph as a learner within `r mlr3`, it is necessary to wrap it in a `r ref("GraphLearner")` object.
This is because there are various differences between the classes, most notably the return values of the `$train()` and `$predict()` methods.
A Graph can be converted to a `r ref("GraphLearner")` using `r ref("as_learner", "as_learner()")`.
```{r 05-pipelines-modeling-2, eval = TRUE}
graph_learner = as_learner(gr)
```

This learner can be used like any other `r ref("Learner")`.
In particular it can be used with `resample()` and `benchmark()`.
Let us compare our sequential pipeline with the `"classif.rpart"`-`Learner` by itself:
```{r 05-pipelines-modeling-3}
grid = benchmark_grid(
  tsks("iris"),
  list(graph_learner, learner_rpart),
  rsmps("repeated_cv")
)
bmr = benchmark(grid)
bmr$aggregate()
```

### Accessing Pipeline Objects

The `graph_learner` variable containing the `GraphLearner` object can be used as an ordinary learner.
However, it is in fact a wrapper around a Graph, which in turn contains PipeOps, which themselves encompass different components.
The following steps demonstrate how to analyze the flow of data in a `GraphLearner`.
First, the `$keep_results` flag needs to be set, so intermediate results are retained.
```{r 05-pipelines-modeling-debugging, eval = TRUE}
graph_learner$graph_model$keep_results = TRUE
graph_learner$train(tsk("iris"))
```

The Graph can be accessed through the `$graph_model` field.
Using this field, one can now investigate the data fed to the `"classif.rpart"` learner by examining the output of the `"mutate"`-PipeOp.
As expected, it includes the additional feature `Petal.Area`.
```{r 05-pipelines-modeling-debugging-1, eval = TRUE}
mutate_result = graph_learner$graph_model$pipeops$mutate$.result
mutate_result
mutate_result[[1]]$head()
```

One can also look at the `$state` of the various PipeOps to investigate the trained model.
Here the trained `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` classification tree is interesting.
However, it is wrapped inside a `r ref("PipeOpLearner")`: The trained `r ref("Learner")` object has to be extracted before inspection.
```{r 05-pipelines-modeling-debugging-2, eval = TRUE}
trained_p_rpart = graph_learner$graph_model$pipeops$classif.rpart
trained_l_rpart = trained_p_rpart$learner_model
trained_l_rpart
trained_l_rpart$model
```

:::{.callout-tip}
A more straightforward approach to access the learner at the end of a Graph in a `r ref("GraphLearner")` is to use the `$base_learner()` method.
One can therefore also use `graph_learner$base_learner()$model` to access the trained model.
However, this method does not work for ensembling `r ref("GraphLearner")` objects containing multiple learners.
:::

### Pipeline Hyperparameters {#sec-pipelines-hyperparameters}

Much like `r ref("Learner")`s, PipeOps have *hyperparameters*, provided by the `r mlr3book::ref_pkg("paradox")` package.
These can be accessed through the `$param_set` field, providing information about the adjustable hyperparameters.

```{r 05-pipelines-in-depth-032, eval = TRUE}
po_pca = po("pca")
po_pca$param_set
```

Similar to `r ref("Learner")` objects, the `$param_set$values` field can be used to alter hyperparameter settings; alternatively, hyperparameter values can be set using the `$param_set$set_values()` function or during construction.

```{r 05-pipelines-in-depth-033, eval = TRUE}
po_pca$param_set$values$center = FALSE
# More convenient when multiple hyperparameters need to be set:
po_pca$param_set$set_values(center = TRUE)
# Alternatively:
po_pca = po("pca", center = FALSE)
# All of these have the same result:
po_pca$param_set$values
```

Each PipeOp can have its own individual hyperparameters, which are collected together in the Graph's `$param_set`.
A PipeOp's parameter names are prefixed with its ID to avoid parameter name clashes.

```{r 05-pipelines-in-depth-035, eval = TRUE}
gr = po_pca %>>% po("scale", center = TRUE)
gr$param_set
```


The hyperparameter settings of a `r ref("GraphLearner")` can be changed directly (recommended), but they can also be accessed indirectly by reading (and modifying) the underlying Graph's, PipeOp's, or learner's hyperparameters.

:::{.callout-warning}
When a learner is encapsulated in a `r ref("PipeOpLearner")` through `as_pipeop()`, its `ParamSet` is exposed.
Once this PipeOp becomes part of a Graph, the hyperparameters are prefixed with the PipeOp's ID, which by default is the learner's ID.
When a Graph is converted back into a learner using `as_learner()`, the resulting `r ref("GraphLearner")` retains the Graph's `r ref("ParamSet")`.
Therefore the original learner's hyperparameters are now prefixed with the learner's ID.
For example, if a `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` is encapsulated in a Graph, its `maxdepth` hyperparameter becomes `classif.rpart.maxdepth`.
:::

### IDs and Name Clashes

To ensure that PipeOps can be accessed by their ID within Graphs, their IDs within a Graph must be unique.
IDs can be set during construction using the `id` argument of `po()`, or they can be modified for existing PipeOps.
For PipeOps already in a Graph, the `$set_names()` method can also be employed to change IDs, although this should rarely be necessary.

```{r 05-pipelines-in-depth-040, eval = TRUE}
# Without the `id` argument, this would lead to a name collision error
gr = po("pca") %>>% po("pca", id = "pca2")
gr
gr$set_names(
  old = c("pca", "pca2"),
  new = c("pca_1", "pca_2")
)
gr
```

:::{.callout-warning}
Avoid changing the ID of a PipeOp that is already in a Graph through `graph$pipeops$<old_id>$id = <new_id>`, as this will only alter the PipeOp's record of its own ID, not the Graph's record.
This would result in undefined behavior for the Graph.
:::

## Non-Sequential Graphs {#sec-pipelines-nonsequential}

{{< include _optional.qmd >}}

So far, we have shown how simple sequential Graphs can be constructed from preprocessing PipeOps and encapsulated learners.
We will now present more involved pipelines that can perform more complex operations.

### Parallel `PipeOp`s

Beyond chaining PipeOps sequentially to perform preprocessing operations in order, it is also possible to arrange PipeOps in parallel.
Most Graph layouts can be assembled using two tools:

* The `r ref("gunion", "gunion()")` operation, which takes multiple PipeOps, Graphs, or a mixture of them, and arranges them in parallel, and
* the `%>>%`-operator, which is capable of chaining Graphs that contain parallel elements, as long as the number of inputs and outputs matches.
  It can even connect a Graph with a single output to a Graph with multiple inputs (the data is distributed to all inputs), or a Graph with multiple outputs to certain special PipeOps with a single input.

The following creates a Graph that first centers its inputs, and then copies the scaled data to two parallel streams: one replaces the data with columns that indicate whether data is missing, the other imputes missing data using the median. The outputs of both streams are then combined into a single dataset using `r ref("PipeOpFeatureUnion")`.

```{r 05-pipelines-modeling-003, fig.width = 8, eval = TRUE}
gr = po("scale", center = TRUE, scale = FALSE) %>>%
  gunion(list(
    po("missind"),
    po("imputemedian")
  )) %>>%
  po("featureunion")
gr$plot(horizontal = TRUE)
```

Processing the first five lines of the "Pima" dataset with this Graph shows how the missing values of the `"insulin"` and `"triceps"` features are handled:
They are imputed, and the corresponding `"missing"`-columns indicate where values were missing.
```{r 05-pipelines-modeling-004, eval = TRUE}
pima_head = tsk("pima")$filter(1:5)
pima_head$data(cols = c("diabetes", "insulin", "triceps"))
result = gr$train(pima_head)[[1]]
result$data(cols = c("diabetes", "insulin", "missing_insulin", "triceps",
  "missing_triceps"))
```

### `po("select")`, `po("featureunion")`, and `affect_columns`

A typical pattern for Graphs is that an operation should be applied to a certain subset of features, but not to another subset.
This can be realized in two ways, as illustrated in @fig-pipelines-select-affect

1. Many preprocessing PipeOps have an `affect_columns` hyperparameter.
   It can be set so that the PipeOp only operates on a certain subset of columns.
1. One can use the `r ref("PipeOpSelect", "po(\"select\")")` operator in parallel, selecting certain features on which operations should be performed, and uniting the result using `r ref("PipeOpFeatureUnion", "po(\"featureunion\")")`.

```{r eval = TRUE}
#| label: fig-pipelines-select-affect
#| layout-nrow: 2
#| fig-cap: "
#|   Two ways of setting up preprocessing operators (`po(op1)` and `po(op2)`) so that they operate on complementary features of an input task.
#|   Both rely on having a `Selector` \"X\", and its complement \"¬X\" = `selector_invert(X)`.
#|   The simpler case, a single operator working only on a subset of all features, is reached by omitting one of `po(op1)` or  `po(op2)`.
#|   (a): The `affect_columns` hyperparameter that many preprocessing PipeOps provide can be used to restrict operations on subsets of features.
#|        PipeOps that should transform complementary features can easily be put in sequence.
#|   (b): Using the `po(\"select\")` operator, one can remove undesired features, causing subsequent operations to only see the remaining ones.
#|        Here the different subsets of the task are processed on concurrent paths and then combined using a `PipeOpFeatureUnion`.
#|        Note that although the two different `po(\"select\")` operators have distinct inputs, they are fed the same data if they have the same predecessor in the Graph.
#| "
#| fig-alt:
#|   - "po(op1, affect_columns: X), followed by po(op2, affect_columns: not X)"
#|   - "Two alternative paths, one through po(\"select\", X) and po(op1), another through po(\"select\", not X) and po(op2), both followed by po(\"featureunion\")"
#| fig-subcap:
#|   - Operating on subsets of tasks using `affect_columns`.
#|   - "Operating on subsets of tasks using concurrent paths and `po(\"select\")`."
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/affect_pipe.svg")
knitr::include_graphics("Figures/select_pipe.svg")
```

Both of these solutions make use of `r ref("Selector")`-functions.
These are helper-functions that indicate to a PipeOp which features an operation it should apply to.
Straightforward Selectors include, for example, `r ref("selector_grep", "selector_grep()")`, which selects features by name matching a regular expression, or `r ref("selector_type", "selector_type()")`, which selects by type.
Other Selectors can perform set-operations (`r ref("selector_union", "selector_union()")`, `r ref("selector_setdiff", "selector_setdiff()")`) or take all features *not* taken by another Selector (`r ref("selector_invert", "selector_invert()")`).

If one wants to perform PCA on the "Petal"-features of the Iris dataset, but only do scaling on the other features, one would first need a Selector that selects these two columns.
Solving the problem with the `affect_columns` hyperparameter would then work as follows:
```{r 05-pipelines-multicol-1, eval = TRUE}
sel_petal = selector_grep("^Petal")
sel_not_petal = selector_invert(sel_petal)

gr = po("scale", affect_columns = sel_not_petal) %>>%
  po("pca", affect_columns = sel_petal)

result = gr$train(tsk("iris"))
result[[1]]$head()
```

:::{.callout-warning}
Care should be taken with this approach if PipeOps rename or add columns.
In this example, if the `r ref("PipeOpPCA", "po(\"pca\")")` were used before the `r ref("PipeOpScale", "po(\"scale\")")`, the `r ref("PipeOpPCA", "po(\"pca\")")` would add columns `"PC1"` and `"PC2"`, and the `r ref("PipeOpScale", "po(\"scale\")")` would scale these columns as well, since they would match `sel_not_petal`.
:::

Solving this using parallel paths makes use of the `r ref("PipeOpSelect", "po(\"select\")")` operator.
It removes all features that are not selected by a given Selector, making it possible to have independent data processing streams for different feature subsets.
Since two `r ref("PipeOpSelect", "po(\"select\")")` operators are present, it is necessary to give them different IDs to avoid ID name clashes.
The solution makes use of the fact that parallel paths all receive copies of the input data when they are at the beginning of a Graph.
```{r 05-pipelines-multicol-3, fig.width = 8, eval = TRUE}
gr = gunion(list(
  po("select", id = "s_petal", selector = sel_petal) %>>% po("pca"),
  po("select", id = "s_sepal", selector = sel_not_petal) %>>% po("scale")
)) %>>% po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-multicol-4, eval = TRUE}
result = gr$train(tsk("iris"))
result[[1]]$head()
```

The advantage of the first method is that it creates a very simple, sequential Graph.
However, sometimes it is not possible to perform a desired operation only using `affect_columns`, particularly when the same set of features is used in multiple operations, or when the original features should be kept.
For example, the following performs PCA on the "Petal" features, but also keeps all original features.
The latter is accomplished using the `r ref("PipeOpNOP", "po(\"nop\")")` operator, which does not change its operand.


```{r 05-pipelines-multicol-5, fig.width = 8, eval = TRUE}
gr = gunion(list(
  po("select", id = "sel_petal", selector = sel_petal) %>>% po("pca"),
  po("nop")
)) %>>% po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-multicol-6, eval = TRUE}
result = gr$train(tsk("iris"))
result[[1]]$head()
```

### Example: Bagging {#sec-pipelines-bagging}

The basic idea of Bagging, introduced by [@Breiman1996], is to create multiple predictors and then aggregate those to a single, more powerful predictor.
Predictions are aggregated by averaging (regression) or majority vote (classification).
The underlying intuition behind bagging is that averaging set of weak, but different (i.e., only weakly correlated) predictors can reduce the variance of the overall prediction.

We can achieve this by subsampling our data before training a learner, repeating this a number of times, and then performing a majority vote on the predictions.
A schematic is shown in @fig-pipelines-bagging.


```{r eval = TRUE}
#| label: fig-pipelines-bagging
#| fig-cap: "Graph that performs Bagging by independently subsampling data and fitting individual decision tree learners. The resulting predictions are aggregated by a majority vote PipeOp. Note that the name of the majority vote PipeOp is \"classif.avg\" for naming consistency."
#| fig-alt: "Bagging Graph. Data flows through independent subsampling PipeOps and decision tree learners, to be combined by a majority vote PipeOp."
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/nonlinear_pipeops.svg")
```

Although there is a `"bagging"` entry in `r ref("ppl", "ppl()")` that automatically creates a bagging Graph (@sec-pipelines-ppl), it is instructive to think about how bagging can be constructed from scratch, using the building blocks provided by `mlr3pipelines`.
First, we create a simple pipeline that uses `r ref("PipeOpSubsample", "po(\"subsample\")")` before a learner is trained:

```{r 05-pipelines-non-sequential-009, eval = TRUE}
single_pred = po("subsample", frac = 0.7) %>>% lrn("classif.rpart")
```

We can now copy this operation 10 times using `r ref("pipeline_greplicate", "ppl(\"greplicate\")")`.
`r ref("pipeline_greplicate", "ppl(\"greplicate\")")`` allows us to parallelize many copies of an operation by creating a Graph containing `n` copies of the input Graph.
Afterwards we need to aggregate the 10 pipelines to form a single model:

```{r 05-pipelines-non-sequential-010, eval = TRUE}
pred_set = ppl("greplicate", graph = single_pred, n = 10)

bagging = pred_set %>>%
  po("classifavg", innum = 10)
```

The following plot shows the layout of the resulting Graph.

```{r 05-pipelines-non-sequential-012, fig.width = 16, eval = TRUE}
bagging$plot(vertex.label.cex = 1)
```

The bagging pipeline can be converted to a learner using `r ref("as_learner", "as_learner()")`.
The following code compares it to a single `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` on the "`r ref("mlr_tasks_sonar", "sonar")`" dataset.
This dataset contains sonar response levels in different frequency bands, where the task is to differentiated between metal objects (such as mines) and rocks.
The bagged learner performs noticeably better in this example.
We note, however, that the bagged decision tree is still outperformed by the random forest (`r ref("LearnerClassifRanger", "lrn(\"classif.ranger\")")`).

```{r 05-pipelines-non-sequential-013}
l_bag = as_learner(bagging)
l_bag$id = "bagging"
learner_rpart = lrn("classif.rpart")
rsmp_sonar = rsmp("cv")$instantiate(tsk("sonar"))
grid = benchmark_grid(tsks("sonar"),
  list(l_bag, learner_rpart, lrn("classif.ranger")), list(rsmp_sonar)
)
bmr = benchmark(grid)
bmr$aggregate()
```

We can, however, use `mlr3pipelines` and `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` to come very close to an actual random forest!
The main difference is that the random forest also performs "feature bagging", where only a random subset of available features is used for each tree.
We can implement using the `r ref("PipeOpSelect", "po(\"select\")")` operator and a custom `Selector`.
We also use the `r ref("pipeline_bagging", "ppl(\"bagging\")")` method mentioned above.
It makes use of the `r ref("Multiplicity")` construct, which is a more efficient way of building massively parallel Graphs.
We therefore need to tell `r ref("PipeOpClassifAvg", "po(\"classifavg\")")` to accept a `r ref("Multiplicity")` object as input, which is done by setting `collect_multiplicity = TRUE`.
Here we use 300 trees, just as the random forest.

```{r 05-bagging-ex}

selector_subsample = function(task) {
  sample(task$feature_names, sqrt(length(task$feature_names)))
}

bagging_quasi_rf = ppl("bagging",
  graph = po("select", selector = selector_subsample) %>>%
    lrn("classif.rpart", minsplit = 1),
  iterations = 300,
  averager = po("classifavg", collect_multiplicity = TRUE)
)

bagging_quasi_rf$param_set$values$subsample.frac = 1
bagging_quasi_rf$param_set$values$subsample.replace = FALSE

l_quasi_rf = as_learner(bagging_quasi_rf)
l_quasi_rf$id = "quasi.rf"

grid = benchmark_grid(tsks("sonar"),
  list(l_quasi_rf, lrn("classif.ranger")), list(rsmp_sonar)
)
bmr = benchmark(grid)
bmr$aggregate(msrs(c("classif.ce", "time_both")))
```

The result shows that we managed to build a learner that behaves and performs very closely to `r ref("ranger::ranger")`.
The `time_both` measure also tells us that our implementation is slower by orders of magnitude, since `r ref("ranger::ranger")` is written in C++.
However, it also took us much less time construct our custom learner, compared to what the authors of `r ref("ranger::ranger")` likely needed.
If the goal is to construct new kinds of learning algorithms that work for a specific purpose, then computer time is often much cheaper than developer time!


### Example: `PipeOpLearnerCV` and Stacking {#sec-pipelines-stack}

Stacking [@Wolpert1992] is another technique that can improve model performance.
The basic idea behind stacking is to use of predictions from one model as features for a subsequent model to possibly improve performance.
See @fig-pipelines-stacking for a conceptual illustration.

```{r eval=TRUE, fig.align='center', eval = TRUE}
#| label: fig-pipelines-stacking
#| fig-cap: "Graph that performs Stacking by fitting various models and using their output as features for another model. The `PipeOpLearnerCV` wrapping both a linear model and an SVM will replace the training data by predictions made by these learners. The \"NULL\" (`po(\"nop\")`) operation does not change the training data and makes sure that the original features also remain present. Their combined output is given to the feature union PipeOp, which creates a single training task to be given to the Random Forest learner. "
#| fig-alt: "Stacking Graph. Data flows through independent PipeOps fitting both a linear model and an SVM, as well as a NULL operator. Their results all flow into a \"Feature Union\" PipeOp, which gives its result to a \"Random Forest\" `PipeOpLearner`."
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/stacking.svg")
```

Just as for bagging, it is possible to create a stacking pipeline using `ppl()`, as described in @sec-pipelines-ppl, but we show how to construct it manually as an illustrative example.
Here we choose to train an ensemble of different models, the outputs of which are then used as features for a logistic regression model.

To limit overfitting, we must create the stacking features from predictions made for data that was not in the training sample.
We therefore use a `r ref("PipeOpLearnerCV", "po(\"learner_cv\")")`, wich performs cross-validation on the training data, fitting a model in each fold.
Each of the models is then used to predict on the out-of-fold data.
As a result, we obtain predictions for every data point in our input data.

We first create various learners, which we call the "level 0" learners, which produce predictions that will be used as features.
Besides the `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` that we have already seen, we also use the k-nearest-neighbor (KNN) learner `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")` and the LASSO learner `r ref("LearnerClassifCVGlmnet", "lrn(\"classif.cv_glmnet\")")`.
We set the `predict_type` to `"prob"` for all learners, so that they produce class probabilities instead of hard class predictions.
We have also set the `kernel` hyperparameter of the KNN learner to `"rectangular"`, which is the simplest distance kernel.
Note that the "`cv`" in the name of the LASSO learner indicates that it performs cross-validation to select the regularization parameter -- it happens independently of the cross-validation performed by `r ref("PipeOpLearnerCV", "po(\"learner_cv\")")`.

```{r 05-pipelines-non-sequential-015, eval = TRUE}
learner_rpart = lrn("classif.rpart", predict_type = "prob")
po_rpart_cv = po("learner_cv",
  learner = learner_rpart,
  resampling.folds = 10, id = "rpart_cv"
)
learner_knn = lrn("classif.kknn",
  kernel = "rectangular",
  predict_type = "prob"
)
po_knn_cv = po("learner_cv",
  learner = learner_knn,
  resampling.folds = 10, id = "knn_cv"
)
learner_glmnet = lrn("classif.cv_glmnet", predict_type = "prob")
po_glmnet_cv = po("learner_cv",
  learner = learner_glmnet,
  resampling.folds = 10, id = "glmnet_cv"
)
```

We combine the the level 0 learners using `r ref("gunion", "gunion()")`, in order to send all their predictions to the next level.
The output produced by an example `$train()` run is instructive:
```{r 05-pipelines-non-sequential-016, eval = TRUE}
level_0 = gunion(list(po_rpart_cv, po_knn_cv, po_glmnet_cv))
combined = level_0 %>>% po("featureunion")

combined$train(tsk("sonar"))
```

:::{.callout-tip}
Each PipeOp has removed the original features and only kept the predictions made by the learners they wrap.
If you also wanted to keep the original features, you could add a `r ref("PipeOpNOP")` to the list given to `r ref("gunion", "gunion()")`, next to the level 0 learners.
It pipes the original features through without changing them.
:::

We see that the resulting task contains the predicted probabilities made by each of the level 0 learners.
However, these predictions are redundant, since the probabilities for each class sum to 1.
We will therefore use a `r ref("PipeOpSelect", "po(\"select\")")` to remove the predictions for the "`R`" (Rock) class and keep only the predictions for the "`M`" (Mine) class.
We append a final PipeOp containing the learner that we want to train on top of the combined features.

```{r 05-pipelines-non-sequential-017, eval = TRUE}
stack = combined %>>%
  po("select", selector = selector_grep("\\.M")) %>>%
  po("learner", lrn("classif.log_reg"))
```

The resulting layout can be visualized by the Graphs `$plot()` function:
```{r 05-pipelines-non-sequential-018, fig.width = 10, eval = TRUE}
stack$plot(horizontal = TRUE)
```

After training this pipeline, we can see the relative degree by which the `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` weights the level 0 learners by inspecting its `$model`.
```{r 05-pipelines-non-sequential-019-x, eval = TRUE}
learner_stack = as_learner(stack)
learner_stack$id = "stacking"
learner_stack$train(tsk("sonar"))
learner_stack$graph_model$pipeops$classif.log_reg$learner_model$model
```

We can see that the output of the KNN learner influences the overall prediction the most, as it has the largest coefficient.
A benchmark of the individual models confirms that the KNN learner is indeed the best individual model.
The benchmark also shows that the stacking model performs better than any of the individual models.

```{r 05-pipelines-non-sequential-019-1-background}
grid = benchmark_grid(
  tsks("sonar"),
  list(learner_rpart, learner_knn, learner_glmnet, learner_stack),
  rsmps("repeated_cv")
)
bmr = benchmark(grid)
bmr$aggregate()
```

In real-world applications, stacking can be done for multiple levels and on multiple different representations of the dataset.
On a lower level, different preprocessing methods can be defined in conjunction with several learners.
On a higher level, we can then combine those predictions in order to form a very powerful model.

## Tuning Graphs {#sec-pipelines-tuning}

Having as many options for preprocessing as `r mlr3pipelines` provides, has many benefits, but it also comes with a drawback:
It enlarges the space of possible hyperparameter configurations considerably.
Not only do preprocessing operations bring their own hyperparameter settings, but whether to do preprocessing, and which preprocessing operation to perform, now also needs to be decided.
Tuning ML models with `r mlr3pipelines` comes in three levels of complexity:

1. Tuning the hyperparameters of a Learner or a PipeOp individually when it is part of a Graph.
2. Jointly tuning the hyperparameters of both Learner and its preprocessing operations.
3. Tuning not only the hyperparameters, but also the choice of which operation to perform.

The first level is not much different from tuning individual Learners, as described in @sec-optimization.
The only thing to watch out for here is that a Learner's hyperparameter names are prefixed with its ID, as shown in @sec-pipelines-hyperparameters.
The second level is demonstrated in the following @sec-pipelines-combined.
The third level is also referred to as the "Combined Algorithm Selection and Hyperparameter optimization" (CASH) [@Thornton2013].
It is demonstrated in @sec-pipelines-branch, whish shows how to use alternative path branching.
An alternative way of implementing it is to use `r ref("PipeOpProxy", "po(\"proxy\")")`, demonstrated in the (optional) @sec-pipelines-proxy.

In this section, we will use the well-known "MNIST" dataset [@lecun1998gradient], which comprises 28 x 28 pixel images of handwritten digits.
It is a classification task with the goal of identifying the digits accurately.
It is often used to demonstrate deep learning with convolutional layers.
However, in this demonstration, we will not use the information about the relative location of pixels.
Instead, we use each pixel's intensity as a separate feature.
We obtain the data from OpenML, which is described in greater detail in @sec-large-benchmarking.
To speed up performance estimation, we subset the given data to 5% of its original size.
```{r 06-pipelines-get-mnist-openml}
library("mlr3oml")
mnist_data = odt(id = 554)
subset = sample(mnist_data$nrow, mnist_data$nrow * 0.05, replace = FALSE)
mnist_task = as_task_classif(mnist_data$data[subset],
    target = "class", id = "mnist")
head(mnist_task$feature_names)
```


### Tuning Combined Spaces {#sec-pipelines-combined}

Instead of using deep learning, we will use the much simpler k-nearest-neighbor (KNN) learner `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")`, again with the simple `"rectangular"` distance kernel.
We investigate if doing a principal component analysis using `r ref("PipeOpPCA", "po(\"pca\")")`, and selecting the highest variance components using the `rank.` hyperparameter, can improve performance.
Because the `k` hyperparameter of the KNN learner also needs to be found, we need to tune it simultaneously.

First we need to define the Graph that we tune.
We have the option of setting each component's hyperparameter being tuned to `to_tune()` as is done in @sec-optimization, but here we demonstrate how to define the search space `r ref("ParamSet")` directly.

```{r 05-pipelines-modeling-008}
library("mlr3learners")
graph_learner = po("pca") %>>%
  lrn("classif.kknn", kernel = "rectangular")
graph_learner = as_learner(graph_learner)

library("paradox")
search_space = ps(
  pca.rank. = p_int(
    lower = 2,
    upper = length(mnist_task$feature_names),
    logscale = TRUE
  ),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE)
)
```

We tune this using the `r ref("tune")` function on a 6x6 grid, stepping through both the number of selected principal components (`pca.rank.`) and the KNN's `k` hyperparameter on a log-scale.


```{r debug-dummy, echo=FALSE, eval = TRUE}
instance = list(result_x_domain = list(pca.rank. = 30))
```
```{r 05-pipelines-modeling-009}
library("mlr3tuning")

instance = tune(
  tuner = tnr("grid_search", resolution = 6, batch_size = 100),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  search_space = search_space
)

instance$result
```

:::{.callout-tip}
Tuning complex pipelines can be very slow:
Not only are the individual training steps often more complex and therefore take longer; the increased search space dimension usually also means that more evaluations need to be performed to get an acceptable level of performance.
It is therefore recommended to make use of parallelization using `r ref_pkg("future")`, which is demonstrated in @sec-parallelization.
:::


Note that the output values are the log of the actual result, which is:
```{r 05-pipelines-modeling-009-2}
instance$result_x_domain
```

The observed performance values are shown in @fig-pipelines-opttrace-1.
It becomes clear that, for different values of `rank.`, different `k` values can be optimal, so jointly tuning both hyperparameters is prudent.

```{r calcbaseline, include = FALSE}
baselineperf = resample(mnist_task, po("pca") %>>% lrn("classif.kknn", kernel = "rectangular"), rsmp("cv", folds = 3))$aggregate(msr("classif.ce"))
baselineperf2 = resample(mnist_task, lrn("classif.kknn", kernel = "rectangular"), rsmp("cv", folds = 3))$aggregate(msr("classif.ce"))
```

```{r fig.align='center', fig.width = 7, fig.height = 3}
#| label: fig-pipelines-opttrace-1
#| fig-cap: "Observed performance values when optimizing both `rank.` of `po(\"pca\")` and `k` of `lrn(\"classif.kknn\")` at the same time.
#|   The x-axis shows `k` values (log scale). Each facet (i.e. individual plot) shows the behavior for a different `rank.` value, displayed at the top.
#|   The red star shows the performance of the untuned model: using `po(\"pca\") %>>% lrn(\"classif.kknn\")` with their defaults: `rank.` set to the number of features, and `k` set to 7."
#| fig-alt: "Plot showing performance values of pca, followed by KNN."
#| echo: false
library("ggplot2")

ggplot(instance$archive$data[, c(rbindlist(x_domain), list(classif.ce = classif.ce))], aes(x = classif.kknn.k, y = classif.ce)) +
  geom_line() +
  ggstar::geom_star(data = data.frame(classif.kknn.k = 7, classif.ce = baselineperf, pca.rank. = length(mnist_task$feature_names)), fill = "red", size = 4, starshape = 1) +
  facet_grid(cols = vars(pca.rank.)) +
  scale_x_log10() +
  theme_minimal() +
  labs(title = "Performance of po(\"pca\") %>>% lrn(\"classif.kknn\")")
```

### Tuning Alternative Paths with `po("branch")` {#sec-pipelines-branch}

While we see that tuning the `r ref("PipeOpPCA", "po(\"pca\")")` has benefits, we have not yet seen whether using PCA at all is beneficial.
It is possible that using the tuned PCA simply does the least damage, or that a much simpler operation would perform equally well or better.

Here we can use the `r ref("PipeOpBranch", "po(\"branch\")")` and `r ref("PipeOpUnbranch", "po(\"unbranch\")")` PipeOps, which make it possible to specify multiple alternative paths.
Data only flows along one of these paths, which can be controlled by a hyperparameter, as is shown in @fig-pipelines-alternatives (a).
This concept makes it possible to tune alternative preprocessing methods or alternative learner models.

`PipeOp(Un)Branch` is initialized either with the number of branches, or with a `character`-vector indicating the names of the branches.
If names are given, the "branch-choosing" hyperparameter becomes more readable.
In the following, we set three options:

1. Doing nothing (`r ref("PipeOpNOP", "po(\"nop\")")`)
2. Applying a PCA
3. Removing constant features and applying the Yeo-Johnson transform, using `r ref("PipeOpYeoJohnson", "po(\"yeojohnson\")")`

It is important to "unbranch" again after "branching", so that the outputs are merged into one result objects.

For this demo, we will use the `rank.` value that was the optimum in the last optimization.

```{r 05-pipelines-non-sequential-003, eval = TRUE}
rank_opt = instance$result_x_domain$pca.rank.

graph = po("branch", c("nop", "pca", "yeojohnson")) %>>%
  gunion(list(
    po("nop"),
    po("pca", rank. = rank_opt),
    po("removeconstants") %>>% po("yeojohnson")
  )) %>>% po("unbranch", c("nop", "pca", "yeojohnson"))
```

The resulting Graph looks as follows:

```{r 05-pipelines-non-sequential-004, fig.width = 11, eval = TRUE}
graph$plot(horizontal = TRUE)
```

The output of this Graph depends on the setting of the `branch.selection` hyperparameter:

```{r 05-pipelines-branch-01}
graph$param_set$values$branch.selection = "pca"  # use the "PCA" path
head(graph$train(mnist_task)[[1]]$feature_names)
graph$param_set$values$branch.selection = "nop"  # use the "No-Op" path
head(graph$train(mnist_task)[[1]]$feature_names)
```

Tuning this hyperparameter can be used to determine which of the possible options works best in combination with a given learner.
Branching can even be used to tune which of several learners is most appropriate for a given dataset.
We now extend this example so that both the preprocessing (PCA, Yeo-Johnson transform, or no preprocessing), as well as the model to use (KNN or decision tree) can be tuned.
For this, we add another branching pathway to our Graph:

```{r 05-pipelines-branch-02, eval = TRUE}
par(cex = 0.7)
graph_learner = graph %>>%
  po("branch", c("classif.rpart", "classif.kknn"), id = "branch2") %>>%
    gunion(list(
      lrn("classif.rpart"),
      lrn("classif.kknn", kernel = "rectangular")
    )) %>>%
  po("unbranch", c("classif.rpart", "classif.kknn"), id = "unbranch2")
graph_learner = as_learner(graph_learner)
graph_learner$graph$plot()
```

Note that it is necessary to give the two branching operations different IDs to avoid name clashes.

Finally, we would still like to tune over the `k` hyperparameter of the KNN learner, since it may again depend on the kind of preprocessing being done.
However, this hyperparameter is only active when the "`classif.kknn`" path is chosen.
We therefore have to declare a dependency in the search space.
The search space that tunes over all options of both branching operators as well as the KNN learner's `k` hyperparameter therefore looks like the following:


```{r 05-pipelines-branch-03}
search_space = ps(
  branch.selection = p_fct(c("nop", "pca", "yeojohnson")),
  branch2.selection = p_fct(c("classif.rpart", "classif.kknn")),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE,
    depends = branch2.selection == "classif.kknn")
)

# set seed for comparison with the alternative optimization method below
set.seed(1)
instance = tune(
  tuner = tnr("grid_search", resolution = 6, batch_size = 100),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  search_space = search_space
)

cbind(
  as.data.table(instance$result_x_domain),
  classif.ce = instance$result_y
)
```

Looking at the result, we see that the tuned KNN performs better than the untuned decision tree, "classif.rpart".
A more thorough investigation could try to tune the decision tree to check if its performance can be brought to the level of the KNN algorithm.
Increasing the number of options, however, increases the search space vastly.
The `grid_search` tuner, which we have used here because of its straightforward interpretability, is not recommended for search spaces that have more than a very small number of dimensions.
Therefore, if this investigation were to be carried further, one would need to use a different optimizer, such as random search or Bayesian optimization, the latter of which is demonstrated in @sec-bayesian-optimization.

```{r fig.align='center', fig.width = 7, fig.height = 3}
#| label: fig-pipelines-opttrace-2
#| fig-cap: "Observed performance values when optimizing both preprocessing (between \"pca\", \"yeojohnson\", and \"nop\", which is no preprocessing) and Learner (between KNN and the decision tree \"classif.rpart\") at the same time.
#|   The x-axis shows preprocessing. Each facet (i.e. individual plot) shows the Learner being used, together with setting for \"k\", if applicable."
#| fig-alt: "Plot showing performance values of pca, yeojohnson, or nop, followed by KNN or decision tree."
#| echo: false

ggplot(instance$archive$data[, learner := ifelse(is.na(classif.kknn.k), "classif.rpart", sprintf("classif.kknn\nk = % 2s", (sapply(x_domain, `[[`, "classif.kknn.k"))))],
  aes(x = branch.selection, y = classif.ce)) +
  geom_point() +
  facet_grid(cols = vars(learner)) +
  theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Performance of Various Learners with Different Preprocessing")
```

:::{.callout-tip}
Graphs with alternative path branching can also be created using `ppl()`, see @sec-pipelines-ppl
:::

### Tuning with `po("proxy")` {#sec-pipelines-proxy}

{{< include _optional.qmd >}}

The `r ref("PipeOpProxy", "po(\"proxy\")")` operator is a meta-operator that performs the operation that is stored in its `content` hyperparameter.
This can either be another PipeOp, or an entire Graph.
Having a PipeOp that can itself contain Graphs that can be tuned over makes it possible to optimize between different PipeOps, similarly to `PipeOpBranch` / `PipeOpUnbranch`.
@fig-pipelines-alternatives shows the conceptual difference between `PipeOpBranch` and `r ref("PipeOpProxy", "po(\"proxy\")")`.

```{r eval = TRUE}
#| label: fig-pipelines-alternatives
#| layout-nrow: 2
#| fig-cap: "
#|   Two ways of parametrizing the PipeOps or Learners that should be used.
#|   The setups shown in both examples have the same effect: Data is PCA-transformed before being fed to the learner.
#|   (a): Using `PipeOpBranch` it is possible to choose which one out of various alternative paths should be taken.
#|        In this example, the PCA PipeOp is active, the alternatives (Yeo-Johnson transform, or no operation through PipeOpNop) are inactive.
#|        A `PipeOpUnbranch` is necessary to mark the end of the alternative paths.
#|   (b): `PipeOpProxy` has the hyperparameter `content`, which can be set to a another PipeOp. In this example, it is set to PCA.
#| "
#| fig-alt:
#|   - "PipeOpBranch, followed by, alternatively, PipeOpNop, PCA, and Yeo-Johnson transform, which are all followed by PipeOpUnbranch and a Learner.
#|      The PCA branch is active."
#|   - PipeOpProxy, followed by a Learner. The content of the PipeOpProxy is set to a PCA PipeOp.
#| fig-subcap:
#|   - Usage of `PipeOpBranch`
#|   - Usage of `PipeOpProxy`
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/branching.svg")
knitr::include_graphics("Figures/proxy.svg")
```

To use `r ref("PipeOpProxy", "po(\"proxy\")")` instead of alternative path branching to perform the above optimization, one would first set up a Graph that contains `r ref("PipeOpProxy", "po(\"proxy\")")` operators as placeholders for the operations (preprocessing, learning) that should be tuned.
Note the different IDs to avoid a name clash.

```{r}
graph_learner = po("proxy", id = "preproc") %>>%
  po("proxy", id = "learner")
graph_learner = as_learner(graph_learner)
```

The tuning space for the `content` hyperparameters can now be set to a discrete set of the possibilities that should be tried out.
Using `r ref("p_fct", "paradox::p_fct()")` with a named list of PipeOps that should be inserted works here.
Internally, `r paradox` is creating a transformation function here, see @sec-defining-search-spaces for more details.
For the Learner-part, it is necessary to use a more complicated trafo-function, since here the Learner to use depends on more than one search space component.
This is defined here using `.extra_trafo`, which takes the values as generated by the search space as input `x`, and returns the value to be given to the Graph.
The help page of `r ref("ps", "ps()")` gives more details on this.
Inside this transformation, we need to clone the `learner.content` value before modifying it, since we would otherwise modify the original `Learner` object inside the search space by reference!
Observe how this optimization, when performed with the same seed, has the same result as above.

```{r}
search_space = ps(
  preproc.content = p_fct(list(
    nop = po("nop"),
    pca = po("pca", rank. = rank_opt),
    yeojohnson = po("removeconstants") %>>% po("yeojohnson")
  )),
  learner.content = p_fct(list(
    classif.rpart = lrn("classif.rpart"),
    classif.kknn = lrn("classif.kknn", kernel = "rectangular")
  )),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE,
    depends = learner.content == "classif.kknn"),
  .extra_trafo = function(x, param_set) {
    if (!is.null(x$classif.kknn.k)) {
      x$learner.content = x$learner.content$clone(deep = TRUE)
      x$learner.content$param_set$values$k = x$classif.kknn.k
      x$classif.kknn.k = NULL
    }
    x
  }
)

set.seed(1)  # for comparison with the optimization above
instance = tune(
  tuner = tnr("grid_search", resolution = 6, batch_size = 100),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  search_space = search_space
)

as.data.table(instance$result)[,
  .(preproc.content, learner.content,
    classif.kknn.k = x_domain[[1]]$learner.content$param_set$values$k,
    classif.ce)
]
```

##  Common Patterns and `ppl()` {#sec-pipelines-ppl}

{{< include _optional.qmd >}}

There are certain parts of Graphs that often occur in different contexts, but that could not reasonably be provided as single PipeOps by `mlr3pipelines`.
Examples for this were presented in the previous section:
patterns such as alternative paths or stacking are generally useful, but they reflect specific ways of connecting PipeOps instead of singular operations.

There are other commonly occurring problems that are usually solved by a combination of more than one PipeOp.
An example is converting data to make it compatible with a given Learner:
It is often necessary to impute missing values *and* to one-hot-encode categorical features, which are both provided as PipeOps.

We call these frequently needed building blocks "Graph elements".
They can be constructed using the `r ref("mlr_graphs")` `Dictionary`.
The best way to access these is the `ppl()` function, which takes a name as its first argument, followed by various other arguments specific to the element being constructed.
The help page for a Graph element with name `"<name>"` can be queried as `?ml_graphs_<name>`.

The following is a list of the provided Graph elements.
Their mandatory arguments are shown.
They also have optional arguments for fine adjustments which are described on their help page.

* **`r ref("mlr_graphs_robustify", "ppl(\"robustify\")")`**: Perform preprocessing that makes a given `r ref("Task")` compatible with a given `r ref("Learner")`.
  Optional arguments are the `Task` and `Learner` in question, as well as individual switches that decide which kind of preprocessing should be done.
  The "robustify" Graph element queries the metadata provided by ghe respective objects and does only the necessary preprocessing.
  E.g., if a given `Learner` has the `"missings"` property (i.e. supports missing values) but does not have `"factor"` in its `$feature_types` (i.e. can not handle categorical features), then `r ref("pipeline_robustify", "ppl(\"robustify\")")` will numerically encode categorical features, but will not do imputation.
* **`r ref("mlr_graphs_branch", "ppl(\"branch\", graphs)")`**: Alternative path branching, as described in @sec-pipelines-branch.
  The mandatory `graphs` argument must be a list of PipeOps or Graphs that should lie on the resulting alternative paths.
  The choice between PCA, Yeo-Johnson transform, and no-op that is shown in @sec-pipelines-branch can, for example, be produced by calling:
  ```{r, eval = FALSE}
  ppl("branch", graphs = pos(c("pca", "yeojohnson", "nop")))
  ```
* **`r ref("mlr_graphs_stacking", "ppl(\"stacking\", base_learners, super_learner)")`**: Stacking, as described in @sec-pipelines-stack. `base_learners` must be a list of learners that are used to augment the incoming data.
  The learner given to `super_learner` is then trained on the original data (unless the optional `use_features` is set to `FALSE`) and the predictions made by these learners.
  The example from @sec-pipelines-stack can thus be written:
  ```{r, eval = FALSE}
  ppl("stacking",
    base_learners = lrns("classif.rpart"),
    super_learner = lrn("classif.rpart")
  )
  ```
* **`r ref("mlr_graphs_bagging", "ppl(\"bagging\", graph)")`**: Bagging, as described in @sec-pipelines-bagging.
  `graph` can be a single learner, but it can also contain a more elaborate pipeline, e.g. involving preprocessing, as long as it produces a prediction at the end.
  Optional parameters control the number of bagging iterations (`iterations`, default 10), the fraction of samples for each bagging learner (`frac`, default 0.7), and the PipeOp doing the aggregation of predictions (`averager`, defaults to simple averaging).
  The bagging pipeline shown in @sec-pipelines-bagging is therefore constructed by calling
  ```{r, eval = FALSE}
  ppl("bagging", graph = lrn("classif.rpart"))
  ```
* **`r ref("mlr_graphs_greplicate", "ppl(\"greplicate\", graph, n)")`**: Create a Graph that contains `n` copies of `graph`.
  `graph` can be a Graph, but can also be a single PipeOp.
  `r ref("pipeline_greplicate", "ppl(\"greplicate\")")` in particular takes care of avoiding ID collisions by automatically adding a suffix to each PipeOp that counts up from 1.
  It is particularly useful when building bagging Graphs manually, and an example call is shown in @sec-pipelines-bagging.
* **`r ref("mlr_graphs_targettrafo", "ppl(\"targettrafo\", graph)")`**: Create a Graph that transforms the prediction target of a task.
  The problem with modifying the target column of a task is that a learner that is trained on this task will make predictions relative to the transformed scale.
  It is therefore necessary to perform an additional operation on the predictions made by such a learner which inverts the prediction, bringing them to the original scale.
  The `"targettrafo"` Graph element takes care of this.
  The `graph` argument should be the learner or pipeline that should be executed after the target was transformed, but before inversion.
  The transformation / inverter functions are set through the resulting Graph's `$targetmutate.trafo` and `$targetmutate.inverter` hyperparameters.

  The following is an example Graph that log-transforms the target before fitting a linear model, the predictions of which are later exponentiated.
  ```{r, eval = FALSE}
  gr = ppl("targettrafo", graph = lrn("regr.lm"))
  gr$param_set$values$targetmutate.trafo = function(x) log(x)
  gr$param_set$values$targetmutate.inverter = function(x) exp(x)
  ```
* **`r ref("mlr_graphs_ovr", "ppl(\"ovr\", graph)")`**: Do one-versus-rest classification.
  This graph element splits a single multiclass classification task into many binary classification tasks, one for each class in the original task.
  These tasks are then evaluated by the given `graph`, which should be a learner (or a pipeline containing a learner that emits a prediction).

  The predictions made on the binary tasks are then combined into the multiclass prediction needed for the original task.
  If possible, the `$predict_type` of the learner(s) in `graph` should be set to `"prob"`.

```{r pipelines-teardown, include = FALSE, cache = FALSE}
future::plan(.oldplan)
parallel::stopCluster(.cl)

```


## Recap

`r ref_pkg("mlr3pipelines")` provides `r ref("PipeOp")` objects that provide preprocessing, postprocessing, and ensembling operations that can be created using the `r ref("po", "po()")` constructor function.
PipeOps have an ID and hyperparameters that can be set during construction changed later.

PipeOps are concatenated using the `r ref("concat_graphs", "%>>%")`-operator to form `r ref("Graph")` objects.
Graphs can be converted to `r ref("Learner")` objects using `r ref("as_learner")`, after which they can be resampled, benchmarked, and tuned using the tools provided by `r ref_pkg("mlr3")`.
Various standard Graphs are provided by the `r ref("ppl", "ppl()")` constructor function.

## Exercises

1. Create a learner containing a Graph that first imputes missing values using `r ref("PipeOpImputeOOR", "po(\"imputeoor\")")`, standardizes the data using `r ref("PipeOpScale", "po(\"scale\")")`, and then fits a logistic linear model using `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")`.
2. Train the Graph from the previous exercise on the `r ref("mlr_tasks_pima", "tsk(\"pima\")")` task and display the coefficients of the resulting model.
  What are two different ways to access the model?
3. You want to verify that the "`age`" column of the input task to `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` in the previous exercise is indeed standardized.
  One way to do this would be to look at the `$data` field of the `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` model; however, that is specific to that particular learner and does not work in general.
  What would be a different, more general way to do this?
  Hint: use the `$keep_results` flag.
4. Consider the `r ref("PipeOpSelect", "po(\"select\")")` in @sec-pipelines-stack that is used to only keep the columns ending in "`M`".
  If the classification task had more than two classes, it would be more appropriate to list the single class we *do not* want to keep, instead of listing all the classes we do want to keep.
  How would you do this, using the `r ref("Selector")` functions provided by `r ref_pkg("mlr3pipelines")`?
  (Note: The `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` learner used in @sec-pipelines-stack is not able to handle more than two classes, so you would need to use a different learner, such as `r ref("LearnerClassifMultinom", "lrn(\"classif.multinom\")")`, to built the entire stack.)
5. How would you solve the previous exercise without even explicitly naming the class you want to exclude, so that your Graph works for any classification task?
  Hint: look at the `selector_subsample` in @sec-pipelines-bagging.
6. Use the `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` PipeOp to impute missing values in the `r ref("mlr_tasks_penguins", "tsk(\"penguins\")")` task using `r ref("ranger::ranger")`-based learners.
  Hint 1: you will need to use `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` twice, once for numeric features with `r ref("LearnerRegrRanger", "lrn(\"regr.ranger\")")`, and once for categorical features with `r ref("LearnerClassifRanger", "lrn(\"classif.ranger\")")`.
  Using the `affect_columns` argument of `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` will help you here.
  Hint 2: `r ref("ranger::ranger")` itself does not support missing values, but it is trained on all the features of `r ref("mlr_tasks_penguins", "tsk(\"penguins\")")` that it is not currently imputing, some of which will also contain missings.
  A simple way to avoid problems here is to use `r ref("pipeline_robustify", "ppl(\"robustify\")")` *inside* `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` next to the `r ref("ranger::ranger")` learner.
