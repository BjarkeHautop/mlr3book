---
author:
  - name: Lennart Schneider
    orcid: 0000-0003-4152-5308
    email: lennart.schneider@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
  Many optimization problems are a black box, i.e., the only observable information is the output value of the function given an input value.
  A prime example is given by hyperparameter optimization where we configure a learner with a hyperparameter configuration and estimate its generalization performance via resampling.
  Moreover, hyperparameter optimization can be extremely expensive and therefore calls for very sample efficient optimization algorithms.
  This chapter presents an introduction to Bayesian optimization (also called model-based optimization) within the mlr3 ecosystem.
  We start with the general black box optimization setting and proceed to introduce the building blocks of Bayesian optimization - a class of sample efficient iterative global optimization algorithms.
  After that we show that Bayesian optimization can be used out of the box for hyperparameter optimization of machine learning algorithms.
  We further show how multi-objective Bayesian optimization can be performed and briefly discuss noisy objective functions.
  The chapter closes with comments on robustness, safety nets, defaults and practical considerations regarding Bayesian optimization algorithms.
---

# Bayesian Optimization {#sec-bayesian-optimization}

```{r}
set.seed(1005)
```

{{< include _setup.qmd >}}

Black-box optimization considers the optimization of a so-called black box function whose structure and analytical description is unknown, unexploitable or non-existent.
As a result, the only observable information is the output value of the function given an input value.
Usually, the objective function is subject so some box constraint, i.e., in the case of numeric variables the domain is assumed to be a hyperrectangle.
Examples for black box optimization problems range from real-life experiments, e.g., crash tests or chemical reactions to computer simulations, e.g., the design of a helicopter rotor blade.

Hyperparameter optimization (HPO, see @sec-optimization) is a prime example for black box optimization.
Recall that in HPO, we configure a learner with a hyperparameter configuration and evaluate the learner via a resampling technique to measure its performance with the goal to find the optimal hyperparameter configuration.
In general no analytical description for this mapping from a hyperparameter configuration to performance exists and gradient information is not available.
Besides, evaluating the performance of a learner can take a substantial amount of time, making HPO an expensive black box optimization problem.

Many algorithm classes exists that can be used for black box optimization that differ in how they tackle this optimization problem.
In general, most black box optimizers work iterative, i.e., they propose new points for evaluation by making use of the information collected during the evaluation of previous points.
Evolutionary strategies for example maintain a so-called population and generate new points for evaluation by choosing parents from that population and performing recombination operators such as mutation and crossover to generate the offspring of the next generation.
In general, evolutionary strategies are very suited for black box optimization, however, if the overhead of the evaluation of the black box function becomes large (e.g., as in HPO), sample efficiency of an optimizer becomes highly relevant.
Bayesian optimization (BO) describes a class of sample efficient iterative global black box optimization algorithms that make use of a so-called surrogate trained on observed data modelling the black box function.
Based on this (non-linear) regression model and a so-called acquisition function quantifying the attractiveness of each point in the search space, the next candidate point is obtained that is to be evaluated next.
After evaluating this candidate, the surrogate model is updated or re-trained and the process repeats itself until a termination criteria is met.
Often, using BO results in very good optimization performance, especially if the overhead of the black box evaluation becomes large and optimization budget is tight.

This chapter is of general relevance to users concerned with sample efficient black box optimization and HPO.
In the following, we will give a brief general introduction to black box optimization making use of the `r bbotk`\index{bbotk} package.
We then show how BO can be performed within the mlr3 ecosystem making use of the `r mlr3mbo`\index{mlr3mbo} package.
Detailed introductions to black box optimization and BO are given in @hpo_practical, @hpo_automl and @garnett_2022.

## Black-Box Optimization {#sec-black-box-optimization}

The `r bbotk` package is the workhorse package for black box optimization within the mlr3 ecosystem.
At the heart of the package are the R6 classes

* `r ref("OptimInstanceSingleCrit")` and `r ref("OptimInstanceMultiCrit")`, which are used to construct an optimization 'instance' which describes the optimization problem and stores the results; and
* `r ref("Optimizer")` which is used to get and set optimization algorithms.

This should sound very familiar.
Indeed, in @sec-optimization, the classes `r ref("TuningInstanceSingleCrit")`, `r ref("TuningInstanceMultiCrit")` and `r ref("Tuner")` were already introduced.
Those three R6 classes essentially build upon `r ref("OptimInstanceSingleCrit")`, `r ref("OptimInstanceMultiCrit")` and `r ref("Optimizer")` which are the more general black box optimization base classes.

Our running example will be to optimize the following sinusoidal function which is characterized by two local minima and one global minimum:
$f: [0, 1] \rightarrow \mathbb{R}, x \mapsto 2x + \sin(14x)$.
The local optima can pose a risk for many optimization algorithms to get stuck.

At the core of an `r ref("OptimInstanceSingleCrit")` lies an `r ref("Objective")` function wrapping the function, domain and codomain.
Objective functions can be constructed using the classes `r ref("ObjectiveRFun")` (wraps a custom R function that expects a list as input), `r ref("ObjectiveRFunMany")` (wraps a custom R function that expects a list of configurations as input) or `r ref("ObjectiveRFunDt")` (wraps a custom R function that works on a `r ref("data.table")`).
In the following, we will use `r ref("ObjectiveRFunDt")`:

```{r}
sinus_1D = function(xdt) {
  y = 2 * xdt$x * sin(14 * xdt$x)
  data.table(y = y)
}
```

An `r ref("Objective")` always requires the specification of the domain and codomain in the form of a `r ref("ParamSet")`:

```{r}
domain = ps(x = p_dbl(lower = 0, upper = 1))
codomain = ps(y = p_dbl(tags = "minimize"))
objective = ObjectiveRFunDt$new(sinus_1D,
  domain = domain, codomain = codomain)
```

We can proceed to visualize the sinusoidal function by generating a grid of points on which we evaluate the function:

```{r}
xydt = generate_design_grid(domain, resolution = 1001)$data
xydt[, y := objective$eval_dt(xydt)$y]
```

```{r}
#| echo: true
#| label: fig-bayesian-optimization-sinusoidal
#| fig-cap: Visualization of the sinusoidal function.
#| fig-alt: FIXME
library(ggplot2)
ggplot(aes(x = x, y = y), data = xydt) +
  geom_line() +
  theme_minimal()
```

The global minimum is located at `x = 0.7918238`:

```{r}
sinus_1D(data.table(x = 0.7918238))
```

By wrapping the objective function in an `r ref("OptimInstanceSingleCrit")` we can proceed to optimize it, e.g., via a simple random search.

```{r, output=FALSE}
instance = OptimInstanceSingleCrit$new(objective,
  terminator = trm("evals", n_evals = 20))
optimizer = opt("random_search", batch_size = 20)
optimizer$optimize(instance)
```

```{r}
instance$archive$best()
```

To see all available optimizers you can inspect the following dictionary:

```{r}
as.data.table(mlr_optimizers)
```

Note that evolutionary strategies are available within the mlr3 ecosystem via the `miesmuschel` package which we will not cover in this chapter.

## Building Blocks of Bayesian Optimization {#sec-bayesian-optimization-blocks}

Bayesian optimization (BO) is an iterative optimization algorithm that makes use of a so-called surrogate to model the unknown black box function.
After having observed an initial design of observations, the surrogate model is trained on all data points observed so far and a so-called acquisition function is used to determine which point of the search space is a promising candidate that should be evaluated next.
The acquisition function relies on the predictions of the surrogate model and requires no evaluation of the true black box function and therefore is comparably cheap to optimize.
The acquisition function should balance exploration and exploitation of the BO algorithm, i.e., we want to exploit knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty but also want to make sure that we do not miss crucial regions and therefore also want to explore into regions where the uncertainty of the surrogate model is high.
After having evaluated the next candidate, the process repeats itself until a given termination criteria is met.

Most BO flavors therefore follow a simple loop:

    1. Fit the surrogate model on all observations made so far.
    2. Optimize the acquisition function to find the next candidate that should be evaluated.
    3. Evaluate the next candidate.

Note that we speak of BO flavors, as BO is highly modular, i.e., users can choose different types of surrogate models, acquisition functions and acquisition function optimizers to their liking.
`r mlr3mbo` makes BO available within the mlr3 ecosystem.
At the heart of the package are the two R6 classes `r ref("OptimizerMbo")` and `r ref("TunerMbo")` which can be configured with respect to their `r ref("loop_function")` (determining the general loop structure of the BO algorithm), `r ref("Surrogate")` (surrogate model), `r ref("AcqFunction")` (acquisition function) and `r ref("AcqOptimizer")` (acquisition function optimizer).

### The Initial Design

Before we can fit a surrogate model, we need data.
The initial design refers of the first set of points the black box function was evaluated on before BO algorithms start their loop.

`r mlr3mbo` offers two different ways for specifying an initial design:

    1. One can simply evaluate points on the `OptimInstance` that is to be optimized prior to using an `OptimizerMbo`. In this case, the `loop_function` should skip the construction and evaluation of an initial design.
    2. If no points were already evaluated on the `OptimInstance`, the `loop_function` should construct an initial design itself and evaluate it.

Functions for creating different initial designs are part of the `r paradox` package, e.g.:

1. `r ref("generate_design_random")`: uniformly at random
1. `r ref("generate_design_grid")`: uniform sized grid
1. `r ref("generate_design_lhs")`: Latin hypercube sampling
1. `r ref("generate_design_sobol")`: Sobol sequence

For illustrative purposes we will briefly compare these samplers on a two dimensional domain:
```{r}
sample_domain = ps(x1 = p_dbl(lower = 0, upper = 1), x2 = p_dbl(lower = 0, upper = 1))
random_design = generate_design_random(sample_domain, n = 9)$data
grid_design = generate_design_grid(sample_domain, resolution = 3)$data
lhs_design = generate_design_lhs(sample_domain, n = 9)$data
sobol_design = generate_design_sobol(sample_domain, n = 9)$data
```

```{r, echo=FALSE}
#| echo: FALSE
#| label: fig-bayesian-optimization-designs
#| fig-cap: Comparing different samplers for constructing an initial design of 9 points on a domain of two numeric variables ranging from 0 to 1. Dotted horizontal and vertical lines partition the domain into equally sized bins. Histograms on the top and right visualize the marginal distributions of the generated sample.
#| fig-alt: FIXME
library(ggplot2)
library(ggExtra)
library(gridExtra)

qs = seq(from = 0, to = 1, length.out = 10)

g_random = ggplot(aes(x = x1, y = x2), data = random_design) +
  geom_point(size = 3L) +
  geom_vline(xintercept = qs, linetype = 2) +
  geom_hline(yintercept = qs, linetype = 2) +
  labs(title = "Random Design", x = expression(x[1]), y = expression(x[2])) +
  theme_minimal() +
  xlim(c(0, 1)) +
  ylim(c(0, 1))
g_random = ggMarginal(g_random, type = "histogram", bins = 10)

g_grid = ggplot(aes(x = x1, y = x2), data = grid_design) +
  geom_point(size = 3L) +
  geom_vline(xintercept = qs, linetype = 2) +
  geom_hline(yintercept = qs, linetype = 2) +
  labs(title = "Grid Design", x = expression(x[1]), y = expression(x[2])) +
  theme_minimal() +
  xlim(c(0, 1)) +
  ylim(c(0, 1))
g_grid = ggMarginal(g_grid, type = "histogram", bins = 10)

g_lhs = ggplot(aes(x = x1, y = x2), data = lhs_design) +
  geom_point(size = 3L) +
  geom_vline(xintercept = qs, linetype = 2) +
  geom_hline(yintercept = qs, linetype = 2) +
  labs(title = "LHS Design", x = expression(x[1]), y = expression(x[2])) +
  theme_minimal() +
  xlim(c(0, 1)) +
  ylim(c(0, 1))
g_lhs = ggMarginal(g_lhs, type = "histogram", bins = 10)

g_sobol = ggplot(aes(x = x1, y = x2), data = sobol_design) +
  geom_point(size = 3L) +
  geom_vline(xintercept = qs, linetype = 2) +
  geom_hline(yintercept = qs, linetype = 2) +
  labs(title = "Sobol Design", x = expression(x[1]), y = expression(x[2])) +
  theme_minimal() +
  xlim(c(0, 1)) +
  ylim(c(0, 1))
g_sobol = ggMarginal(g_sobol, type = "histogram", bins = 10)

grid.arrange(g_random, g_grid, g_lhs, g_sobol, nrow = 2, ncol = 2)
```

We observe that a random design does not cover the domain well and simply by chance samples points close to each other.
A grid design will always result in points being marginally equidistant from their nearest neighbour and does not cover the domain well.
In contrast, an LHS design will result in good coverage of the domain and even for a small number of samples the marginal distributions will be perfectly uniform (i.e., in the plot, in each row and column there is exactly one sample).
A Sobol design has a similar goal in mind but must not guarantee this for a small number of samples.
However, constructing a Sobol design can be done much more efficiently than an LHS design, especially if the number of samples and dimensions grows.
Moreover, a Sobol design has better coverage properties than an LHS design if the number of dimensions grows large.

Of course, one can always evaluate a custom initial design in the form of a `data.table`.
Coming back to our running example, we will now evaluate the following custom design:

```{r}
instance$archive$clear()
design = data.table(x = c(0.1, 0.3, 0.65, 1))
instance$eval_batch(design)
```

### Loop Function

The `r ref("loop_function")` determines the behavior of the BO algorithm on a global level, i.e., how the subroutine should look like that is performed at each iteration.

To get an overview of readily available loop functions, the following dictionary can be inspected:

```{r}
as.data.table(mlr_loop_functions)
```

Technically, all `loop_function`s are members of the `S3` class `r ref("loop_function")` (and are actually simply `r ref("base::function", "functions")` with some attributes set).

For sequential single-objective black box optimization, the Efficient Global Optimization (EGO) algorithm [@jones_1998] can be considered the reference algorithm.
In `r mlr3mbo`, the EGO algorithm is implemented via the `r ref("mlr_loop_functions_ego", "bayesopt_ego")` loop function.
After having made some assertions and safety checks, and having evaluated the initial design, `bayesopt_ego` essentially repeatedly performs the following steps:

1. `acq_function$surrogate$update()`: update the surrogate model
2. `acq_function$update()`: update the acquisition function
3. `acq_optimizer$optimize()`: optimize the acquisition function to yield a new candidate

### Surrogate

A surrogate encapsulates a regression learner that models the unknown black box function based on observed data.
In `r mlr3mbo`, `r ref("SurrogateLearner")` and `r ref("SurrogateLearnerCollection")` are the higher-level R6 classes which should be used to construct a surrogate, inheriting from the base `r ref("Surrogate")` class.

As a learner, any `r ref("LearnerRegr", "regression learner")` from `r "mlr3"` can be used, however, most acquisition functions require both a mean and a variance prediction (therefore not all learners are suitable for all scenarios).
Typical choices include:

* A `r ref("mlr3learners::mlr_learners_regr.km", "Gaussian Process")` for low dimensional numeric search spaces
* A `r ref("mlr3learners::mlr_learners_regr.ranger", "Random Forest")` for higher dimensional mixed (and / or hierarchical) search spaces

We will not dive into details regarding Gaussian Processes here.
A detailed introduction to Gaussian Processes can be found in @williams_2006 and in-depth focus to Gaussian Processes in the context of surrogate models in BO is given in @garnett_2022.

A `SurrogateLearner` can be constructed via:

```{r}
surrogate = srlrn(lrn("regr.km", covtype = "matern5_2", optim.method = "BFGS", control = list(trace = FALSE)))
```
Here, we use a Gaussian Process with Matérn 5/2 kernel, optimizing the marginal likelihood via BFGS and set `trace = FALSE` to prevent too much output during fitting.

When using a `Surrogate` interactively, i.e., outside of an `OptimizerMbo` or `TunerMbo`, the `archive` of the instance must be specified:

```{r}
surrogate = srlrn(lrn("regr.km", covtype = "matern5_2", optim.method = "BFGS", control = list(trace = FALSE)), archive = instance$archive)
```

The encapsulated learner can be accessed via the `$learner` field:

```{r}
surrogate$learner
```

Internally, the learner is fitted on a `r ref("mlr3::TaskRegr", "regression task")` constructed from the `r ref("bbotk::Archive", "Archive")` of the `r ref("bbotk::OptimInstance", "OptimInstance")` that is to be optimized.
Features are given by the variables of the domain whereas the target is given by the variable of the codomain.
Depending on the choice of the loop function, multiple targets must be modelled by multiple surrogates, in which case a `r ref("SurrogateLearnerCollection")` should be used, see, e.g., @sec-multi-objective-bayesian-optimization.

Updating the surrogate results in the fields of the `$learner` being populated as expected:

```{r}
surrogate$update()
surrogate$learner
surrogate$learner$model
surrogate$learner$state$train_task
```

The following figures visualize the mean and variance prediction of the surrogate in light-blue (@fig-bayesian-optimization-mean-variance).
Note that when using a Gaussian Process that interpolates the training data, the variance prediction is zero for training data.
The initial design is given by the black points and the true sinusoidal function by the black line.

```{r}
xydt[, c("mean", "variance") := surrogate$predict(xydt[, surrogate$x_cols, with = FALSE])]
```

```{r}
#| echo: true
#| label: fig-bayesian-optimization-mean-variance
#| fig-cap: Mean and variance prediction of the Gaussian Process surrogate model trained on an initial design of four points (black). Ribbons represent the mean plus minus the variance prediction.
#| fig-alt: FIXME
ggplot() +
  geom_point(aes(x = x, y = y), size = 2, data = instance$archive$data) +
  geom_line(aes(x = x, y = y), data = xydt) +
  geom_line(aes(x = x, y = mean), colour = "steelblue", linetype = 2, data = xydt) +
  geom_ribbon(aes(x = x, min = mean - variance, max = mean + variance), fill = "steelblue", colour = NA, alpha = 0.1, data = xydt) +
  theme_minimal()
```

### Acquisition Function

Based on a surrogate, an acquisition function quantifies the attractiveness of each point of the search space if it were to be evaluated in the next iteration.

A popular example is given by the Expected Improvement [@jones_1998]:

$$
\mathbb{E} \left[ \max \left( f_{\mathrm{min}} - Y(\mathbf{x}), 0 \right) \right]
$$
Here, $Y(\mathbf{x)}$ is the surrogate prediction (a random variable) for a given point $\mathbf{x}$ (which when using a Gaussian Process follows a normal distribution) and $f_{\mathrm{min}}$ is the currently best function value observed so far (when assuming minimization).

To get an overview of available acquisition functions, the following dictionary can be inspected:

```{r}
as.data.table(mlr_acqfunctions)
```

Technically, all acquisition functions inherit from the `R6` class `r ref( "mlr3mbo::AcqFunction", "AcqFunction")` which itself simply inherits from the base `r ref("bbotk::Objective", "Objective")` class.

Construction is straightforward via:

```{r}
acq_function = acqf("ei")
```

When working interactively the `surrogate` on which the acquisition function operates on must be specified:

```{r}
acq_function = acqf("ei", surrogate = surrogate)
```

We now want to use the Expected Improvement to choose the next candidate for evaluation:

```{r}
acq_function$update()
xydt[, "ei" := acq_function$eval_dt(xydt[, surrogate$x_cols, with = FALSE])]
```

@fig-bayesian-optimization-ei shows that the Expected Improvement is high in regions where the mean prediction of the Gaussian Process is low but the variance prediction suggests uncertainty:

```{r}
#| echo: true
#| label: fig-bayesian-optimization-ei
#| fig-cap: Expected Improvement based on the mean and variance prediction of the Gaussian Process surrogate model.
#| fig-alt: FIXME
ggplot(aes(x = x, y = ei), data = xydt) +
  geom_line() +
  labs(y = "Expected Improvement") +
  theme_minimal()
```

### Acquisition Function Optimizer

To find the most promising next candidate for evaluation, the acquisition function itself must be optimized.
Internally, an `r ref("OptimInstance")` is constructed using the acquisition function as an `r ref("bbotk::Objective", "Objective")`.

An acquisition function optimizer is then used to solve this optimization problem.
Technically, this optimizer is a member of the `r ref("mlr3mbo::AcqOptimizer", "AcqOptimizer")` `R6` class.

Construction requires specifying an `r ref("bbotk::Optimizer", "Optimizer")` as well as a `r ref( "bbotk::Terminator", "Terminator")`:

```{r}
acq_optimizer = acqo(opt("nloptr", algorithm = "NLOPT_GN_ORIG_DIRECT"),
  terminator = trm("stagnation", iters = 100, threshold = 1e-5))
```

When working interactively, the `acq_function` must be specified as well:

```{r}
acq_optimizer = acqo(opt("nloptr", algorithm = "NLOPT_GN_ORIG_DIRECT"),
  terminator = trm("stagnation", iters = 100, threshold = 1e-5),
  acq_function = acq_function)
```

In this example we use the DIRECT algorithm provided by the `nloptr` package to optimize the Expected Improvement.
We will terminate the acquisition function optimization if we no longer improve at least `1e-5` over a history of `100` iterations.

```{r}
candidate = acq_optimizer$optimize()
candidate
```

@fig-bayesian-optimization-ei-argmax visualizes the location of the next candidate found by maximizing the Expected Improvement.

```{r}
#| echo: true
#| label: fig-bayesian-optimization-ei-argmax
#| fig-cap: Next candidate point (darkred) obtained by maximizing the Expected Improvement based on the mean and variance prediction of the Gaussian Process surrogate model.
#| fig-alt: FIXME
ggplot(aes(x = x, y = ei), data = xydt) +
  geom_line() +
  geom_point(aes(x = x, y = acq_ei), colour = "darkred", size = 2, data = candidate) +
  labs(y = "Expected Improvement") +
  theme_minimal()
```

We proceed to evaluate the candidate and would then continue with the next iteration of the loop.

```{r}
instance$eval_batch(candidate)
```

## Bayesian Optimization for Black-Box Optimization {#sec-bayesian-black-box-optimization}

Of course, users usually do not want to perform all steps of the BO loop manually.
Instead one can simply construct an `r ref("OptimizerMbo")` and use it to optimize the instance.
Note that when not using `r mlr3mbo` interactively, passing arguments such as `archive` to the `surrogate` is not required:

```{r}
surrogate = srlrn(lrn("regr.km", covtype = "matern5_2", optim.method = "BFGS", control = list(trace = FALSE)))
acq_function = acqf("ei")
acq_optimizer = acqo(opt("nloptr", algorithm = "NLOPT_GN_ORIG_DIRECT"),
  terminator = trm("stagnation", iters = 100, threshold = 1e-5))
optimizer = opt("mbo",
  loop_function = bayesopt_ego,
  surrogate = surrogate,
  acq_function = acq_function,
  acq_optimizer = acq_optimizer)
```

```{r, output=FALSE}
instance$archive$clear()
instance$eval_batch(design)
optimizer$optimize(instance)
```

```{r}
instance$archive$best()
```

We see that BO is much more sample efficient and comes close to the true global optimum using few function evaluations.

Visualizing the sequential decision making process of the BO algorithm (i.e., the sampling trajectory of points) shows that focus is given more and more to regions around the (as we know) global optimum (@fig-bayesian-optimization-sampling).
Nevertheless, even in later optimization stages, exploration is performed, illustrating that the Expected Improvement indeed balances exploration and exploitation.

```{r}
#| echo: true
#| label: fig-bayesian-optimization-sampling
#| fig-cap: Sampling trajectory of the BO algorithm. Points of the initial design in black. Points that were evaluated in later stages of the BO process are coloured in a lighter red.
#| fig-alt: FIXME
ggplot() +
  geom_line(aes(x = x, y = y), data = xydt) +
  geom_point(aes(x = x, y = y), colour = "black", size = 2, data = instance$archive$data[batch_nr == 1]) +
  geom_point(aes(x = x, y = y, colour = batch_nr), size = 2, data = instance$archive$data[batch_nr > 1]) +
  scale_color_gradient(low = "darkred", high = "lavenderblush") +
  labs(colour = "Batch Nr.") +
  theme_minimal()
```

If we replicate running BO ten times (with random initial designs and varying random seeds) and compare this to a random search, we can see that BO indeed performs much better and reaches the global optimum after around 15 function evaluations (@fig-bayesian-sinusoidal_bo_rs).

```{r, eval=FALSE, echo=FALSE}
library(mlr3misc)
library(pammtools)
set.seed(2906)
results = map_dtr(1:10, function(replication) {
  instance$archive$clear()
  optimizer$optimize(instance)
  bo_result = instance$archive$data
  bo_result[, nr_eval := seq_len(.N)]
  bo_result[, y_min := cummin(y)]
  bo_result[, method := "BO"]

  instance$archive$clear()
  opt("random_search", batch_size = 20)$optimize(instance)
  rs_result = instance$archive$data
  rs_result[, nr_eval := seq_len(.N)]
  rs_result[, y_min := cummin(y)]
  rs_result[, method := "RS"]

  result = rbind(bo_result, rs_result, fill = TRUE)
  result[, replication := replication]
  result
})
mean_results = results[, .(mean_y_min = mean(y_min), se_y_min = sd(y_min) / sqrt(.N)), by = .(method, nr_eval)]
g = ggplot() +
  geom_step(aes(x = nr_eval, y = mean_y_min, colour = method), data = mean_results) +
  geom_stepribbon(aes(x = nr_eval, min = mean_y_min - se_y_min, max = mean_y_min + se_y_min, fill = method), colour = NA, alpha = 0.1, data = mean_results) +
  labs(y = "Best Observed Function Value", x = "Nr. Function Evaluations", fill = "Method", colour = "Method") +
  theme_minimal() +
  theme(legend.position = "bottom")
ggsave("Figures/bo_1d_sinusoidal_bo_rs.pdf", plot = g, width = 6, height = 4)
ggsave("Figures/bo_1d_sinusoidal_bo_rs.png", plot = g, width = 6, height = 4)
```

```{r, echo=FALSE}
#| echo: false
#| label: fig-bayesian-sinusoidal_bo_rs
#| fig-cap: Anytime performance of BO and random search (RS) on the 1D sinusoidal function given a budget of 20 function evaluations. Solid line depicts the best observed target value averaged over 10 replications. Ribbons represent standard errors.
#| fig-alt: FIXME
knitr::include_graphics("Figures/bo_1d_sinusoidal_bo_rs.png")
```

## Bayesian Optimization for Hyperparameter Optimization {#sec-bayesian-tuning}

`r mlr3mbo` can be used out of the box for HPO (tuning) within the mlr3 ecosystem using a `r ref("TunerMbo")`.
For illustrative purposes, we revisit the tuning example of @sec-tuning-instance and perform BO instead of a grid search.

First, we construct the tuning instance:

```{r}
resampling = rsmp("cv", folds = 3)

measure = msr("classif.ce")

learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 25)
)
```

We can then simply construct an `r ref("TunerMbo")` and use it to optimize the instance.
Note that `r ref("TunerMbo")` is simply a light-weight wrapper around `r ref("OptimizerMbo")`.

```{r}
tuner = tnr("mbo",
  loop_function = bayesopt_ego,
  surrogate = surrogate,
  acq_function = acq_function,
  acq_optimizer = acq_optimizer)
```

```{r, output=FALSE}
tuner$optimize(instance)
```

```{r}
instance$result
```

We see that BO finds a substantially better hyperparameter configuration than a grid or random search (using the same budget):

```{r, output=FALSE}
instance$archive$clear()
tuner = tnr("grid_search", resolution = 5)
tuner$optimize(instance)
```

```{r}
instance$result
```

```{r}
instance$archive$clear()
tuner = tnr("random_search", batch_size = 25)
tuner$optimize(instance)
```

```{r, output=FALSE}
instance$result
```

## Multi-Objective Bayesian Optimization {#sec-multi-objective-bayesian-optimization}

BO cannot only be used to optimize single-objective black box functions but also multi-objective black box functions (recall that we already introduced multi-objective optimization in @sec-multi-metrics-tuning).
Multi-Objective BO algorithms can differ in many design choices, for example whether they use a scalarization approach of objectives and only rely on a single surrogate model or fit a surrogate model for each objective.
More details on multi-objective BO can for example be found in @hpo_multi.

We will now illustrate how ParEGO [@knowles_2006] can be used for multi-objective HPO and revisit the example of @sec-multi-metrics-tuning:

```{r}
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit = to_tune(2, 128, logscale = TRUE),
  maxdepth = to_tune(1, 30)
)

measures = msrs(c("classif.ce", "selected_features"))

instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = measures,
  terminator = trm("evals", n_evals = 20),
  store_models = TRUE  # required to inspect selected_features
)
```

ParEGO (implemented via the `r ref("mlr_loop_functions_parego", "bayesopt_parego")` loop function) tackles multi-objective BO via a scalarization approach and models a single scalarized objective function via a single surrogate model and then proceeds to find the next candidate for evaluation making use of a standard single-objective acquisition function such as the Expected Improvement:

```{r}
surrogate = srlrn(lrn("regr.km", covtype = "matern5_2", optim.method = "BFGS", control = list(trace = FALSE)))
acq_function = acqf("ei")
acq_optimizer = acqo(opt("random_search", batch_size = 1000),
  terminator = trm("evals", n_evals = 1000))

tuner = tnr("mbo",
  loop_function = bayesopt_parego,
  surrogate = surrogate,
  acq_function = acq_function,
  acq_optimizer = acq_optimizer)
```

```{r, output=FALSE}
tuner$optimize(instance)
```

```{r}
#| echo: true
#| label: fig-pareto
#| fig-cap: Pareto front of selected features and classification error obtained via ParEGO. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.
#| fig-alt: FIXME
library(viridisLite)
ggplot(as.data.table(instance$archive), aes(x = selected_features, y = classif.ce)) +
  geom_point(
    data = ,
    shape = 21,
    size = 3,
    fill = viridis(3, end = 0.8)[1],
    alpha = 0.8,
    stroke = 0.5) +
  geom_step(
    data = instance$archive$best(),
    direction = "vh",
    colour = viridis(3, end = 0.8)[2],
    linewidth = 1) +
  geom_point(
    data = instance$archive$best(),
    shape = 21,
    size = 3,
    fill = viridis(3, end = 0.8)[2],
    alpha = 0.8,
    stroke = 0.5) +
  theme_minimal()
```

An alternative approach to multi-objective BO would be to model each objective function via a separate surrogate model and finding the next candidate for evaluation via a so-called aggregating acquisition function.
An example for this approach is given by the SMS-EGO algorithm [@ponweiser_2008] (implemented via the `r ref("mlr_loop_functions_smsego", "bayesopt_smsego")` loop function).

Using SMS-EGO for the multi-objective tuning problem above would look like the following:

```{r}
  instance$archive$clear()
```

```{r}
surrogate = srlrnc(list(lrn("regr.km", covtype = "matern5_2", optim.method = "BFGS", control = list(trace = FALSE)), lrn("regr.km", covtype = "matern5_2", optim.method = "BFGS", control = list(trace = FALSE))))
acq_function = acqf("smsego")
acq_optimizer = acqo(opt("random_search", batch_size = 1000),
  terminator = trm("evals", n_evals = 1000))

tuner = tnr("mbo",
  loop_function = bayesopt_smsego,
  surrogate = surrogate,
  acq_function = acq_function,
  acq_optimizer = acq_optimizer)
```

## Noisy Bayesian Optimization {#sec-noisy-bayesian-optimization}

So far, we implicitly assumed that the black box function we are trying to optimize is deterministic, i.e., repeatedly evaluating the same point will always returns the same objective function value.
Real world black box functions, however, are often noisy, i.e., the true signal of the black box function is augmented by some noise and repeatedly evaluating the same point will return different objective function values.
In `r bbotk`, noisiness of an objective function can be indicated via the `properties` argument.
This allows users to programmatically treat such objectives differently.

```{r}
sinus_1D_noisy = function(xdt) {
  y = 2 * xdt$x * sin(14 * xdt$x) + rnorm(nrow(xdt), mean = 0, sd = 0.1)
  data.table(y = y)
}
domain = ps(x = p_dbl(lower = 0, upper = 1))
codomain = ps(y = p_dbl(tags = "minimize"))
objective = ObjectiveRFunDt$new(sinus_1D_noisy,
  domain = domain, codomain = codomain, properties = "noisy")
objective$properties
```

`r mlr3mbo` allows for several ways how noisiness of objectives can be respected during BO:

1. A surrogate can be used that can model noisiness of observations
2. An acquisition function can be used that properly respects noisiness
3. The final best point(s) after optimization (i.e., the `$result` field of the instance) can be chosen in a way to reflect noisiness

For example, instead of using an interpolating Gaussian Process, Gaussian Process regression that estimates the measurement error can be used:

```{r, output=FALSE}
srlrn(lrn("regr.km", nugget.estim = TRUE))
```

This will result in the Gaussian Process not perfectly interpolating training data and variance being non zero for training data.
A more in-depth discussion of noise free vs. noisy observations in the context of Gaussian Processes can be found in @williams_2006.

An example for an acquisition function that properly respects noisiness of observations is given by the Augmented Expected Improvement [@huang_2012] which essentially rescales the Expected Improvement taking measurement error into account:

```{r, output=FALSE}
acqf("aei")
```

Finally, `r mlr3mbo` allows for explicitly specifying how the final result after optimization is assigned to the instance (i.e. what will be written to `instance$result`) via so-called result assigners.

```{r}
as.data.table(mlr_result_assigners)
```

For example, `r ref("mlr_result_assigners_surrogate", "ResultAssignerSurrogate")` will not simply pick the best point according to the evaluations logged in the `archive` but instead will use a surrogate model to predict the mean of all evaluated points and proceed to choose the point with the best mean prediction as the final result.

## Robustness, Safety Nets, Defaults and Practical Considerations

Optimization is an automatic process and cannot rely on manual intervention.
Robustness of an optimization algorithm is therefore almost as important as good performance.
In the context of BO, there is plenty of room for potential failure of building blocks which can result in potential failure of the whole algorithm.
For example, training the surrogate model can go wrong, e.g., Gaussian Processes are strongly subject to the choice of kernel parameters which are usually obtained via maximum likelihood estimation.
Suboptimal parameter values can result in white noise models with a constant mean and variance prediction (except for the interpolation of training data).
As another example, if two points in the training data are too close to each other, training the Gaussian Process can fail due to singularity of the covariance matrix.
Finally, even if the training of the surrogate model suceeds, the prediction step needed for the computation of the acquisition function can error out.

`r mlr3mbo` has several built-in safety nets that ensure that all kinds of errors can be caught and handled appropriately within the BO algorithm.
Most importantly, all `r ref("Surrogate")` have the `catch_errors` hyperparameter:

```{r}
surrogate = srlrn(lrn("regr.km", covtype = "matern5_2", optim.method = "BFGS", control = list(trace = FALSE)))
surrogate$param_set$params$catch_errors
```

If set to `TRUE`, all errors that occur during training or updating of the surrogate model are caught and encapsulated.
The standard behavior of any `loop_function` then is to trigger a fallback, i.e., proposing the next candidate uniformly at random.

Similarly, `r ref("AcqOptimizer")` have the `catch_errors` hyperparameter:

```{r}
acq_optimizer = acqo(opt("nloptr", algorithm = "NLOPT_GN_ORIG_DIRECT"),
  terminator = trm("stagnation", iters = 100, threshold = 1e-5))
acq_optimizer$param_set$params$catch_errors
```

If set to `TRUE`, all errors that occur during the acquistion function optimization (either due to the surrogate model failing to predict or the acquisition function or acquisition function optimizer erroring out) are caught and encapsulated.
Again, the standard behavior of any `loop_function` then is to trigger a fallback, i.e., proposing the next candidate uniformly at random.

In the worst-case (all iterations erroring out), the BO algorithm will therefore simply perform a random search.
Ideally, the learner wrapped within the surrogate makes use of encapsulation and can rely on a fallback learner (see @sec-encapsulation-fallback) that will jump into action before this final safety net of proposing the next candidate uniformly at random is triggered.
Note that the value of the acquisition function is always also logged into the archive of the optimization instance.
To make sure that the BO algorithm behaved as expected, users should always inspect the log of the optimization process and inspect the archive and check whether the acquisition function column is populated as expected.

`r mlr3mbo` tries to use 'intelligent' defaults regarding the choice of surrogate model, acquisition function, acquisition function optimizer and even the loop function.
To see an up-to-date overview of these defaults, users should inspect the following man page:

```{r}
?mbo_defaults
```

For example, in the case of a purely numeric search space, `r mlr3mbo` will by default use a Gaussian Process as surrogate model and a random forest as fallback learner and additionally encapsulates the learner via the `r ref_pkg("evaluate")` package.
As a result of defaults existing for all building blocks, users can perform BO without specifying any building blocks and can still expect decent optimization performance:

```{r}
sinus_1D = function(xdt) {
  y = 2 * xdt$x * sin(14 * xdt$x)
  data.table(y = y)
}
domain = ps(x = p_dbl(lower = 0, upper = 1))
codomain = ps(y = p_dbl(tags = "minimize"))
objective = ObjectiveRFunDt$new(sinus_1D,
  domain = domain, codomain = codomain)
instance = OptimInstanceSingleCrit$new(objective,
  terminator = trm("evals", n_evals = 20))
optimizer = opt("mbo")
optimizer$optimize(instance)
```

FIXME:
robust behavior, safety nets, fallback learner, HPO practical paper verweisen, parallel mit multi point?
overhead von modellklassen kurz diskutieren, gp vs rf kurz diskutieren

## Conclusion

`r mlr3mbo` is built modular relying on a `r ref("Surrogate")`, `r ref("AcqFunction")` and `r ref("AcqOptimizer")` as well as a general `r ref("loop_function")` that build the actual optimizer or tuner constructed in the form of an `r ref("OptimizerMbo")` or `r ref("TunerMbo")`.
A more in-depth introduction to `r mlr3mbo` is given in its getting started vignette.

FIXME: same table as in Tuning

### Resources{.unnumbered .unlisted}

FIXME:

## Exercises

1. Minimize the 2D Rastrigin function ($f(\mathbf{x})=10 D+\sum_{i=1}^D\left[x_i^2-10 \cos \left(2 \pi x_i\right)\right]$, with $D = 2$) via BO (standard sequential single-objective BO via `bayesopt_ego`) using the lower confidence bound with `lambda = 1` as acquisition function and `"NLOPT_GN_ORIG_DIRECT"` via `nloptr` as acquisition function optimizer (similarly as above).
Use either a Gaussian Process with Matérn 5/2 kernel (`"regr.km"`, similarly as above) or a random forest (`"regr.ranger"`) as surrogate model and compare the anytime performance of these two BO algorithms.

FIXME:

