## ROC Curve and Thresholds {#binary-roc}

As we have seen before, binary classification is unique because of the presence of a positive and negative class and a threshold probability to distinguish between the two.
ROC Analysis, which stands for receiver operating characteristics, applies specifically to this case and allows a better picture of the trade-offs when choosing between the two classes.

We saw earlier that one can retrieve the confusion matrix of a `r ref("Prediction")` by accessing the `$confusion` field.
In the following code chunk, we first retrieve the Sonar classification task and construct a classification tree learner.
Next, we use the `r ref("partition()")` helper function to randomly split the rows of the Sonar task into two disjunct sets: a training set and a test set.
We train the learner on the training set and use the trained model to generate predictions on the test set.
Finally, we retrieve the confusion matrix.

```{r performance-001}
task = tsk("sonar")
learner = lrn("classif.rpart", predict_type = "prob")

# split into training and test
splits = partition(task, ratio = 0.8)
print(str(splits))

pred = learner$train(task, splits$train)$predict(task, splits$test)
pred$confusion
```

The upper left quadrant denotes the number of times our model predicted the positive class and was correct about it.
Similarly, the lower right quadrant denotes the number of times our model predicted the negative class and was also correct about it.
Together, the elements on the diagonal are called True Positives (TP) and True Negatives (TN).
The upper right quadrant denotes the number of times we falsely predicted a positive label and is called False Positives (FP).
The lower left quadrant is called False Negatives (FN).

We can derive the following performance metrics from the confusion matrix:

* **True Positive Rate (TPR)**: How many of the true positives did we predict as positive?
* **True Negative Rate (TNR)**: How many of the true negatives did we predict as negative?
* **Positive Predictive Value PPV**: If we predict positive how likely is it a true positive?
* **Negative Predictive Value NPV**: If we predict negative how likely is it a true negative?

It is difficult to achieve a high TPR and low FPR simultaneously.
We can characterize the behavior of a binary classifier for different thresholds by plotting the TPR and FPR values -- this is the ROC curve.
The best classifier lies on the top-left corner -- the TPR is 1 and the FPR is 0.
Classifiers on the diagonal produce labels randomly (possibly with different proportions).
For example, if each positive $x$ will be randomly classified with 25\% as "positive", we get a TPR of 0.25.
If we assign each negative $x$ randomly to "positive" we get a FPR of 0.25.
In practice, we should never obtain a classifier clearly below the diagonal -- ROC curves for different labels are symmetric with respect to the diagonal, so a curve below the diagonal would indicate that the positive and negative class labels have been switched by the classifier.

For `r mlr3` prediction objects, the ROC curve can easily be constructed with `r mlr3viz` which relies on the `r ref_pkg("precrec")` to calculate and plot ROC curves:

```{r performance-002}
library("mlr3viz")

# TPR vs FPR / Sensitivity vs (1 - Specificity)
autoplot(pred, type = "roc")
```

We can also plot the precision-recall curve (PPV vs. TPR).
The main difference between ROC curves and precision-recall curves (PRC) is that the number of true-negative results is not used for making a PRC.
PRCs are preferred over ROC curves for imbalanced populations.

```{r performance-003}
# Precision vs Recall
autoplot(pred, type = "prc")
```
